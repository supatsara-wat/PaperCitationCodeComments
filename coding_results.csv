comment_text,comment_location,language,link available?,paper type?,knowledge type?,knowledge size?,purpose of providing link?,repository type?,knowledge transfer?,same authors?
"`` < -- globalinfo-start -- > The class that splits a KDTree node based on the median value of a dimension in which the node 's points have the widest spread. < br > < br > For more information see also : < br > < br > Jerome H. Friedman , Jon Luis Bentley , Raphael Ari Finkel ( 1977 ) . An Algorithm for Finding Best Matches in Logarithmic Expected Time . ACM Transactions on Mathematics Software . 3 ( 3 ) :209-226 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Friedman1977 , author = { Jerome H. Friedman and Jon Luis Bentley and Raphael Ari Finkel } , journal = { ACM Transactions on Mathematics Software } , month = { September } , number = { 3 } , pages = { 209-226 } , title = { An Algorithm for Finding Best Matches in Logarithmic Expected Time } , volume = { 3 } , year = { 1977 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > < -- options-end -- > @ author Ashraf M. Kibriya ( amk14 [ at-the-rate ] cs [ dot ] waikato [ dot ] ac [ dot ] nz ) @ version $ Revision : 1.1 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_revisionhandler/weka/core/neighboursearch/kdtrees/MedianOfWidestDimension.java#L29,Java,yes,journal,algorithm,class,reference,machine learning framework,pseudocode to source code,no
"`` Values from : G. Jaeschke , On strong pseudoprimes to several bases , Math . Comp . 61 ( 204 ) :915-926 , 1993. ``",https://github.com/freebsd/freebsd/blob/27b7f1989483d2014a93a8316f4dd211e6b25563/usr.bin/primes/spsp.c#L143,C,yes,journal,numbers,method,reference,networks and os,numbers to hard coded values,no
"`` `` '' '' Return the directed modularity matrix of G. The modularity matrix is the matrix B = A - < A > , where A is the adjacency matrix and < A > is the expected adjacency matrix , assuming that the graph is described by the configuration model . More specifically , the element B_ij of B is defined as B_ij = A_ij - k_i ( out ) k_j ( in ) m where k_i ( in ) is the in degree of node i , and k_j ( out ) is the out degree of node j , with m the number of edges in the graph . Parameters -- -- -- -- -- G : DiGraph A NetworkX DiGraph nodelist : list , optional The rows and columns are ordered according to the nodes in nodelist . If nodelist is None , then the ordering is produced by G.nodes ( ) . Returns -- -- -- - B : Numpy matrix The modularity matrix of G. Examples -- -- -- -- > > > import networkx as nx > > > G = nx.DiGraph ( ) > > > G.add_edges_from ( ( ( 1,2 ) , ( 1,3 ) , ( 3,1 ) , ( 3,2 ) , ( 3,5 ) , ( 4,5 ) , ( 4,6 ) , ... ( 5,4 ) , ( 5,6 ) , ( 6,4 ) ) ) > > > B = nx.directed_modularity_matrix ( G ) Notes -- -- - NetworkX defines the element A_ij of the adjacency matrix as 1 if there is a link going from node i to node j. Leicht and Newman use the opposite definition . This explains the different expression for B_ij . See Also -- -- -- -- to_numpy_matrix adjacency_matrix laplacian_matrix modularity_matrix References -- -- -- -- -- .. [ 1 ] E. A. Leicht , M. E. J. Newman , `` Community structure in directed networks '' , Phys . Rev Lett. , vol . 100 , no . 11 , p. 118703 , 2008. `` '' '' ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/networkx/linalg/modularitymatrix.py#L80,Python,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` @ brief applies a luminance correction ( initially High Dynamic Range ( HDR ) tone mapping ) using only the 2 local adaptation stages of the retina parvocellular channel : photoreceptors level and ganlion cells level . Spatio temporal filtering is applied but limited to temporal smoothing and eventually high frequencies attenuation . This is a lighter method than the one available using the regular retina : :run method . It is then faster but it does not include complete temporal filtering nor retina spectral whitening . Then , it can have a more limited effect on images with a very high dynamic range . This is an adptation of the original still image HDR tone mapping algorithm of David Alleyson , Sabine Susstruck and Laurence Meylan 's work , please cite : - > Meylan L. , Alleysson D. , and Susstrunk S. , A Model of Retinal Local Adaptation for the Tone Mapping of Color Filter Array Images , Journal of Optical Society of America , A , Vol . 24 , N 9 , September , 1st , 2007 , pp . 2807-2816 @ param inputImage the input image to process RGB or gray levels @ param outputToneMappedImage the output tone mapped image ``",https://github.com/cpvrlab/SLProject/blob/90e5a60b550a44489654386458d35457940c8b73/lib-SLExternal/opencv/include/opencv2/bioinspired/retinafasttonemapping.hpp#L103,C++,yes,journal,algorithm,method,reference,computer vision,numbers to hard coded values,no
"`` u '' '' '' FORMER RATES [ 89 ] Tsang , W. ; Hampson , R.F . J. Phys . Chem . Ref . Data 1986 , 15 , 1087.C2H4 + O2 -- > C2H3 + HO2 C.D.W divided original rate expression by 4 , to get rate expression per H atom.pg 1097 , Chemical Kinetic Database For Combustion Chemistry , 2 . Index of Reactions and Summary of Recommended Rate Expressions . No . 18,3.Verified by Karma Jamespg . 1184 : Discussion on evaluated dataRecommended data follows Walker 's estimates for O2+alkaneNote : The authors note that a lower lying channel , involving addition andrearrangement prior to decomposition , may exist.MRH 28-Aug-2009CURRENT RATESHua , H. ; B. Ruscic ; B. Wang . Chemical Physics 2005 , 311 , 335-341.C2H4 + O2 -- > C2H3 + HO2.Divided rate expression by 4 to get the rate expression per H atom . See page 338.Overall , this agrees with the earlier rate that we used.JDM 15-Jun-2010 . '' '' '' ''",https://github.com/ReactionMechanismGenerator/RMG-database/blob/4ab827e5df28b51b08c3f4c7f4c7d7c47e367b01/input/kinetics/families/H_Abstraction/rules.py#L2107,Python,yes,journal,science,method,reference,science,scientific finding to hardcoded rule,no
"`` Purpose : HERMITE_LOOKUP_POINTS looks up abscissas for Hermite quadrature . Discussion : The integral : integral ( -oo < x < +oo ) exp ( - x x ) f ( x ) dx The quadrature rule : sum ( 1 < = i < = n ) w ( i ) f ( x ( i ) ) . Mathematica can numerically estimate the abscissas of order N to P digits by the command : NSolve [ HermiteH [ n , x ] == 0 , x , p ] Licensing : This code is distributed under the GNU LGPL license . Modified : 27 April 2010 Author : John Burkardt Reference : Milton Abramowitz , Irene Stegun , Handbook of Mathematical Functions , National Bureau of Standards , 1964 , ISBN : 0-486-61272-4 , LC : QA47.A34 . Vladimir Krylov , Approximate Calculation of Integrals , Dover , 2006 , ISBN : 0486445798 , LC : QA311.K713 . Arthur Stroud , Don Secrest , Gaussian Quadrature Formulas , Prentice Hall , 1966 , LC : QA299.4G3S7 . Stephen Wolfram , The Mathematica Book , Fourth Edition , Cambridge University Press , 1999 , ISBN : 0-521-64314-7 , LC : QA76.95.W65 . Daniel Zwillinger , editor , CRC Standard Mathematical Tables and Formulae , 30th Edition , CRC Press , 1996 , ISBN : 0-8493-2479-3 , LC : QA47.M315 . Parameters : Input , int N , the order . N must be between 1 and 20 . Output , Scalar X [ N ] , the abscissas. ``",https://github.com/trilinos/Trilinos/blob/d1a98e6faf63065bee9155d63ad18248d3d32ad8/packages/intrepid/src/Shared/Intrepid_BurkardtRulesDef.hpp#L1771,C++,yes,book,numbers,method,reference,science,numbers to hard coded values,no
"`` < -- globalinfo-start -- > The class that splits a node into two based on the midpoint value of the dimension in which the node 's rectangle is widest . If after splitting one side is empty then it is slided towards the non-empty side until there is at least one point on the empty side. < br > < br > For more information see also : < br > < br > David M. Mount ( 2006 ) . ANN Programming Manual . College Park , MD , USA . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; manual { Mount2006 , address = { College Park , MD , USA } , author = { David M. Mount } , organization = { Department of Computer Science , University of Maryland } , title = { ANN Programming Manual } , year = { 2006 } , HTTP = { Available from http : www.cs.umd.edu ~mount ANN } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > < -- options-end -- > @ author Ashraf M. Kibriya ( amk14 @ waikato.ac.nz ) @ version $ Revision : 1.2 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_revisionhandler/weka/core/neighboursearch/kdtrees/SlidingMidPointOfWidestSide.java#L29,Java,yes,manual (user manual for library),algorithm,class,reference,machine learning framework,description to source code,no
"`` The algorithm implemented by these routines bears a startling resemblance to one discovered by Beate Commentz-Walter , although it is not identical . See `` A String Matching Algorithm Fast on the Average , '' Technical Report , IBM-Germany , Scientific Center Heidelberg , Tiergartenstrasse 15 , D-6900 Heidelberg , Germany . See also Aho , A.V. , and M. Corasick , `` Efficient String Matching : An Aid to Bibliographic Search , '' CACM June 1975 , Vol . 18 , No . 6 , which describes the failure function used below. ``",https://github.com/PKRoma/git/blob/53f9a3e157dbbc901a02ac2c73346d375e24978c/kwset.c#L27,C,yes,book,algorithm,file,reference,other,pseudocode to source code,no
"`` Binary Fingerprint Methods brief This class provides efficient similarity calculation functionality for 2D binary fingerprints . BinaryFingerprintMethods uses a blocked inverted index data structures as a representation of 2D binary fingerprints . This data structure enables efficient calculation of similarity coefficients which use shared feature counts between two fingerprints . The shared feature count is the number of on-bits which two different fingerprints have in common . Currently the class provides the Tanimoto coefficient . Based on this similarity algorithm the class provides different useful public methods : link BinaryFingerprintMethods : :cutoffSearch Cutoff Search endlink : Similarities of all molecules in a query library to all molecules in a target library are calculated and the pairs whose similarities exceed a predefined cutoff are written to an output file . The performance of the method depends on size of the input query . For a single molecule query , the method has minimum performance and grows exponentially with the number of query molecules reaching a maximum performance at about 400 query molecules . As a consequence it is faster to bundle cutoff searches for multiple molecules if available . link BinaryFingerprintMethods : :connectedComponents Connected Components endlink : For an input library all pairwise similarities are calculated . A similarity graph is generated by inserting molecule pairs as edges . A variable similarity cutoff is applied to insert only those edges whose corresponding similarity exceeds this cutoff . Thereby , a similarity network is created and the connected components are finally calculated . If desired , the nearest neighbour of every molecule can additionally be returned . link BinaryFingerprintMethods : :averageLinkageClustering Hierarchical Clustering endlink : A set of input molecules is hierarhically clustered according to the average linkage criterion . Two different algorithms are implemented to make efficient use of the inverted index algorithm and the available hardware resources . Both methods are based on % Reciprocal Nearest Neighbours . The link BinaryFingerprintMethods : :averageLinkageParallel parallel variant endlink is used to handle large input library sizes . If the similarity matrix for the remaining clusters or possibly all input molecules fit into main memory , the matrix is calculated and the link BinaryFingerprintMethods : :NNChainCore Nearest Neighbour Chain algorithm endlink is used . Finally , the method of link BinaryFingerprintMethods : :clusterSelectionKGS Kelly et al . endlink is used to select an level for cluster selection and the created clusters are returned . link BinaryFingerprintMethods : :calculateSelectionMedoid Calculate Medoid endlink : For a set of input molecules the Medoid is calculated . The medoid is the molecule with the highest averaged similarity to all other molecules . Additionally , the method returns the averaged pairwise similarities for every molecule in the input set . Citations : Clustering Algorithms : F. Murtagh , Compstat Lectures vol . 4 , 1985 , Physica-Verlag W√ºrzburg-Wien . ( Volume titel : Multidimensional clustering algorithms ) . Similarity Update : G. Lance and W. Williams , Comput J . ( 1967 ) , 9 , 373-380 . ( doi : 10.1093 comjnl 9.4.373 ) . Cutoff Selection : L.A. Kelley , S.P . Gardner and M.J. Sutcliffe , % Protein Eng . ( 1996 ) , 9 , 1063-1065 . ( doi : 10.1093 protein 9.11.1063 ) . ``",https://github.com/BALL-Project/ball/blob/b9ac048ae7d9307d3bfc9d09c8853f5a2d797580/include/BALL/STRUCTURE/binaryFingerprintMethods.h#L32,C++,yes,book,algorithm,class,reference,science,pseudocode to source code,no
"`` Copyright ( c ) 2011-2013 : G-CSC , Goethe University Frankfurt This file is part of UG4 . UG4 is free software : you can redistribute it and or modify it under the terms of the GNU Lesser General Public License version 3 ( as published by the Free Software Foundation ) with the following additional attribution requirements ( according to LGPL GPL v3 ¬ß7 ) : ( 1 ) The following notice must be displayed in the Appropriate Legal Notices of covered and combined works : `` Based on UG4 ( www.ug4.org license ) '' . ( 2 ) The following notice must be displayed at a prominent place in the terminal output of covered works : `` Based on UG4 ( www.ug4.org license ) '' . ( 3 ) The following bibliography is recommended for citation and must be preserved in all covered files : `` Reiter , S. , Vogel , A. , Heppner , I. , Rupp , M. , and Wittum , G. A massively parallel geometric multigrid solver on hierarchically distributed grids . Computing and visualization in science 16 , 4 ( 2013 ) , 151-164 '' `` Vogel , A. , Reiter , S. , Rupp , M. , N√§gel , A. , and Wittum , G. UG4 -- a novel flexible software system for simulating pde based models on high performance computers . Computing and visualization in science 16 , 4 ( 2013 ) , 165-179 '' This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU Lesser General Public License for more details. ``",https://github.com/UG4/ugcore/blob/fd783d5abfa67f0bbaae380cd52cbb2beff8e0d4/ugbase/lib_disc/quadrature/gauss/gauss_quad_triangle.cpp#L1,C++,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` Implements John C. Platt 's sequential minimal optimization algorithm for training a support vector classifier using polynomial kernels . This implementation globally replaces all missing values and transforms nominal attributes into binary ones . For more information on the SMO algorithm , see < p > J. Platt ( 1998 ) . < i > Fast Training of Support Vector Machines using Sequential Minimal Optimization < i > . Advances in Kernel Methods - Support Vector Learning , B. SchÔøΩlkopf , C. Burges , and A. Smola , eds. , MIT Press . < p > S.S. Keerthi , S.K . Shevade , C. Bhattacharyya , K.R.K . Murthy ( 1999 ) . < i > Improvements to Platt 's SMO Algorithm for SVM Classifier Design < i > . Technical Report CD-99-14 . Control Division , Dept of Mechanical and Production Engineering , National University of Singapore . < p > Note : for improved speed normalization should be turned off when operating on SparseInstances. < p > Valid options are : < p > -C num < br > The complexity constant C. ( default 1 ) < p > -E num < br > The exponent for the polynomial kernel . ( default 1 ) < p > -N < br > Do n't normalize the training instances . < p > -L < br > Rescale kernel . < p > -O < br > Use lower-order terms . < p > -A num < br > Sets the size of the kernel cache . Should be a prime number . ( default 1000003 ) < p > -T num < br > Sets the tolerance parameter . ( default 1.0e-3 ) < p > -P num < br > Sets the epsilon for round-off error . ( default 1.0e-12 ) < p > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Shane Legg ( shane @ intelligenesis.net ) ( sparse vector code ) @ author Stuart Inglis ( stuart @ intelligenesis.net ) ( sparse vector code ) @ version $ Revision : 1.28 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-3-1/weka/classifiers/functions/SMO.java#L32,Java,yes,book,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` `` '' '' Color a graph using various strategies of greedy graph coloring . Attempts to color a graph using as few colors as possible , where no neighbours of a node can have same color as the node itself . The given strategy determines the order in which nodes are colored . The strategies are described in [ 1 ] _ , and smallest-last is based on [ 2 ] _. Parameters -- -- -- -- -- G : NetworkX graph strategy : string or function ( G , colors ) A function ( or a string representing a function ) that provides the coloring strategy , by returning nodes in the ordering they should be colored . `` G `` is the graph , and `` colors `` is a dictionary of the currently assigned colors , keyed by nodes . The function must return an iterable over all the nodes in `` G `` . If the strategy function is an iterator generator ( that is , a function with `` yield `` statements ) , keep in mind that the `` colors `` dictionary will be updated after each `` yield `` , since this function chooses colors greedily . If `` strategy `` is a string , it must be one of the following , each of which represents one of the built-in strategy functions . `` 'largest_first ' `` `` 'random_sequential ' `` `` 'smallest_last ' `` `` 'independent_set ' `` `` 'connected_sequential_bfs ' `` `` 'connected_sequential_dfs ' `` `` 'connected_sequential ' `` ( alias for the previous strategy ) `` 'strategy_saturation_largest_first ' `` `` 'DSATUR ' `` ( alias for the previous strategy ) interchange : bool Will use the color interchange algorithm described by [ 3 ] _ if set to `` True `` . Note that `` strategy_saturation_largest_first `` and `` strategy_independent_set `` do not work with interchange . Furthermore , if you use interchange with your own strategy function , you can not rely on the values in the `` colors `` argument . Returns -- -- -- - A dictionary with keys representing nodes and values representing corresponding coloring . Examples -- -- -- -- > > > G = nx.cycle_graph ( 4 ) > > > d = nx.coloring.greedy_color ( G , strategy='largest_first ' ) > > > d in [ { 0 : 0 , 1 : 1 , 2 : 0 , 3 : 1 } , { 0 : 1 , 1 : 0 , 2 : 1 , 3 : 0 } ] True Raises -- -- -- NetworkXPointlessConcept If `` strategy `` is `` strategy_saturation_largest_first `` or `` strategy_independent_set `` and `` interchange `` is `` True `` . References -- -- -- -- -- .. [ 1 ] Adrian Kosowski , and Krzysztof Manuszewski , Classical Coloring of Graphs , Graph Colorings , 2-19 , 2004 . ISBN 0-8218-3458-4 . .. [ 2 ] David W. Matula , and Leland L. Beck , `` Smallest-last ordering and clustering and graph coloring algorithms . '' J. ACM 30 , 3 ( July 1983 ) , 417‚Äì427 . < https : doi.org 10.1145 2402.322385 > .. [ 3 ] Maciej M. Sys≈Ço , Marsingh Deo , Janusz S. Kowalik , Discrete Optimization Algorithms with Pascal Programs , 415-424 , 1983 . ISBN 0-486-45353-7. `` '' '' ''",https://github.com/networkx/networkx/blob/404679f7a8a9f015a693eaa01a5b319091123b31/networkx/algorithms/coloring/greedy_coloring.py#L250,Python,yes,book,algorithm,method,reference,networks and os,pseudocode to source code,no
"`` `` '' '' gdtrix ( a , b , p , out=None ) Inverse of ` gdtr ` vs x . Returns the inverse with respect to the parameter ` x ` of `` p = gdtr ( a , b , x ) `` , the cumulative distribution function of the gamma distribution . This is also known as the p'th quantile of the distribution . Parameters -- -- -- -- -- a : array_like ` a ` parameter values of ` gdtr ( a , b , x ) ` . ` 1 a ` is the `` scale '' parameter of the gamma distribution . b : array_like ` b ` parameter values of ` gdtr ( a , b , x ) ` . ` b ` is the `` shape '' parameter of the gamma distribution . p : array_like Probability values . out : ndarray , optional If a fourth argument is given , it must be a numpy.ndarray whose size matches the broadcast result of ` a ` , ` b ` and ` x ` . ` out ` is then the array returned by the function . Returns -- -- -- - x : ndarray Values of the ` x ` parameter such that ` p = gdtr ( a , b , x ) ` . See Also -- -- -- -- gdtr : CDF of the gamma distribution . gdtria : Inverse with respect to ` a ` of ` gdtr ( a , b , x ) ` . gdtrib : Inverse with respect to ` b ` of ` gdtr ( a , b , x ) ` . Notes -- -- - Wrapper for the CDFLIB [ 1 ] _ Fortran routine ` cdfgam ` . The cumulative distribution function ` p ` is computed using a routine by DiDinato and Morris [ 2 ] _. Computation of ` x ` involves a seach for a value that produces the desired value of ` p ` . The search relies on the monotinicity of ` p ` with ` x ` . References -- -- -- -- -- .. [ 1 ] Barry Brown , James Lovato , and Kathy Russell , CDFLIB : Library of Fortran Routines for Cumulative Distribution Functions , Inverses , and Other Parameters . .. [ 2 ] DiDinato , A. R. and Morris , A. H. , Computation of the incomplete gamma function ratios and their inverse . ACM Trans . Math . Softw . 12 ( 1986 ) , 377-393 . Examples -- -- -- -- First evaluate ` gdtr ` . > > > from scipy.special import gdtr , gdtrix > > > p = gdtr ( 1.2 , 3.4 , 5.6 ) > > > print ( p ) 0.94378087442 Verify the inverse . > > > gdtrix ( 1.2 , 3.4 , p ) 5.5999999999999996 `` '' '' ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/scipy/special/add_newdocs.py#L2103,Python,yes,library documentation,implementation,method,reference,other,documentation,no
"`` `` '' '' Perform wavelet denoising on an image . Parameters -- -- -- -- -- image : ndarray ( [ M [ , N [ , ... P ] ] [ , C ] ) of ints , uints or floats Input data to be denoised . ` image ` can be of any numeric type , but it is cast into an ndarray of floats for the computation of the denoised image . sigma : float or list , optional The noise standard deviation used when computing the wavelet detail coefficient threshold ( s ) . When None ( default ) , the noise standard deviation is estimated via the method in [ 2 ] _. wavelet : string , optional The type of wavelet to perform and can be any of the options `` pywt.wavelist `` outputs . The default is ` 'db1 ' ` . For example , `` wavelet `` can be any of `` { 'db2 ' , 'haar ' , 'sym9 ' } `` and many more . mode : { 'soft ' , 'hard ' } , optional An optional argument to choose the type of denoising performed . It noted that choosing soft thresholding given additive noise finds the best approximation of the original image . wavelet_levels : int or None , optional The number of wavelet decomposition levels to use . The default is three less than the maximum number of possible decomposition levels . multichannel : bool , optional Apply wavelet denoising separately for each channel ( where channels correspond to the final axis of the array ) . convert2ycbcr : bool , optional If True and multichannel True , do the wavelet denoising in the YCbCr colorspace instead of the RGB color space . This typically results in better performance for RGB images . method : { 'BayesShrink ' , 'VisuShrink ' } , optional Thresholding method to be used . The currently supported methods are `` BayesShrink '' [ 1 ] _ and `` VisuShrink '' [ 2 ] _. Defaults to `` BayesShrink '' . Returns -- -- -- - out : ndarray Denoised image . Notes -- -- - The wavelet domain is a sparse representation of the image , and can be thought of similarly to the frequency domain of the Fourier transform . Sparse representations have most values zero or near-zero and truly random noise is ( usually ) represented by many small values in the wavelet domain . Setting all values below some threshold to 0 reduces the noise in the image , but larger thresholds also decrease the detail present in the image . If the input is 3D , this function performs wavelet denoising on each color plane separately . The output image is clipped between either [ -1 , 1 ] and [ 0 , 1 ] depending on the input image range . When YCbCr conversion is done , every color channel is scaled between 0 and 1 , and ` sigma ` values are applied to these scaled color channels . Many wavelet coefficient thresholding approaches have been proposed . By default , `` denoise_wavelet `` applies BayesShrink , which is an adaptive thresholding method that computes separate thresholds for each wavelet sub-band as described in [ 1 ] _ . If `` method == `` VisuShrink '' `` , a single `` universal threshold '' is applied to all wavelet detail coefficients as described in [ 2 ] _ . This threshold is designed to remove all Gaussian noise at a given `` sigma `` with high probability , but tends to produce images that appear overly smooth . References -- -- -- -- -- .. [ 1 ] Chang , S. Grace , Bin Yu , and Martin Vetterli . `` Adaptive wavelet thresholding for image denoising and compression . '' Image Processing , IEEE Transactions on 9.9 ( 2000 ) : 1532-1546 . DOI : 10.1109 83.862633 .. [ 2 ] D. L. Donoho and I. M. Johnstone . `` Ideal spatial adaptation by wavelet shrinkage . '' Biometrika 81.3 ( 1994 ) : 425-455 . DOI : 10.1093 biomet 81.3.425 Examples -- -- -- -- > > > from skimage import color , data > > > img = img_as_float ( data.astronaut ( ) ) > > > img = color.rgb2gray ( img ) > > > img += 0.1 np.random.randn ( img.shape ) > > > img = np.clip ( img , 0 , 1 ) > > > denoised_img = denoise_wavelet ( img , sigma=0.1 ) `` '' '' ''",https://github.com/scikit-image/scikit-image/blob/51f598aaedc73ef180913c670d2c20a8032aaf1e/skimage/restoration/_denoise.py#L497,Python,yes,journal,algorithm,method,reference,computer vision,formulas to source code,no
"`` Find KD-tree node whose key is nearest neighbor to key . Implements the Nearest Neighbor algorithm ( Table 6.4 ) of < PRE > & 064 ; techreport { AndrewMooreNearestNeighbor , author = { Andrew Moore } , title = { An introductory tutorial on kd-trees } , institution = { Robotics Institute , Carnegie Mellon University } , year = { 1991 } , number = { Technical Report No . 209 , Computer Laboratory , University of Cambridge } , address = { Pittsburgh , PA } } < PRE > @ param key key for KD-tree node @ return object at node nearest to key , or null on failure @ throws KeySizeException if key.length mismatches K ``",https://github.com/deric/clueminer/blob/3f7b27ea5a0d70237106832e56ef496d1e3ed40c/modules/clustering-dist/src/main/java/org/clueminer/kdtree/KDTree.java#L127,Java,yes,book,algorithm,method,reference,machine learning,paper not available,no
"`` TCP Westwood+ : end-to-end bandwidth estimation for TCP Angelo Dell'Aera : author of the first version of TCP Westwood+ in Linux 2.4 Support at http : c3lab.poliba.it index.php Westwood Main references in literature : - Mascolo S , Casetti , M. Gerla et al . `` TCP Westwood : bandwidth estimation for TCP '' Proc . ACM Mobicom 2001 - A. Grieco , s. Mascolo `` Performance evaluation of New Reno , Vegas , Westwood+ TCP '' ACM Computer Comm . Review , 2004 - A. Dell'Aera , L. Grieco , S. Mascolo . `` Linux 2.4 Implementation of Westwood+ TCP with Rate-Halving : A Performance Evaluation Over the Internet '' ( ICC 2004 ) , Paris , June 2004 Westwood+ employs end-to-end bandwidth measurement to set cwnd and ssthresh after packet loss . The probing phase is as the original Reno. ``",https://github.com/Epirex/Samsung_STE_Kernel/blob/ac7219c4b8dcec7bc5a598d42c6be0db4aa36332/net/ipv4/tcp_westwood.c#L1,C,yes,conference,algorithm,file,reference,networks and os,pseudocode to source code,no
"`` Implementation of the HiCO algorithm , an algorithm for detecting hierarchies of correlation clusters . < p > Reference : E. Achtert , C. B√∂hm , P. Kr√∂ger , A. Zimek : < br > Mining Hierarchies of Correlation Clusters . < br > In : Proc . Int . Conf . on Scientific and Statistical Database Management ( SSDBM'06 ) , Vienna , Austria , 2006 . < p > @ author Elke Achtert @ since 0.3 @ apiviz.composedOf HiCO.Instance @ param < V > the type of NumberVector handled by the algorithm ``",https://github.com/elki-project/elki/blob/3e820f9f6382cb82e40955692337737c24f94b54/elki-clustering/src/main/java/de/lmu/ifi/dbs/elki/algorithm/clustering/correlation/HiCO.java#L75,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` Copyright ( c ) 2013-2014 : G-CSC , Goethe University Frankfurt Author : Torbjoern Klatt , Martin Scherer This file is part of UG4 . UG4 is free software : you can redistribute it and or modify it under the terms of the GNU Lesser General Public License version 3 ( as published by the Free Software Foundation ) with the following additional attribution requirements ( according to LGPL GPL v3 ¬ß7 ) : ( 1 ) The following notice must be displayed in the Appropriate Legal Notices of covered and combined works : `` Based on UG4 ( www.ug4.org license ) '' . ( 2 ) The following notice must be displayed at a prominent place in the terminal output of covered works : `` Based on UG4 ( www.ug4.org license ) '' . ( 3 ) The following bibliography is recommended for citation and must be preserved in all covered files : `` Reiter , S. , Vogel , A. , Heppner , I. , Rupp , M. , and Wittum , G. A massively parallel geometric multigrid solver on hierarchically distributed grids . Computing and visualization in science 16 , 4 ( 2013 ) , 151-164 '' `` Vogel , A. , Reiter , S. , Rupp , M. , N√§gel , A. , and Wittum , G. UG4 -- a novel flexible software system for simulating pde based models on high performance computers . Computing and visualization in science 16 , 4 ( 2013 ) , 165-179 '' This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU Lesser General Public License for more details. ``",https://github.com/UG4/ugcore/blob/fd783d5abfa67f0bbaae380cd52cbb2beff8e0d4/ugbase/common/util/base64_file_writer.cpp#L1,C++,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` fdomain.c -- Future Domain TMC-16x0 SCSI driver Created : Sun May 3 18:53:19 1992 by faith @ cs.unc.edu Revised : Mon Dec 28 21:59:02 1998 by faith @ acm.org Author : Rickard E. Faith , faith @ cs.unc.edu Copyright 1992-1996 , 1998 Rickard E. Faith ( faith @ acm.org ) Shared IRQ supported added 7 7 2001 Alan Cox < alan @ lxorguk.ukuu.org.uk > This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation ; either version 2 , or ( at your option ) any later version . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . You should have received a copy of the GNU General Public License along with this program ; if not , write to the Free Software Foundation , Inc. , 675 Mass Ave , Cambridge , MA 02139 , USA . SUMMARY : Future Domain BIOS versions supported for autodetect : 2.0 , 3.0 , 3.2 , 3.4 ( 1.0 ) , 3.5 ( 2.0 ) , 3.6 , 3.61 Chips are supported : TMC-1800 , TMC-18C50 , TMC-18C30 , TMC-36C70 Boards supported : Future Domain TMC-1650 , TMC-1660 , TMC-1670 , TMC-1680 , TMC-1610M MER MEX Future Domain TMC-3260 ( PCI ) Quantum ISA-200S , ISA-250MG Adaptec AHA-2920A ( PCI ) [ BUT NOT AHA-2920C -- use aic7xxx instead ] IBM ? LILO INSMOD command-line options : fdomain= < PORT_BASE > , < IRQ > [ , < ADAPTER_ID > ] NOTE : The Adaptec AHA-2920C has an Adaptec AIC-7850 chip on it . Use the aic7xxx driver for this board . The Adaptec AHA-2920A has a Future Domain chip on it , so this is the right driver for that card . Unfortunately , the boxes will probably just say `` 2920 '' , so you 'll have to look on the card for a Future Domain logo , or a letter after the 2920 . THANKS : Thanks to Adaptec for providing PCI boards for testing . This finally enabled me to test the PCI detection and correct it for PCI boards that do not have a BIOS at a standard ISA location . For PCI boards , LILO INSMOD command-line options should no longer be needed . -- RF 18Nov98 DESCRIPTION : This is the Linux low-level SCSI driver for Future Domain TMC-1660 1680 TMC-1650 1670 , and TMC-3260 SCSI host adapters . The 1650 and 1670 have a 25-pin external connector , whereas the 1660 and 1680 have a SCSI-2 50-pin high-density external connector . The 1670 and 1680 have floppy disk controllers built in . The TMC-3260 is a PCI bus card . Future Domain 's older boards are based on the TMC-1800 chip , and this driver was originally written for a TMC-1680 board with the TMC-1800 chip . More recently , boards are being produced with the TMC-18C50 and TMC-18C30 chips . The latest and greatest board may not work with this driver . If you have to patch this driver so that it will recognize your board 's BIOS signature , then the driver may fail to function after the board is detected . Please note that the drive ordering that Future Domain implemented in BIOS versions 3.4 and 3.5 is the opposite of the order ( currently ) used by the rest of the SCSI industry . If you have BIOS version 3.4 or 3.5 , and have more than one drive , then the drive ordering will be the reverse of that which you see under DOS . For example , under DOS SCSI ID 0 will be D : and SCSI ID 1 will be C : ( the boot device ) . Under Linux , SCSI ID 0 will be dev sda and SCSI ID 1 will be dev sdb . The Linux ordering is consistent with that provided by all the other SCSI drivers for Linux . If you want this changed , you will probably have to patch the higher level SCSI code . If you do so , please send me patches that are protected by ifdefs . If you have a TMC-8xx or TMC-9xx board , then this is not the driver for your board . Please refer to the Seagate driver for more information and possible support . HISTORY : Linux Driver Driver Version Version Date Support Notes 0.0 3 May 1992 V2.0 BIOS ; 1800 chip 0.97 1.9 28 Jul 1992 0.98.6 3.1 27 Nov 1992 0.99 3.2 9 Dec 1992 0.99.3 3.3 10 Jan 1993 V3.0 BIOS 0.99.5 3.5 18 Feb 1993 0.99.10 3.6 15 May 1993 V3.2 BIOS ; 18C50 chip 0.99.11 3.17 3 Jul 1993 ( now under RCS ) 0.99.12 3.18 13 Aug 1993 0.99.14 5.6 31 Oct 1993 ( reselection code removed ) 0.99.15 5.9 23 Jan 1994 V3.4 BIOS ( preliminary ) 1.0.8 1.1.1 5.15 1 Apr 1994 V3.4 BIOS ; 18C30 chip ( preliminary ) 1.0.9 1.1.3 5.16 7 Apr 1994 V3.4 BIOS ; 18C30 chip 1.1.38 5.18 30 Jul 1994 36C70 chip ( PCI version of 18C30 ) 1.1.62 5.20 2 Nov 1994 V3.5 BIOS 1.1.73 5.22 7 Dec 1994 Quantum ISA-200S board ; V2.0 BIOS 1.1.82 5.26 14 Jan 1995 V3.5 BIOS ; TMC-1610M MER MEX board 1.2.10 5.28 5 Jun 1995 Quantum ISA-250MG board ; V2.0 , V2.01 BIOS 1.3.4 5.31 23 Jun 1995 PCI BIOS-32 detection ( preliminary ) 1.3.7 5.33 4 Jul 1995 PCI BIOS-32 detection 1.3.28 5.36 17 Sep 1995 V3.61 BIOS ; LILO command-line support 1.3.34 5.39 12 Oct 1995 V3.60 BIOS ; proc 1.3.72 5.39 8 Feb 1996 Adaptec AHA-2920 board 1.3.85 5.41 4 Apr 1996 2.0.12 5.44 8 Aug 1996 Use ID 7 for all PCI cards 2.1.1 5.45 2 Oct 1996 Update ROM accesses for 2.1.x 2.1.97 5.46 23 Apr 1998 Rewritten PCI detection routines [ mj ] 2.1.11x 5.47 9 Aug 1998 Touched for 8 SCSI disk majors support 5.48 18 Nov 1998 BIOS no longer needed for PCI detection 2.2.0 5.50 28 Dec 1998 Support insmod parameters REFERENCES USED : `` TMC-1800 SCSI Chip Specification ( FDC-1800T ) '' , Future Domain Corporation , 1990 . `` Technical Reference Manual : 18C50 SCSI Host Adapter Chip '' , Future Domain Corporation , January 1992 . `` LXT SCSI Products : Specifications and OEM Technical Manual ( Revision B September 1991 ) '' , Maxtor Corporation , 1991 . `` 7213S product Manual ( Revision P3 ) '' , Maxtor Corporation , 1992 . `` Draft Proposed American National Standard : Small Computer System Interface - 2 ( SCSI-2 ) '' , Global Engineering Documents . ( X3T9.2 86-109 , revision 10h , October 17 , 1991 ) Private communications , Drew Eckhardt ( drew @ cs.colorado.edu ) and Eric Youngdale ( ericy @ cais.com ) , 1992 . Private communication , Tuong Le ( Future Domain Engineering department ) , 1994 . ( Disk geometry computations for Future Domain BIOS version 3.4 , and TMC-18C30 detection . ) Hogan , Thom . The Programmer 's PC Sourcebook . Microsoft Press , 1988 . Page 60 ( 2.39 : Disk Partition Table Layout ) . `` 18C30 Technical Reference Manual '' , Future Domain Corporation , 1993 , page 6-1 . NOTES ON REFERENCES : The Maxtor manuals were free . Maxtor telephone technical support is great The Future Domain manuals were $ 25 and $ 35 . They document the chip , not the TMC-16x0 boards , so some information I had to guess at . In 1992 , Future Domain sold DOS BIOS source for $ 250 and the UN X driver source was $ 750 , but these required a non-disclosure agreement , so even if I could have afforded them , they would not have been useful for writing this publicly distributable driver . Future Domain technical support has provided some information on the phone and have sent a few useful FAXs . They have been much more helpful since they started to recognize that the word `` Linux '' refers to an operating system : - ) . ALPHA TESTERS : There are many other alpha testers that come and go as the driver develops . The people listed here were most helpful in times of greatest need ( mostly early on -- I 've probably left out a few worthy people in more recent times ) : Todd Carrico ( todd @ wutc.wustl.edu ) , Dan Poirier ( poirier @ cs.unc.edu ) , Ken Corey ( kenc @ sol.acs.unt.edu ) , C. de Bruin ( bruin @ bruin @ sterbbs.nl ) , Sakari Aaltonen ( sakaria @ vipunen.hit.fi ) , John Rice ( rice @ xanth.cs.odu.edu ) , Brad Yearwood ( brad @ optilink.com ) , and Ray Toy ( toy @ soho.crd.ge.com ) . Special thanks to Tien-Wan Yang ( twyang @ cs.uh.edu ) , who graciously lent me his 18C50-based card for debugging . He is the sole reason that this driver works with the 18C50 chip . Thanks to Dave Newman ( dnewman @ crl.com ) for providing initial patches for the version 3.4 BIOS . Thanks to James T. McKinley ( mckinley @ msupa.pa.msu.edu ) for providing patches that support the TMC-3260 , a PCI bus card with the 36C70 chip . The 36C70 chip appears to be `` completely compatible '' with the 18C30 chip . Thanks to Eric Kasten ( tigger @ petroglyph.cl.msu.edu ) for providing the patch for the version 3.5 BIOS . Thanks for Stephen Henson ( shenson @ nyx10.cs.du.edu ) for providing the patch for the Quantum ISA-200S SCSI adapter . Thanks to Adam Bowen for the signature to the 1610M MER MEX scsi cards , to Martin Andrews ( andrewm @ ccfadm.eeg.ccf.org ) for the signature to some random TMC-1680 repackaged by IBM ; and to Mintak Ng ( mintak @ panix.com ) for the version 3.61 BIOS signature . Thanks for Mark Singer ( elf @ netcom.com ) and Richard Simpson ( rsimpson @ ewrcsdra.demon.co.uk ) for more Quantum signatures and detective work on the Quantum RAM layout . Special thanks to James T. McKinley ( mckinley @ msupa.pa.msu.edu ) for providing patches for proper PCI BIOS32-mediated detection of the TMC-3260 card ( a PCI bus card with the 36C70 chip ) . Please send James PCI-related bug reports . Thanks to Tom Cavin ( tec @ usa1.com ) for preliminary command-line option patches . New PCI detection code written by Martin Mares < mj @ atrey.karlin.mff.cuni.cz > Insmod parameter code based on patches from Daniel Graham < graham @ balance.uoregon.edu > . All of the alpha testers deserve much thanks . NOTES ON USER DEFINABLE OPTIONS : DEBUG : This turns on the printing of various debug information . ENABLE_PARITY : This turns on SCSI parity checking . With the current driver , all attached devices must support SCSI parity . If none of your devices support parity , then you can probably get the driver to work by turning this option off . I have no way of testing this , however , and it would appear that no one ever uses this option . FIFO_COUNT : The host adapter has an 8K cache ( host adapters based on the 18C30 chip have a 2k cache ) . When this many 512 byte blocks are filled by the SCSI device , an interrupt will be raised . Therefore , this could be as low as 0 , or as high as 16 . Note , however , that values which are too high or too low seem to prevent any interrupts from occurring , and thereby lock up the machine . I have found that 2 is a good number , but throughput may be increased by changing this value to values which are close to 2 . Please let me know if you try any different values . RESELECTION : This is no longer an option , since I gave up trying to implement it in version 4.x of this driver . It did not improve performance at all and made the driver unstable ( because I never found one of the two race conditions which were introduced by the multiple outstanding command code ) . The instability seems a very high price to pay just so that you do n't have to wait for the tape to rewind . If you want this feature implemented , send me patches . I 'll be happy to send a copy of my ( broken ) driver to anyone who would like to see a copy. ``",https://github.com/linux-scraping/linux-grsecurity/blob/acb68e4a24be1872bc9eb5b0ddc340c3816349bf/drivers/scsi/fdomain.c#L1,C,yes,specification,specification,n/a,reference,networks and os,paper not available,no
"`` Find minimum of an univariate function f. < p > Algorithm : G.Forsythe , M.Malcolm , C.Moler , Computer methods for mathematical computations . M. , Mir , 1980 , p.180 of the Russian edition @ param { function } f Function , whose minimum is to be found @ param { Array } x0 Start interval enclosing the minimum @ param { Object } context Parent object in case f is method of it @ returns { Number } the approximation of the minimum value position @ memberof JXG.Math.Numerics ``",https://github.com/jsxgraph/jsxgraph/blob/6c153a5686a93f98e3e59e50851c08e7ee63e13a/JSXCompressor/jsxgraphcore.js#L7411,JavaScript,yes,book,algorithm,method,reference,other,paper not available,no
"`` - - coding : utf-8 - - TheVirtualBrain-Framework Package . This package holds all Data Management , and Web-UI helpful to run brain-simulations . To use it , you also need do download TheVirtualBrain-Scientific Package ( for simulators ) . See content of the documentation-folder for more details . See also http : www.thevirtualbrain.org ( c ) 2012-2017 , Baycrest Centre for Geriatric Care ( `` Baycrest '' ) and others This program is free software : you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation , either version 3 of the License , or ( at your option ) any later version . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . You should have received a copy of the GNU General Public License along with this program . If not , see < http : www.gnu.org licenses > . CITATION : When using The Virtual Brain for scientific publications , please cite it as follows : Paula Sanz Leon , Stuart A . Knock , M. Marmaduke Woodman , Lia Domide , Jochen Mersmann , Anthony R. McIntosh , Viktor Jirsa ( 2013 ) The Virtual Brain : a simulator of primate brain network dynamics . Frontiers in Neuroinformatics ( 7:10. doi : 10.3389 fninf.2013.00010 ) `` '' '' Created on Nov 1 , 2011.. moduleauthor : : Bogdan Neacsa < bogdan.neacsa @ codemart.ro > '' '' '' ''",https://github.com/the-virtual-brain/tvb-framework/blob/d69b424048d9b07aeab5f27644d51ffa01a6ac43/tvb/core/removers_factory.py#L1,Python,yes,journal,background,n/a,related,science,no transfer,yes
"`` Generic binary BCH encoding decoding library This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License version 2 as published by the Free Software Foundation . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . You should have received a copy of the GNU General Public License along with this program ; if not , write to the Free Software Foundation , Inc. , 51 Franklin St , Fifth Floor , Boston , MA 02110-1301 USA . Copyright ¬© 2011 Parrot S.A . Author : Ivan Djelic < ivan.djelic @ parrot.com > Description : This library provides runtime configurable encoding decoding of binary Bose-Chaudhuri-Hocquenghem ( BCH ) codes . Call init_bch to get a pointer to a newly allocated bch_control structure for the given m ( Galois field order ) , t ( error correction capability ) and ( optional ) primitive polynomial parameters . Call encode_bch to compute and store ecc parity bytes to a given buffer . Call decode_bch to detect and locate errors in received data . On systems supporting hw BCH features , intermediate results may be provided to decode_bch in order to skip certain steps . See decode_bch ( ) documentation for details . Option CONFIG_BCH_CONST_PARAMS can be used to force fixed values of parameters m and t ; thus allowing extra compiler optimizations and providing better ( up to 2x ) encoding performance . Using this option makes sense when ( m , t ) are fixed and known in advance , e.g . when using BCH error correction on a particular NAND flash device . Algorithmic details : Encoding is performed by processing 32 input bits in parallel , using 4 remainder lookup tables . The final stage of decoding involves the following internal steps : a . Syndrome computation b . Error locator polynomial computation using Berlekamp-Massey algorithm c. Error locator root finding ( by far the most expensive step ) In this implementation , step c is not performed using the usual Chien search . Instead , an alternative approach described in [ 1 ] is used . It consists in factoring the error locator polynomial using the Berlekamp Trace algorithm ( BTA ) down to a certain degree ( 4 ) , after which ad hoc low-degree polynomial solving techniques [ 2 ] are used . The resulting algorithm , called BTZ , yields much better performance than Chien search for usual ( m , t ) values ( typically m > = 13 , t < 32 , see [ 1 ] ) . [ 1 ] B. Biswas , V. Herbert . Efficient root finding of polynomials over fields of characteristic 2 , in : Western European Workshop on Research in Cryptology - WEWoRC 2009 , Graz , Austria , LNCS , Springer , July 2009 , to appear . [ 2 ] [ Zin96 ] V.A . Zinoviev . On the solution of equations of degree 10 over finite fields GF ( 2^q ) . In Rapport de recherche INRIA no 2829 , 1996. ``",https://github.com/Tomoms/helium_kernel/blob/a20b5d0758d5308803394385a8eb753b6b24d345/lib/bch.c#L1,C,yes,conference,algorithm,file,reference,networks and os,pseudocode to source code,no
"`` `` '' '' Minimize a function using the Constrained Optimization BY Linear Approximation ( COBYLA ) method . This method wraps a FORTRAN implementation of the algorithm . Parameters -- -- -- -- -- func : callable Function to minimize . In the form func ( x , args ) . x0 : ndarray Initial guess . cons : sequence Constraint functions ; must all be `` > =0 `` ( a single function if only 1 constraint ) . Each function takes the parameters ` x ` as its first argument , and it can return either a single number or an array or list of numbers . args : tuple , optional Extra arguments to pass to function . consargs : tuple , optional Extra arguments to pass to constraint functions ( default of None means use same extra arguments as those passed to func ) . Use `` ( ) `` for no extra arguments . rhobeg : float , optional Reasonable initial changes to the variables . rhoend : float , optional Final accuracy in the optimization ( not precisely guaranteed ) . This is a lower bound on the size of the trust region . iprint : { 0 , 1 , 2 , 3 } , optional Controls the frequency of output ; 0 implies no output . Deprecated . disp : { 0 , 1 , 2 , 3 } , optional Over-rides the iprint interface . Preferred . maxfun : int , optional Maximum number of function evaluations . catol : float , optional Absolute tolerance for constraint violations . Returns -- -- -- - x : ndarray The argument that minimises ` f ` . See also -- -- -- -- minimize : Interface to minimization algorithms for multivariate functions . See the 'COBYLA ' ` method ` in particular . Notes -- -- - This algorithm is based on linear approximations to the objective function and each constraint . We briefly describe the algorithm . Suppose the function is being minimized over k variables . At the jth iteration the algorithm has k+1 points v_1 , ... , v_ ( k+1 ) , an approximate solution x_j , and a radius RHO_j . ( i.e . linear plus a constant ) approximations to the objective function and constraint functions such that their function values agree with the linear approximation on the k+1 points v_1 , .. , v_ ( k+1 ) . This gives a linear program to solve ( where the linear approximations of the constraint functions are constrained to be non-negative ) . However the linear approximations are likely only good approximations near the current simplex , so the linear program is given the further requirement that the solution , which will become x_ ( j+1 ) , must be within RHO_j from x_j . RHO_j only decreases , never increases . The initial RHO_j is rhobeg and the final RHO_j is rhoend . In this way COBYLA 's iterations behave like a trust region algorithm . Additionally , the linear program may be inconsistent , or the approximation may give poor improvement . For details about how these issues are resolved , as well as how the points v_i are updated , refer to the source code or the references below . References -- -- -- -- -- Powell M.J.D . ( 1994 ) , `` A direct search optimization method that models the objective and constraint functions by linear interpolation . `` , in Advances in Optimization and Numerical Analysis , eds . S. Gomez and J-P Hennart , Kluwer Academic ( Dordrecht ) , pp . 51-67 Powell M.J.D . ( 1998 ) , `` Direct search algorithms for optimization calculations '' , Acta Numerica 7 , 287-336 Powell M.J.D . ( 2007 ) , `` A view of algorithms for optimization without derivatives '' , Cambridge University Technical Report DAMTP 2007 NA03 Examples -- -- -- -- Minimize the objective function f ( x , y ) = x y subject to the constraints x 2 + y 2 < 1 and y > 0 : : > > > def objective ( x ) : ... return x [ 0 ] x [ 1 ] ... > > > def constr1 ( x ) : ... return 1 - ( x [ 0 ] 2 + x [ 1 ] 2 ) ... > > > def constr2 ( x ) : ... return x [ 1 ] ... > > > from scipy.optimize import fmin_cobyla > > > fmin_cobyla ( objective , [ 0.0 , 0.1 ] , [ constr1 , constr2 ] , rhoend=1e-7 ) array ( [ -0.70710685 , 0.70710671 ] ) The exact solution is ( -sqrt ( 2 ) 2 , sqrt ( 2 ) 2 ) . `` '' '' ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/scipy/optimize/cobyla.py#L30,Python,yes,book,algorithm,method,reference,other,paper not available,no
"`` `` '' '' Compute the window-convoled power spectrum multipoles , for a data set with non-trivial survey geometry . This estimator builds upon the work presented in Bianchi et al . 2015 and Scoccimarro et al . 2015 , but differs in the implementation . This class uses the spherical harmonic addition theorem such that only : math : ` 2 ell+1 ` FFTs are required per multipole , rather than the : math : ` ( ell+1 ) ( ell+2 ) 2 ` FFTs in the implementation presented by Bianchi et al . and Scoccimarro et al . References -- -- -- -- -- Bianchi , Davide et al. , ` Measuring line-of-sight-dependent Fourier-space clustering using FFTs ` , MNRAS , 2015 Scoccimarro , Roman , ` Fast estimators for redshift-space clustering ` , Phys . Review D , 2015 `` '' '' ''",https://github.com/bccp/nbodykit/blob/c90c009c9b7ab69447a7199a57605837dad9ae91/nbodykit/algorithms/convpower.py#L404,Python,yes,journal,algorithm,method,reference,data science,formulas to source code,no
"`` Algorithm 3 , page 14 , Nakos , G. C. , Turner , P. R. , Williams , R. M. ( 1997 ) . Fraction-free algorithms for linear and polynomial equations . ACM SIGSAM Bulletin , 31 ( 3 ) , 11‚Äì19 . doi:10.1145 271130.271133 . This algorithms is not a true factorization of the matrix A ( i.e . A = LU ) ) but can be used to solve linear systems by applying forward and backward substitutions respectively . ''",https://github.com/symengine/symengine/blob/b3a2159b7e6950726157626069c30712427d55b2/symengine/dense_matrix.cpp#L964,C++,yes,book,algorithm,method,reference,science,pseudocode to source code,no
"`` This operator implements the terrain flattening algorithm proposed by David Small . For details , see the paper below and the references therein . David Small , `` Flattening Gamma : Radiometric Terrain Correction for SAR imagery '' , IEEE Transaction on Geoscience and Remote Sensing , Vol . 48 , No . 8 , August 2011. ``",https://github.com/senbox-org/s1tbx/blob/b92d2cff73e217d6c19e02e1c7df0f07898f9c63/s1tbx-op-sar-processing/src/main/java/org/esa/s1tbx/sar/gpf/geometric/TerrainFlatteningOp.java#L51,Java,yes,journal,algorithm,class,reference,science,pseudocode to source code,no
"`` < -- globalinfo-start -- > Finds rules according to confirmation measure ( Tertius-type algorithm ) . < br > < br > For more information see : < br > < br > P. A. Flach , N. Lachiche ( 1999 ) . Confirmation-Guided Discovery of first-order rules with Tertius . Machine Learning . 42:61-95 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Flach1999 , author = { P. A. Flach and N. Lachiche } , journal = { Machine Learning } , pages = { 61-95 } , title = { Confirmation-Guided Discovery of first-order rules with Tertius } , volume = { 42 } , year = { 1999 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -K & lt ; number of values in result & gt ; Set maximum number of confirmation values in the result . ( default : 10 ) < pre > < pre > -F & lt ; frequency threshold & gt ; Set frequency threshold for pruning . ( default : 0 ) < pre > < pre > -C & lt ; confirmation threshold & gt ; Set confirmation threshold . ( default : 0 ) < pre > < pre > -N & lt ; noise threshold & gt ; Set noise threshold : maximum frequency of counter-examples . 0 gives only satisfied rules . ( default : 1 ) < pre > < pre > -R Allow attributes to be repeated in a same rule. < pre > < pre > -L & lt ; number of literals & gt ; Set maximum number of literals in a rule . ( default : 4 ) < pre > < pre > -G & lt ; 0=no negation | 1=body | 2=head | 3=body and head & gt ; Set the negations in the rule . ( default : 0 ) < pre > < pre > -S Consider only classification rules. < pre > < pre > -c & lt ; class index & gt ; Set index of class attribute . ( default : last ) . < pre > < pre > -H Consider only horn clauses. < pre > < pre > -E Keep equivalent rules. < pre > < pre > -M Keep same clauses. < pre > < pre > -T Keep subsumed rules. < pre > < pre > -I & lt ; 0=always match | 1=never match | 2=significant & gt ; Set the way to handle missing values . ( default : 0 ) < pre > < pre > -O Use ROC analysis . < pre > < pre > -p & lt ; name of file & gt ; Set the file containing the parts of the individual for individual-based learning. < pre > < pre > -P & lt ; 0=no output | 1=on stdout | 2=in separate window & gt ; Set output of current values . ( default : 0 ) < pre > < -- options-end -- > @ author < a href= '' mailto : adeltour @ netcourrier.com '' > Amelie Deltour < a > @ version $ Revision : 1.8 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_classdiscovery_cleanup/weka/associations/Tertius.java#L65,Java,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` Implementation of selective scheduling approach . The below implementation follows the original approach with the following changes : o the scheduler works after register allocation ( but can be also tuned to work before RA ) ; o some instructions are not copied or register renamed ; o conditional jumps are not moved with code duplication ; o several jumps in one parallel group are not supported ; o when pipelining outer loops , code motion through inner loops is not supported ; o control and data speculation are supported ; o some improvements for better compile time performance were made . Terminology =========== A vinsn , or virtual insn , is an insn with additional data characterizing insn pattern , such as LHS , RHS , register sets used set clobbered , etc . Vinsns also act as smart pointers to save memory by reusing them in different expressions . A vinsn is described by vinsn_t type . An expression is a vinsn with additional data characterizing its properties at some point in the control flow graph . The data may be its usefulness , priority , speculative status , whether it was renamed subsituted , etc . An expression is described by expr_t type . Availability set ( av_set ) is a set of expressions at a given control flow point . It is represented as av_set_t . The expressions in av sets are kept sorted in the terms of expr_greater_p function . It allows to truncate the set while leaving the best expressions . A fence is a point through which code motion is prohibited . On each step , we gather a parallel group of insns at a fence . It is possible to have multiple fences . A fence is represented via fence_t . A boundary is the border between the fence group and the rest of the code . Currently , we never have more than one boundary per fence , as we finalize the fence group when a jump is scheduled . A boundary is represented via bnd_t . High-level overview =================== The scheduler finds regions to schedule , schedules each one , and finalizes . The regions are formed starting from innermost loops , so that when the inner loop is pipelined , its prologue can be scheduled together with yet unprocessed outer loop . The rest of acyclic regions are found using extend_rgns : the blocks that are not yet allocated to any regions are traversed in top-down order , and a block is added to a region to which all its predecessors belong ; otherwise , the block starts its own region . The main scheduling loop ( sel_sched_region_2 ) consists of just scheduling on each fence and updating fences . For each fence , we fill a parallel group of insns ( fill_insns ) until some insns can be added . First , we compute available exprs ( av-set ) at the boundary of the current group . Second , we choose the best expression from it . If the stall is required to schedule any of the expressions , we advance the current cycle appropriately . So , the final group does not exactly correspond to a VLIW word . Third , we move the chosen expression to the boundary ( move_op ) and update the intermediate av sets and liveness sets . We quit fill_insns when either no insns left for scheduling or we have scheduled enough insns so we feel like advancing a scheduling point . Computing available expressions =============================== The computation ( compute_av_set ) is a bottom-up traversal . At each insn , we 're moving the union of its successors ' sets through it via moveup_expr_set . The dependent expressions are removed . Local transformations ( substitution , speculation ) are applied to move more exprs . Then the expr corresponding to the current insn is added . The result is saved on each basic block header . When traversing the CFG , we 're moving down for no more than max_ws insns . Also , we do not move down to ineligible successors ( is_ineligible_successor ) , which include moving along a back-edge , moving to already scheduled code , and moving to another fence . The first two restrictions are lifted during pipelining , which allows us to move insns along a back-edge . We always have an acyclic region for scheduling because we forbid motion through fences . Choosing the best expression ============================ We sort the final availability set via sel_rank_for_schedule , then we remove expressions which are not yet ready ( tick_check_p ) or which dest registers can not be used . For some of them , we choose another register via find_best_reg . To do this , we run find_used_regs to calculate the set of registers which can not be used . The find_used_regs function performs a traversal of code motion paths for an expr . We consider for renaming only registers which are from the same regclass as the original one and using which does not interfere with any live ranges . Finally , we convert the resulting set to the ready list format and use max_issue and reorder hooks similarly to the Haifa scheduler . Scheduling the best expression ============================== We run the move_op routine to perform the same type of code motion paths traversal as in find_used_regs . ( These are working via the same driver , code_motion_path_driver . ) When moving down the CFG , we look for original instruction that gave birth to a chosen expression . We undo the transformations performed on an expression via the history saved in it . When found , we remove the instruction or leave a reg-reg copy speculation check if needed . On a way up , we insert bookkeeping copies at each join point . If a copy is not needed , it will be removed later during this traversal . We update the saved av sets and liveness sets on the way up , too . Finalizing the schedule ======================= When pipelining , we reschedule the blocks from which insns were pipelined to get a tighter schedule . On Itanium , we also perform bundling via the same routine from ia64.c . Dependence analysis changes =========================== We augmented the sched-deps.c with hooks that get called when a particular dependence is found in a particular part of an insn . Using these hooks , we can do several actions such as : determine whether an insn can be moved through another ( has_dependence_p , moveup_expr ) ; find out whether an insn can be scheduled on the current cycle ( tick_check_p ) ; find out registers that are set used clobbered by an insn and find out all the strange stuff that restrict its movement , like SCHED_GROUP_P or CANT_MOVE ( done in init_global_and_expr_for_insn ) . Initialization changes ====================== There are parts of haifa-sched.c , sched-deps.c , and sched-rgn.c that are reused in all of the schedulers . We have split up the initialization of data of such parts into different functions prefixed with scheduler type and postfixed with the type of data initialized : { , sel_ , haifa_ } sched_ { init , finish } , sched_rgn_init finish , sched_deps_init finish , sched_init_ { luids bbs } , etc . The same splitting is done with current_sched_info structure : dependence-related parts are in sched_deps_info , common part is in common_sched_info , and haifa sel etc part is in current_sched_info . Target contexts =============== As we now have multiple-point scheduling , this would not work with backends which save some of the scheduler state to use it in the target hooks . For this purpose , we introduce a concept of target contexts , which encapsulate such information . The backend should implement simple routines of allocating freeing setting such a context . The scheduler calls these as target hooks and handles the target context as an opaque pointer ( similar to the DFA state type , state_t ) . Various speedups ================ As the correct data dependence graph is not supported during scheduling ( which is to be changed in mid-term ) , we cache as much of the dependence analysis results as possible to avoid reanalyzing . This includes : bitmap caches on each insn in stream of the region saying yes no for a query with a pair of UIDs ; hashtables with the previously done transformations on each insn in stream ; a vector keeping a history of transformations on each expr . Also , we try to minimize the dependence context used on each fence to check whether the given expression is ready for scheduling by removing from it insns that are definitely completed the execution . The results of tick_check_p checks are also cached in a vector on each fence . We keep a valid liveness set on each insn in a region to avoid the high cost of recomputation on large basic blocks . Finally , we try to minimize the number of needed updates to the availability sets . The updates happen in two cases : when fill_insns terminates , we advance all fences and increase the stage number to show that the region has changed and the sets are to be recomputed ; and when the next iteration of a loop in fill_insns happens ( but this one reuses the saved av sets on bb headers . ) Thus , we try to break the fill_insns loop only when `` significant '' number of insns from the current scheduling window was scheduled . This should be made a target param . TODO : correctly support the data dependence graph at all stages and get rid of all caches . This should speed up the scheduler . TODO : implement moving cond jumps with bookkeeping copies on both targets . TODO : tune the scheduler before RA so it does not create too much pseudos . References : S.-M . Moon and K. Ebcioglu . Parallelizing nonnumerical code with selective scheduling and software pipelining . ACM TOPLAS , Vol 19 , No . 6 , pages 853 -- 898 , Nov. 1997 . Andrey Belevantsev , Maxim Kuvyrkov , Vladimir Makarov , Dmitry Melnik , and Dmitry Zhurikhin . An interblock VLIW-targeted instruction scheduler for GCC . In Proceedings of GCC Developers ' Summit 2006 . Arutyun Avetisyan , Andrey Belevantsev , and Dmitry Melnik . GCC Instruction Scheduler and Software Pipeliner on the Itanium Platform . EPIC-7 Workshop . http : rogue.colorado.edu EPIC7 . ``",https://github.com/UBERTC/GCC-UBER/blob/33c5ad3da82bf7f49e5f9f6a379a08765dd32b22/gcc/sel-sched.c#L48,C,yes,journal,algorithm,file,reference,other,pseudocode to source code,no
"`` < -- globalinfo-start -- > A meta classifier for handling multi-class datasets with 2-class classifiers by building an ensemble of nested dichotomies. < br > < br > For more info , check < br > < br > Lin Dong , Eibe Frank , Stefan Kramer : Ensembles of Balanced Nested Dichotomies for Multi-class Problems . In : PKDD , 84-95 , 2005. < br > < br > Eibe Frank , Stefan Kramer : Ensembles of nested dichotomies for multi-class problems . In : Twenty-first International Conference on Machine Learning , 2004 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Dong2005 , author = { Lin Dong and Eibe Frank and Stefan Kramer } , booktitle = { PKDD } , pages = { 84-95 } , publisher = { Springer } , title = { Ensembles of Balanced Nested Dichotomies for Multi-class Problems } , year = { 2005 } } & 64 ; inproceedings { Frank2004 , author = { Eibe Frank and Stefan Kramer } , booktitle = { Twenty-first International Conference on Machine Learning } , publisher = { ACM } , title = { Ensembles of nested dichotomies for multi-class problems } , year = { 2004 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -S & lt ; num & gt ; Random number seed . ( default 1 ) < pre > < pre > -I & lt ; num & gt ; Number of iterations . ( default 10 ) < pre > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < pre > -W Full name of base classifier . ( default : weka.classifiers.meta.nestedDichotomies.ND ) < pre > < pre > Options specific to classifier weka.classifiers.meta.nestedDichotomies.ND : < pre > < pre > -S & lt ; num & gt ; Random number seed . ( default 1 ) < pre > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < pre > -W Full name of base classifier . ( default : weka.classifiers.trees.J48 ) < pre > < pre > Options specific to classifier weka.classifiers.trees.J48 : < pre > < pre > -U Use unpruned tree. < pre > < pre > -C & lt ; pruning confidence & gt ; Set confidence threshold for pruning . ( default 0.25 ) < pre > < pre > -M & lt ; minimum number of instances & gt ; Set minimum number of instances per leaf . ( default 2 ) < pre > < pre > -R Use reduced error pruning. < pre > < pre > -N & lt ; number of folds & gt ; Set number of folds for reduced error pruning . One fold is used as pruning set . ( default 3 ) < pre > < pre > -B Use binary splits only. < pre > < pre > -S Do n't perform subtree raising. < pre > < pre > -L Do not clean up after the tree has been built. < pre > < pre > -A Laplace smoothing for predicted probabilities. < pre > < pre > -Q & lt ; seed & gt ; Seed for random data shuffling ( default 1 ) . < pre > < -- options-end -- > Options after -- are passed to the designated classifier. < p > @ author Eibe Frank @ author Lin Dong @ version $ Revision : 1.7 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_revisionhandler/weka/classifiers/meta/END.java#L40,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` 80 Purpose : DUNAVANT_SUBRULE_14 returns a compressed Dunavant rule 14 . Licensing : This code is distributed under the GNU LGPL license . Modified : 11 December 2006 Author : John Burkardt Reference : David Dunavant , High Degree Efficient Symmetrical Gaussian Quadrature Rules for the Triangle , International Journal for Numerical Methods in Engineering , Volume 21 , 1985 , pages 1129-1148 . James Lyness , Dennis Jespersen , Moderate Degree Symmetric Quadrature Rules for the Triangle , Journal of the Institute of Mathematics and its Applications , Volume 15 , Number 1 , February 1975 , pages 19-32 . Parameters : Input , int SUBORDER_NUM , the number of suborders of the rule . Output , double SUBORDER_XYZ [ 3 SUBORDER_NUM ] , the barycentric coordinates of the abscissas . Output , double SUBORDER_W [ SUBORDER_NUM ] , the suborder weights. ``",https://github.com/live-clones/dolfin/blob/474ca26489403e8fe8e5006a836bc9c9dc5194a3/dolfin/geometry/SimplexQuadrature.cpp#L2384,C++,yes,journal,algorithm,method,reference,other,paper not available,no
"`` jfdctint.c Copyright ( C ) 1991-1994 , Thomas G. Lane . This file is part of the Independent JPEG Group 's software . For conditions of distribution and use , see the accompanying README file . This file contains a slow-but-accurate integer implementation of the forward DCT ( Discrete Cosine Transform ) . A 2-D DCT can be done by 1-D DCT on each row followed by 1-D DCT on each column . Direct algorithms are also available , but they are much more complex and seem not to be any faster when reduced to code . This implementation is based on an algorithm described in C. Loeffler , A. Ligtenberg and G. Moschytz , `` Practical Fast 1-D DCT Algorithms with 11 Multiplications '' , Proc . Int ' l . Conf . on Acoustics , Speech , and Signal Processing 1989 ( ICASSP '89 ) , pp . 988-991 . The primary algorithm described there uses 11 multiplies and 29 adds . We use their alternate method with 12 multiplies and 32 adds . The advantage of this method is that no data path contains more than one multiplication ; this allows a very simple and accurate implementation in scaled fixed-point arithmetic , with a minimal number of shifts. ``",https://github.com/rose-compiler/rose-develop/blob/5293d2605afa9dd5831e1fb4f50a350378ccfc6c/projects/SATIrE/testsuite/mrtc/success/jfdctint.c#L74,C,yes,conference,algorithm,file,reference,other,formulas to source code,no
"`` UNIX password , and DES , encryption . By Tom Truscott , trt @ rti.rti.org , from algorithms by Robert W. Baldwin and James Gillogly . References : `` Mathematical Cryptology for Computer Scientists and Mathematicians , '' by Wayne Patterson , 1987 , ISBN 0-8476-7438-X . `` Password Security : A Case History , '' R. Morris and Ken Thompson , Communications of the ACM , vol . 22 , pp . 594-597 , Nov. 1979 . `` DES will be Totally Insecure within Ten Years , '' M.E . Hellman , IEEE Spectrum , vol . 16 , pp . 32-39 , July 1979. ``",https://github.com/FirebirdSQL/firebird/blob/7101efb5e4387226585cce8705d71c54679ccc86/src/common/enc.cpp#L66,C++,yes,book,algorithm,file,reference,other,paper not available,no
"`` @ author hfq based on R. B. Rusu , Z. C. Marton , N. Blodow , M. Dolha , and M. Beetz Towards 3D Point Cloud Based Object Maps for Household Environments Robotics and Autonomous Systems Journal ( Special Issue on Semantic Knowledge ) , 2008 . The algorithm iterates through the entire point cloud twice . On the first iteration it will compute the average distance that each point has to its k nearest neighbors Next , the mean and standard deviation of all the distances are computed in order to determine a distance threshold . The distance threshold will be = mean + stdMul stddev . During the next iteration the point will be classified as inlier or outlier if their average distance is below or above this threshold respectively ``",https://github.com/LSTS/neptus/blob/e944103797c55873126d0f897daf373051f2e541/src/pt/lsts/neptus/vtk/filters/StatisticalOutlierRemoval.java#L41,Java,yes,journal,algorithm,class,reference,other,formulas to source code,no
"`` `` '' '' Spatial transformer layer The layer applies an affine transformation on the input . The affine transformation is parameterized with six learned parameters [ 1 ] _ . The output is interpolated with a bilinear transformation . Parameters -- -- -- -- -- incoming : a : class : ` Layer ` instance or a tuple The layer feeding into this layer , or the expected input shape . The output of this layer should be a 4D tensor , with shape `` ( batch_size , num_input_channels , input_rows , input_columns ) `` . localization_network : a : class : ` Layer ` instance The network that calculates the parameters of the affine transformation . See the example for how to initialize to the identity transform . downsample_factor : float or iterable of float A float or a 2-element tuple specifying the downsample factor for the output image ( in both spatial dimensions ) . A value of 1 will keep the original size of the input . Values larger than 1 will downsample the input . Values below 1 will upsample the input . border_mode : 'nearest ' , 'mirror ' , or 'wrap ' Determines how border conditions are handled during interpolation . If 'nearest ' , points outside the grid are clipped to the boundary . If 'mirror ' , points are mirrored across the boundary . If 'wrap ' , points wrap around to the other side of the grid . See http : stackoverflow.com q 22669252 22670830 22670830 for details . References -- -- -- -- -- .. [ 1 ] Max Jaderberg , Karen Simonyan , Andrew Zisserman , Koray Kavukcuoglu ( 2015 ) : Spatial Transformer Networks . NIPS 2015 , http : papers.nips.cc paper 5854-spatial-transformer-networks.pdf Examples -- -- -- -- Here we set up the layer to initially do the identity transform , similarly to [ 1 ] _ . Note that you will want to use a localization with linear output . If the output from the localization networks is [ t1 , t2 , t3 , t4 , t5 , t6 ] then t1 and t5 determines zoom , t2 and t4 determines skewness , and t3 and t6 move the center position . > > > import numpy as np > > > import lasagne > > > b = np.zeros ( ( 2 , 3 ) , dtype='float32 ' ) > > > b [ 0 , 0 ] = 1 > > > b [ 1 , 1 ] = 1 > > > b = b.flatten ( ) identity transform > > > W = lasagne.init.Constant ( 0.0 ) > > > l_in = lasagne.layers.InputLayer ( ( None , 3 , 28 , 28 ) ) > > > l_loc = lasagne.layers.DenseLayer ( l_in , num_units=6 , W=W , b=b , ... nonlinearity=None ) > > > l_trans = lasagne.layers.TransformerLayer ( l_in , l_loc ) `` '' '' ''",https://github.com/Lasagne/Lasagne/blob/7992faa80fa5233a786e2582a605e854cea7d1cf/lasagne/layers/special.py#L355,Python,yes,conference,algorithm,class,reference,machine learning,formulas to source code,no
"`` < -- globalinfo-start -- > Classifier for building 'Functional trees ' , which are classification trees that could have logistic regression functions at the inner nodes and or leaves . The algorithm can deal with binary and multi-class target variables , numeric and nominal attributes and missing values. < br > < br > For more information see : < br > < br > Joao Gama ( 2004 ) . Functional Trees. < br > < br > Niels Landwehr , Mark Hall , Eibe Frank ( 2005 ) . Logistic Model Trees . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Gama2004 , author = { Joao Gama } , booktitle = { Machine Learning } , number = { 3 } , pages = { 219-250 } , title = { Functional Trees } , volume = { 55 } , year = { 2004 } } & 64 ; article { Landwehr2005 , author = { Niels Landwehr and Mark Hall and Eibe Frank } , booktitle = { Machine Learning } , number = { 1-2 } , pages = { 161-205 } , title = { Logistic Model Trees } , volume = { 95 } , year = { 2005 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -B Binary splits ( convert nominal attributes to binary ones ) < pre > < pre > -P Use error on probabilities instead of misclassification error for stopping criterion of LogitBoost. < pre > < pre > -I & lt ; numIterations & gt ; Set fixed number of iterations for LogitBoost ( instead of using cross-validation ) < pre > < pre > -F & lt ; modelType & gt ; Set Funtional Tree type to be generate : 0 for FT , 1 for FTLeaves and 2 for FTInner < pre > < pre > -M & lt ; numInstances & gt ; Set minimum number of instances at which a node can be split ( default 15 ) < pre > < pre > -W & lt ; beta & gt ; Set beta for weight trimming for LogitBoost . Set to 0 ( default ) for no weight trimming. < pre > < pre > -A The AIC is used to choose the best iteration. < pre > < -- options-end -- > @ author Jo ~ { a } o Gama @ author Carlos Ferreira @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-7-11/packages/external/functionalTrees/src/main/java/weka/classifiers/trees/FT.java#L53,Java,yes,journal,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` < -- globalinfo-start -- > Class for building and using a Discriminative Multinomial Naive Bayes classifier . For more information see , < br > < br > Jiang Su , Harry Zhang , Charles X. Ling , Stan Matwin : Discriminative Parameter Learning for Bayesian Networks . In : ICML 2008 ' , 2008. < br > < br > The core equation for this classifier : < br > < br > P [ Ci|D ] = ( P [ D|Ci ] x P [ Ci ] ) P [ D ] ( Bayes rule ) < br > < br > where Ci is class i and D is a document . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { JiangSu2008 , author = { Jiang Su , Harry Zhang , Charles X. Ling , Stan Matwin } , booktitle = { ICML 2008 ' } , title = { Discriminative Parameter Learning for Bayesian Networks } , year = { 2008 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < -- options-end -- > @ author Jiang Su ( Jiang.Su @ unb.ca ) 2008 @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_scripting_upgrade/weka/src/main/java/weka/classifiers/bayes/DMNBtext.java#L42,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` def compute_rand_index ( self , test_labels , ground_truth_labels , mask ) : `` '' '' Calculate the Rand Index http : en.wikipedia.org wiki Rand_index Given a set of N elements and two partitions of that set , X and Y A = the number of pairs of elements in S that are in the same set in X and in the same set in Y B = the number of pairs of elements in S that are in different sets in X and different sets in Y C = the number of pairs of elements in S that are in the same set in X and different sets in Y D = the number of pairs of elements in S that are in different sets in X and the same set in Y The rand index is : A + B -- -- - A+B+C+D The adjusted rand index is the rand index adjusted for chance so as not to penalize situations with many segmentations . Jorge M. Santos , Mark Embrechts , `` On the Use of the Adjusted Rand Index as a Metric for Evaluating Supervised Classification '' , Lecture Notes in Computer Science , Springer , Vol . 5769 , pp . 175-184 , 2009 . Eqn 6 ExpectedIndex = best possible score ExpectedIndex = sum ( N_i choose 2 ) sum ( N_j choose 2 ) MaxIndex = worst possible score = 1 2 ( sum ( N_i choose 2 ) + sum ( N_j choose 2 ) ) total A total - ExpectedIndex -- -- -- -- -- -- -- -- -- -- -- -- - MaxIndex - ExpectedIndex returns a tuple of the Rand Index and the adjusted Rand Index `` '' '' ground_truth_labels = ground_truth_labels [ mask ] .astype ( numpy.uint64 ) test_labels = test_labels [ mask ] .astype ( numpy.uint64 ) if len ( test_labels ) > 0 : Create a sparse matrix of the pixel labels in each of the sets The matrix , N ( i , j ) gives the counts of all of the pixels that were labeled with label I in the ground truth and label J in the test set . N_ij = scipy.sparse.coo_matrix ( ( numpy.ones ( len ( test_labels ) ) , ( ground_truth_labels , test_labels ) ) ) .toarray ( ) def choose2 ( x ) : `` 'Compute of pairs of x things = x ( x-1 ) 2 '' ' return x ( x - 1 ) 2 Each cell in the matrix is a count of a grouping of pixels whose pixel pairs are in the same set in both groups . The number of pixel pairs is n ( n - 1 ) , so A = sum ( matrix ( matrix - 1 ) ) A = numpy.sum ( choose2 ( N_ij ) ) B is the sum of pixels that were classified differently by both sets . But the easier calculation is to find A , C and D and get B by subtracting A , C and D from the N ( N - 1 ) , the total number of pairs . For C , we take the number of pixels classified as `` i '' and for each `` j '' , subtract N ( i , j ) from N ( i ) to get the number of pixels in N ( i , j ) that are in some other set = ( N ( i ) - N ( i , j ) ) N ( i , j ) We do the similar calculation for D N_i = numpy.sum ( N_ij , 1 ) N_j = numpy.sum ( N_ij , 0 ) C = numpy.sum ( ( N_i [ : , numpy.newaxis ] - N_ij ) N_ij ) 2 D = numpy.sum ( ( N_j [ numpy.newaxis , : ] - N_ij ) N_ij ) 2 total = choose2 ( len ( test_labels ) ) an astute observer would say , why bother computing A and B when all we need is A+B and C , D and the total can be used to do that . The calculations are n't too expensive , though , so I do them . B = total - A - C - D rand_index = ( A + B ) total Compute adjusted Rand Index expected_index = numpy.sum ( choose2 ( N_i ) ) numpy.sum ( choose2 ( N_j ) ) max_index = ( numpy.sum ( choose2 ( N_i ) ) + numpy.sum ( choose2 ( N_j ) ) ) total 2 adjusted_rand_index = ( A total - expected_index ) ( max_index - expected_index ) else : rand_index = adjusted_rand_index = numpy.nan return rand_index , adjusted_rand_index ''",https://github.com/CellProfiler/CellProfiler/blob/6a349582e5923f69d6c544b60765404995256044/cellprofiler/modules/measureobjectoverlap.py#L461,Python,yes,conference,algorithm,method,reference,computer vision,formulas to source code,no
"`` REFERENCES : Chiasson , A . Advances in Modeling of Ground-Source Heat Pump Systems . M.S . Thesis , Oklahoma State University , December 1999 . Chiasson , A.D. , J.D . Spitler , S.J . Rees , M.D . Smith . 2000 . A Model For Simulating The Performance Of A Shallow Pond As A Supplemental Heat Rejecter With Closed-Loop Ground-Source Heat Pump Systems . ASHRAE Transactions . 106 ( 2 ) :107-121 . ''",https://github.com/NREL/EnergyPlus/blob/b89c060c96ebac032d8d93647e94cf73e8cac8bc/src/EnergyPlus/PondGroundHeatExchanger.cc#L109,C++,yes,thesis,background,n/a,related,simulation,no transfer,no
"`` @ see J. Daniels , The Conditioning for Distance Running -- the Scientific Aspects , John Wiley & Sons , New York , 1978 @ see T. Noakes , Lore of Running ( 4th edition ) , Oxford University Press Southern Africa , 2001 @ see http : www.simpsonassociatesinc.com runningmath3.htm @ see http : www.had2know.com health vo2-max-calculator-racing-daniels-gilbert.html ``",https://github.com/Runalyze/Runalyze/blob/75479fa2b0e6d58d8a71452fc338f097a22de619/inc/core/Sports/Running/VO2max/Estimation/DanielsGilbertFormula.php#L5,PHP,yes,book,background,n/a,related,data science,no transfer,no
"`` `` '' '' compute the composed sum `` prod ( p2 ( x - beta ) for beta root of p1 ) `` Examples ======== > > > R , x = ring ( ' x ' , QQ ) > > > f = x 2 - 2 > > > g = x 2 - 3 > > > rs_compose_add ( f , g ) x 4 - 10 x 2 + 1 References ========== .. [ 1 ] A. Bostan , P. Flajolet , B. Salvy and E. Schost `` Fast Computation with Two Algebraic Numbers '' , ( 2002 ) Research Report 4579 , Institut National de Recherche en Informatique et en Automatique. `` '' '' ''",https://github.com/diofant/diofant/blob/4d9f43afde5cae9c6c20890e4ee7352f3f296166/diofant/polys/ring_series.py#L511,Python,yes,report,algorithm,method,reference,science,formulas to source code,no
"`` Computes the PN Coefficients for using in the TaylorT2 timing equation . Terms given in equation 3.8b of : Alessandra Buonanno , Bala R Iyer , Evan Ochsner , Yi Pan , and B S Sathyaprakash , `` Comparison of post-Newtonian templates for compact binary inspiral signals in gravitational-wave detectors '' , Phys . Rev . D 80 , 084043 ( 2009 ) , arXiv:0907.0700v1 ``",https://github.com/lscsoft/lalsuite/blob/8cbd1b7187ce3ed9a825d6ed11cc432f3cfde9a5/lalsimulation/src/LALSimInspiralPNCoefficients.c#L1120,C,404,,,,,,,
"`` `` Superpixel Segmentation using Linear Spectral Clustering '' Zhengqin Li , Jiansheng Chen , IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , Jun . 2015 OpenCV port by : Cristian Balint < cristian dot balint at gmail dot com > ``",https://github.com/cpvrlab/SLProject/blob/90e5a60b550a44489654386458d35457940c8b73/lib-SLExternal/opencv/include/opencv2/ximgproc/lsc.hpp#L37,C++,yes,conference,numbers,class,reference,computer vision,numbers to hard coded values,no
"`` extended_p_square_impl multiple quantile estimation @ brief Multiple quantile estimation with the extended f $ P^2 f $ algorithm Extended f $ P^2 f $ algorithm for estimation of several quantiles without storing samples . Assume that f $ m f $ quantiles f $ xi_ { p_1 } , ldots , xi_ { p_m } f $ are to be estimated . Instead of storing the whole sample cumulative distribution , the algorithm maintains only f $ m+2 f $ principal markers and f $ m+1 f $ middle markers , whose positions are updated with each sample and whose heights are adjusted ( if necessary ) using a piecewise-parablic formula . The heights of these central markers are the current estimates of the quantiles and returned as an iterator range . For further details , see K. E. E. Raatikainen , Simultaneous estimation of several quantiles , Simulation , Volume 49 , Number 4 ( October ) , 1986 , p. 159-164 . The extended f $ P^2 f $ algorithm generalizess the f $ P^2 f $ algorithm of R. Jain and I. Chlamtac , The P^2 algorithmus for dynamic calculation of quantiles and histograms without storing observations , Communications of the ACM , Volume 28 ( October ) , Number 10 , 1985 , p. 1076-1085 . @ param extended_p_square_probabilities A vector of quantile probabilities. ``",https://github.com/humeaua/CVATools/blob/405f7caaed597a63a63811d9fc3ff7759f4b1ba0/CVATools/include/boost/accumulators/statistics/extended_p_square.hpp#L39,C++,yes,journal,algorithm,method,reference,other,paper not available,no
"`` .NAME vtkKochanekSpline - computes an interpolating spline using a Kochanek basis . .SECTION Description Implements the Kochenek interpolating spline described in : Kochanek , D. , Bartels , R. , `` Interpolating Splines with Local Tension , Continuity , and Bias Control , '' Computer Graphics , vol . 18 , no . 3 , pp . 33-41 , July 1984 . These splines give the user more control over the shape of the curve than the cardinal splines implemented in vtkCardinalSpline . Three parameters can be specified . All have a range from -1 to 1 . Tension controls how sharply the curve bends at an input point . A value of -1 produces more slack in the curve . A value of 1 tightens the curve . Continuity controls the continuity of the first derivative at input points . Bias controls the direction of the curve at it passes through an input point . A value of -1 undershoots the point while a value of 1 overshoots the point . These three parameters give the user broad control over the shape of the interpolating spline . The original Kochanek paper describes the effects nicely and is recommended reading . ''",https://github.com/mc01104/CTR/blob/dbc514c371b96db96a492d281d3e904a26fa6459/Lib/vtk/include/vtk-5.8/vtkKochanekSpline.h#L15,C++,yes,conference,algorithm,file,reference,other,formulas to source code,no
"`` class TSpectrumTransform ingroup Spectrum brief Advanced 1-dimensional orthogonal transform functions author Miroslav Morhac Class to carry out transforms of 1D spectra , its filtering and enhancement . It allows to calculate classic Fourier , Cosine , Sin , Hartley , Walsh , Haar transforms as well as mixed transforms ( Fourier- Walsh , Fourier-Haar , Walsh-Haar , Cosine-Walsh , Cosine-Haar , Sin-Walsh and Sin-Haar ) . All the transforms are fast . The algorithms in this class have been published in the following references : 1 . C.V. Hampton , B. Lian , Wm . C. McHarris : Fast-Fourier-transform spectral enhancement techniques for gamma-ray spectroscopy.NIM A353 ( 1994 ) 280-284 . 2 . Morhac M. , Matousek V. , New adaptive Cosine-Walsh transform and its application to nuclear data compression , IEEE Transactions on Signal Processing 48 ( 2000 ) 2693 . 3 . Morhac M. , Matousek V. , Data compression using new fast adaptive Cosine-Haar transforms , Digital Signal Processing 8 ( 1998 ) 63 . 4 . Morhac M. , Matousek V. : Multidimensional nuclear data compression using fast adaptive Walsh-Haar transform . Acta Physica Slovaca 51 ( 2001 ) 307. ``",https://github.com/root-project/root/blob/8921a1c18564d4a606d11a1173f42c7df8501dfd/hist/spectrum/src/TSpectrumTransform.cxx#L4,C++,yes,journal,algorithm,class,reference,data science,formulas to source code,no
"`` Compare the given input string and length against a table of known C storage qualifier keywords . We just ignore these in ctf_lookup_by_name , below . To do this quickly , we use a pre-computed Perfect Hash Function similar to the technique originally described in the classic paper : R.J. Cichelli , `` Minimal Perfect Hash Functions Made Simple '' , Communications of the ACM , Volume 23 , Issue 1 , January 1980 , pp . 17-19 . For an input string S of length N , we use hash H = S [ N - 1 ] + N - 105 , which for the current set of qualifiers yields a unique H in the range [ 0 .. 20 ] . The hash can be modified when the keyword set changes as necessary . We also store the length of each keyword and check it prior to the final strcmp ( ) . ``",https://github.com/freenas/os/blob/0e9eb48d96a1edb197c779609b5a889f5e8306cf/cddl/contrib/opensolaris/common/ctf/ctf_lookup.c#L33,C,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` double dln1px ( double a ) Double precision LN ( 1+X ) Function Returns ln ( 1+x ) Note that the obvious code of LOG ( 1.0+X ) wo n't work for small X because 1.0+X loses accuracy Arguments X -- > Value for which ln ( 1-x ) is desired . X is DOUBLE PRECISION Method Renames ALNREL from : DiDinato , A. R. and Morris , A. H. Algorithm 708 : Significant Digit Computation of the Incomplete Beta Function Ratios . ACM Trans . Math . Softw . 18 ( 1993 ) , 360-373 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - EVALUATION OF THE FUNCTION LN ( 1 + A ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ``",https://github.com/afni/afni/blob/f8eb05c3bd6c404324ad82ab3ae2cf5e9a8dcc0b/src/nifti/nifticdf/nifticdf.c#L6590,C,yes,journal,algorithm,method,reference,computer vision,formulas to source code,no
"`` Reference M. Matsumoto and T. Nishimura , `` Mersenne Twister : A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator '' , ACM Transactions on Modeling and Computer Simulation , Vol . 8 , No . 1 , January 1998 , pp 3-30 . ''",https://github.com/mauge123/mechanical-blender/blob/833f0906b75249c2048a20f24c359d620757e446/intern/smoke/intern/MERSENNETWISTER.h#L16,C,yes,journal,algorithm,file,reference,computer vision,pseudocode to source code,no
"`` `` '' '' Creates a map of the mode of the diffusion tensors given a set of diffusion-weighted images , as well as their associated b-values and b-vectors . Fits the diffusion tensors and calculates tensor mode with Dipy . .. [ 1 ] Daniel B. Ennis and G. Kindlmann , `` Orthogonal Tensor Invariants and the Analysis of Diffusion Tensor Magnetic Resonance Images '' , Magnetic Resonance in Medicine , vol . 55 , no . 1 , pp . 136-146 , 2006 . Example -- -- -- - > > > import nipype.interfaces.dipy as dipy > > > mode = dipy.TensorMode ( ) > > > mode.inputs.in_file = 'diffusion.nii ' > > > mode.inputs.in_bvec = 'bvecs ' > > > mode.inputs.in_bval = 'bvals ' > > > mode.run ( ) doctest : +SKIP `` '' '' ''",https://github.com/nipy/nipype/blob/53e0662534a8e4a02d5f3d7e24c8d5eb49dedb4e/nipype/interfaces/dipy/tensors.py#L92,Python,yes,journal,algorithm,class,reference,science,formulas to source code,no
"`` - Copyright ( c ) 1995 by David Bagley . Permission to use , copy , modify , and distribute this software and its documentation for any purpose and without fee is hereby granted , provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation . This file is provided AS IS with no warranties of any kind . The author shall have no liability with respect to the infringement of copyrights , trade secrets or any patents by this file or any part thereof . In no event will the author be liable for any lost revenue or profits or other special , indirect and consequential damages . Revision History : 01-Nov-2000 : Allocation checks 10-May-1997 : Compatible with xscreensaver 16-Apr-1997 : -neighbors 3 and 8 added 01-Jan-1997 : Updated ant.c to handle more kinds of ants . Thanks to J Austin David < Austin.David @ tlogic.com > . Check it out in java at http : havoc.gtf.gatech.edu austin He thought up the new Ladder ant . 04-Apr-1996 : -neighbors 6 runtime-time option added for hexagonal ants ( bees ) , coded from an idea of Jim Propp 's in Science News , Oct 28 , 1995 VOL . 148 page 287 20-Sep-1995 : Memory leak in ant fixed . Now random colors . 05-Sep-1995 : Coded from A.K . Dewdney 's `` Computer Recreations '' , Scientific American Magazine '' Sep 1989 pp 180-183 , Mar 1990 p 121 Also used Ian Stewart 's Mathematical Recreations , Scientific American Jul 1994 pp 104-107 also used demon.c and life.c as a guide. ``",https://github.com/openbsd/xenocara/blob/54917569e63ba4dd5a90a324998cb3c0c3824eda/app/xlockmore/modes/ant.c#L12,C,yes,other (news),algorithm,file,reference,other,paper not available,no
"`` `` '' '' Factor analysis Parameters -- -- -- -- -- endog : array-like Variables in columns , observations in rows . May be ` None ` if ` corr ` is not ` None ` . n_factor : int The number of factors to extract corr : array-like Directly specify the correlation matrix instead of estimating it from ` endog ` . If provided , ` endog ` is not used for the factor analysis , it may be used in post-estimation . method : str The method to extract factors , currently must be either 'pa ' for principal axis factor analysis or 'ml ' for maximum likelihood estimation . smc : True or False Whether or not to apply squared multiple correlations ( method='pa ' ) endog_names : str Names of endogeous variables . If specified , it will be used instead of the column names in endog nobs : int The number of observations , not used if endog is present . Needs to be provided for inference if endog is None . missing : 'none ' , 'drop ' , or 'raise ' Missing value handling for endog , default is row-wise deletion 'drop ' If 'none ' , no nan checking is done . If 'drop ' , any observations with nans are dropped . If 'raise ' , an error is raised . Notes -- -- - Experimental Supported rotations : 'varimax ' , 'quartimax ' , 'biquartimax ' , 'equamax ' , 'oblimin ' , 'parsimax ' , 'parsimony ' , 'biquartimin ' , 'promax ' If method='ml ' , the factors are rotated to satisfy condition IC3 of Bai and Li ( 2012 ) . This means that the scores have covariance I , so the model for the covariance matrix is L L ' + diag ( U ) , where L are the loadings and U are the uniquenesses . In addition , L ' diag ( U ) ^ { -1 } L must be diagonal . References -- -- -- -- -- .. [ ] Hofacker , C. ( 2004 ) . Exploratory Factor Analysis , Mathematical Marketing . http : www.openaccesstexts.org pdf Quant_Chapter_11_efa.pdf .. [ ] J Bai , K Li ( 2012 ) . Statistical analysis of factor models of high dimension . Annals of Statistics . https : arxiv.org pdf 1205.6617.pdf `` '' '' ''",https://github.com/statsmodels/statsmodels/blob/d465ebc54533afe2d003ca72287ca866498eb399/statsmodels/multivariate/factor.py#L59,Python,yes,book,algorithm,class,reference,science,formulas to source code,no
"`` Class for building and using a PRISM classifier . For more information , see < p > J. Cendrowska ( 1987 ) . < i > PRISM : An algorithm for inducing modular rules < i > . International Journal of Man-Machine Studies . Vol.27 , No.4 , pp.349-370. < p > @ author Ian H. Witten ( ihw @ cs.waikato.ac.nz ) @ version $ Revision : 1.8.2.1 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/branches/stable-3-0-patches/weka/classifiers/Prism.java#L25,Java,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` `` '' '' Mode ( MO ) of a diffusion tensor [ 1 ] _. Parameters -- -- -- -- -- q_form : ndarray The quadratic form of a tensor , or an array with quadratic forms of tensors . Should be of shape ( x , y , z , 3 , 3 ) or ( n , 3 , 3 ) or ( 3 , 3 ) . Returns -- -- -- - mode : array Calculated tensor mode in each spatial coordinate . Notes -- -- - Mode ranges between -1 ( planar anisotropy ) and +1 ( linear anisotropy ) with 0 representing orthotropy . Mode is calculated with the following equation ( equation 9 in [ 1 ] _ ) : .. math : : Mode = 3 sqrt { 6 } det ( widetilde { A } norm ( widetilde { A } ) ) Where $ widetilde { A } $ is the deviatoric part of the tensor quadratic form . References -- -- -- -- -- .. [ 1 ] Daniel B. Ennis and G. Kindlmann , `` Orthogonal Tensor Invariants and the Analysis of Diffusion Tensor Magnetic Resonance Images '' , Magnetic Resonance in Medicine , vol . 55 , no . 1 , pp . 136-146 , 2006. `` '' '' ''",https://github.com/nipy/dipy/blob/6bee25c1d25acc1437752be0e166acce42bfed84/dipy/reconst/dti.py#L459,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` ================ Volumetric Hair Interaction ================ adapted from Volumetric Methods for Simulation and Rendering of Hair ( Petrovic , Henne , Anderson , Pixar Technical Memo 06-08 , Pixar Animation Studios ) as well as `` Detail Preserving Continuum Simulation of Straight Hair '' ( McAdams , Selle 2009 ) ``",https://github.com/UPBGE/blender/blob/fb9d84ff7449a352c364fdd0914ca46d9477a739/source/blender/physics/intern/hair_volume.cpp#L46,C,yes,other (technical memo),algorithm,file,reference,computer vision,formulas to source code,no
"`` Following Float to Half-Float converion code is from the implementation of ftp : www.fox-toolkit.org pub fasthalffloatconversion.pdf , `` Fast Half Float Conversions '' by Jeroen van der Zijp , November 2008 ( Revised September 2010 ) . Specially , the basetable [ 512 ] and shifttable [ 512 ] are generated as follows : unsigned short basetable [ 512 ] ; unsigned char shifttable [ 512 ] ; void generatetables ( ) { unsigned int i ; int e ; for ( i = 0 ; i < 256 ; ++i ) { e = i - 127 ; if ( e < -24 ) { Very small numbers map to zero basetable [ i | 0x000 ] = 0x0000 ; basetable [ i | 0x100 ] = 0x8000 ; shifttable [ i | 0x000 ] = 24 ; shifttable [ i | 0x100 ] = 24 ; } else if ( e < -14 ) { Small numbers map to denorms basetable [ i | 0x000 ] = ( 0x0400 > > ( -e-14 ) ) ; basetable [ i | 0x100 ] = ( 0x0400 > > ( -e-14 ) ) | 0x8000 ; shifttable [ i | 0x000 ] = -e-1 ; shifttable [ i | 0x100 ] = -e-1 ; } else if ( e < = 15 ) { Normal numbers just lose precision basetable [ i | 0x000 ] = ( ( e+15 ) < < 10 ) ; basetable [ i| 0x100 ] = ( ( e+15 ) < < 10 ) | 0x8000 ; shifttable [ i|0x000 ] = 13 ; shifttable [ i|0x100 ] = 13 ; } else if ( e < 128 ) { Large numbers map to Infinity basetable [ i|0x000 ] = 0x7C00 ; basetable [ i|0x100 ] = 0xFC00 ; shifttable [ i|0x000 ] = 24 ; shifttable [ i|0x100 ] = 24 ; } else { Infinity and NaN 's stay Infinity and NaN 's basetable [ i|0x000 ] = 0x7C00 ; basetable [ i|0x100 ] = 0xFC00 ; shifttable [ i|0x000 ] = 13 ; shifttable [ i|0x100 ] = 13 ; } } } ``",https://github.com/qt/qtwebengine-chromium/blob/b45f07bfbe74c333f1017810c2409e1aa6077a1b/chromium/third_party/WebKit/Source/platform/graphics/GraphicsContext3DImagePacking.cpp#L120,C++,yes,report,algorithm,method,reference,other,description to source code,no
"`` Balancing function as described by B. N. Parlett and C. Reinsch , `` Balancing a Matrix for Calculation of Eigenvalues and Eigenvectors '' . In : Numerische Mathematik , Volume 13 , Number 4 ( 1969 ) , 293-304 , Springer Berlin Heidelberg . DOI : 10.1007 BF02165404 ''",https://github.com/lukastoenne/blender/blob/4d5cd638ffba6854bfcee2ee7eb836f446b087ca/extern/ceres/internal/ceres/polynomial.cc#L51,C,yes,journal,algorithm,method,reference,computer vision,paper not available,no
"`` Sorts the specified array of objects into ascending order , according to the { @ linkplain Comparable natural ordering } of its elements . All elements in the array must implement the { @ link Comparable } interface . Furthermore , all elements in the array must be < i > mutually comparable < i > ( that is , { @ code e1.compareTo ( e2 ) } must not throw a { @ code ClassCastException } for any elements { @ code e1 } and { @ code e2 } in the array ) . < p > This sort is guaranteed to be < i > stable < i > : equal elements will not be reordered as a result of the sort . < p > Implementation note : This implementation is a stable , adaptive , iterative mergesort that requires far fewer than n lg ( n ) comparisons when the input array is partially sorted , while offering the performance of a traditional mergesort when the input array is randomly ordered . If the input array is nearly sorted , the implementation requires approximately n comparisons . Temporary storage requirements vary from a small constant for nearly sorted input arrays to n 2 object references for randomly ordered input arrays . < p > The implementation takes equal advantage of ascending and descending order in its input array , and can take advantage of ascending and descending order in different parts of the the same input array . It is well-suited to merging two or more sorted arrays : simply concatenate the arrays and sort the resulting array . < p > The implementation was adapted from Tim Peters 's list sort for Python ( < a href= '' http : svn.python.org projects python trunk Objects listsort.txt '' > TimSort < a > ) . It uses techiques from Peter McIlroy 's `` Optimistic Sorting and Information Theoretic Complexity '' , in Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms , pp 467-474 , January 1993 . @ param a the array to be sorted @ throws ClassCastException if the array contains elements that are not < i > mutually comparable < i > ( for example , strings and integers ) @ throws IllegalArgumentException ( optional ) if the natural ordering of the array elements is found to violate the { @ link Comparable } contract ``",https://github.com/google/j2objc/blob/640f81e761b8f360c5ac8189ca624cad62b3452d/jre_emul/android/platform/libcore/ojluni/src/main/java/java/util/Arrays.java#L1213,Java,yes,conference,algorithm,method,reference,other,formulas to source code,no
"`` Implementation of Schnorr signatures zero-knowledge proofs , based on description in : F. Hao , P. Ryan , `` Password Authenticated Key Exchange by Juggling '' , 16th Workshop on Security Protocols , Cambridge , April 2008 http : grouper.ieee.org groups 1363 Research contributions hao-ryan-2008.pdf ``",https://github.com/UCSD-PL/kraken/blob/f47735cb931669e88b633852cb013fd0032707d4/examples/ssh-proto/openssh-6.1p1/schnorr.c#L18,C,yes,conference,algorithm,file,reference,other,formulas to source code,no
"`` `` '' '' Implements Annealed Importance Sampling for Binary-Binary RBMs Parameters -- -- -- -- -- rbm_params : list list of ` numpy.ndarrays ` containing model parameters : [ weights , visbias , hidbias ] n_runs : int Number of particles to use in AIS simulation ( size of minibatch ) visbias_a : numpy.ndarray , optional Optional override for visible biases . If both visbias_a and data are None , visible biases will be set to the same values of the temperature 1 model . For best results , use the ` data ` parameter . data : numpy.ndarray , optional Training data used to initialize the visible biases of the base-rate model ( usually infinite temperature ) , to the log-mean of the distribution ( maximum likelihood solution assuming a zero-weight matrix ) . This ensures that the base-rate model is the `` closest '' to the model at temperature 1. betas : numpy.ndarray , optional Vector specifying inverse temperature of intermediate distributions ( in increasing order ) . If None , defaults to AIS.dflt_betas key_betas : numpy.ndarray , optional If not None , AIS.run will save the log AIS weights for all temperatures in ` key_betas ` . This allows the user to estimate logZ at several temperatures in a single pass of AIS . rng : None or RandomStream , optional Random number generator object to use . seed : int , optional If rng is None , initialize rng with this seed . References -- -- -- -- -- .. [ 1 ] Neal , R. M. ( 1998 ) `` Annealed importance sampling '' , Technical Report No . 9805 ( revised ) , Dept . of Statistics , University of Toronto , 25 pages .. [ 2 ] Ruslan Salakhutdinov , Iain Murray . `` On the quantitative analysis of deep belief networks '' . Proceedings of the 25th International Conference on Machine Learning , p.872-879 , July 5 -- 9 , 2008 , Helsinki , Finland `` '' '' ''",https://github.com/lisa-lab/pylearn2/blob/58ba37286182817301ed72b0f143a89547b3f011/pylearn2/rbm_tools.py#L117,Python,yes,report,algorithm,method,reference,machine learning,formulas to source code,no
"`` Budget Fair Queueing ( BFQ ) I O scheduler . Based on ideas and code from CFQ : Copyright ( C ) 2003 Jens Axboe < axboe @ kernel.dk > Copyright ( C ) 2008 Fabio Checconi < fabio @ gandalf.sssup.it > Paolo Valente < paolo.valente @ unimore.it > Copyright ( C ) 2010 Paolo Valente < paolo.valente @ unimore.it > Arianna Avanzini < avanzini @ google.com > Copyright ( C ) 2017 Paolo Valente < paolo.valente @ linaro.org > This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation ; either version 2 of the License , or ( at your option ) any later version . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . BFQ is a proportional-share I O scheduler , with some extra low-latency capabilities . BFQ also supports full hierarchical scheduling through cgroups . Next paragraphs provide an introduction on BFQ inner workings . Details on BFQ benefits , usage and limitations can be found in Documentation block bfq-iosched.txt . BFQ is a proportional-share storage-I O scheduling algorithm based on the slice-by-slice service scheme of CFQ . But BFQ assigns budgets , measured in number of sectors , to processes instead of time slices . The device is not granted to the in-service process for a given time slice , but until it has exhausted its assigned budget . This change from the time to the service domain enables BFQ to distribute the device throughput among processes as desired , without any distortion due to throughput fluctuations , or to device internal queueing . BFQ uses an ad hoc internal scheduler , called B-WF2Q+ , to schedule processes according to their budgets . More precisely , BFQ schedules queues associated with processes . Each process queue is assigned a user-configurable weight , and B-WF2Q+ guarantees that each queue receives a fraction of the throughput proportional to its weight . Thanks to the accurate policy of B-WF2Q+ , BFQ can afford to assign high budgets to I O-bound processes issuing sequential requests ( to boost the throughput ) , and yet guarantee a low latency to interactive and soft real-time applications . In particular , to provide these low-latency guarantees , BFQ explicitly privileges the I O of two classes of time-sensitive applications : interactive and soft real-time . This feature enables BFQ to provide applications in these classes with a very low latency . Finally , BFQ also features additional heuristics for preserving both a low latency and a high throughput on NCQ-capable , rotational or flash-based devices , and to get the job done quickly for applications consisting in many I O-bound processes . NOTE : if the main or only goal , with a given device , is to achieve the maximum-possible throughput at all times , then do switch off all low-latency heuristics for that device , by setting low_latency to 0 . BFQ is described in [ 1 ] , where also a reference to the initial , more theoretical paper on BFQ can be found . The interested reader can find in the latter paper full details on the main algorithm , as well as formulas of the guarantees and formal proofs of all the properties . With respect to the version of BFQ presented in these papers , this implementation adds a few more heuristics , such as the one that guarantees a low latency to soft real-time applications , and a hierarchical extension based on H-WF2Q+ . B-WF2Q+ is based on WF2Q+ , which is described in [ 2 ] , together with H-WF2Q+ , while the augmented tree used here to implement B-WF2Q+ with O ( log N ) complexity derives from the one introduced with EEVDF in [ 3 ] . [ 1 ] P. Valente , A. Avanzini , `` Evolution of the BFQ Storage I O Scheduler '' , Proceedings of the First Workshop on Mobile System Technologies ( MST-2015 ) , May 2015. http : algogroup.unimore.it people paolo disk_sched mst-2015.pdf [ 2 ] Jon C.R . Bennett and H. Zhang , `` Hierarchical Packet Fair Queueing Algorithms '' , IEEE ACM Transactions on Networking , 5 ( 5 ) :675-689 , Oct 1997. http : www.cs.cmu.edu ~hzhang papers TON-97-Oct.ps.gz [ 3 ] I. Stoica and H. Abdel-Wahab , `` Earliest Eligible Virtual Deadline First : A Flexible and Accurate Mechanism for Proportional Share Resource Allocation '' , technical report . http : www.cs.berkeley.edu ~istoica papers eevdf-tr-95.pdf ``",https://github.com/mkahola/drm-intel-mika/blob/40421157bade02e7cafc60a9fd59bde22899f9b9/block/bfq-iosched.c#L1,C,yes,conference,background,n/a,related,networks and os,no transfer,no
"`` Copyright ( c ) 2013-2015 : G-CSC , Goethe University Frankfurt Authors : Lisa Grau , Andreas Vogel This file is part of UG4 . UG4 is free software : you can redistribute it and or modify it under the terms of the GNU Lesser General Public License version 3 ( as published by the Free Software Foundation ) with the following additional attribution requirements ( according to LGPL GPL v3 ¬ß7 ) : ( 1 ) The following notice must be displayed in the Appropriate Legal Notices of covered and combined works : `` Based on UG4 ( www.ug4.org license ) '' . ( 2 ) The following notice must be displayed at a prominent place in the terminal output of covered works : `` Based on UG4 ( www.ug4.org license ) '' . ( 3 ) The following bibliography is recommended for citation and must be preserved in all covered files : `` Reiter , S. , Vogel , A. , Heppner , I. , Rupp , M. , and Wittum , G. A massively parallel geometric multigrid solver on hierarchically distributed grids . Computing and visualization in science 16 , 4 ( 2013 ) , 151-164 '' `` Vogel , A. , Reiter , S. , Rupp , M. , N√§gel , A. , and Wittum , G. UG4 -- a novel flexible software system for simulating pde based models on high performance computers . Computing and visualization in science 16 , 4 ( 2013 ) , 165-179 '' This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU Lesser General Public License for more details. ``",https://github.com/UG4/ugcore/blob/fd783d5abfa67f0bbaae380cd52cbb2beff8e0d4/ugbase/lib_disc/quadrature/newton_cotes/newton_cotes.h#L1,C++,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` this is a `` Gaussian Toeplitz matrix '' , as mentioned in Example 2 of I. Gohbert , T. Kailath and V. Olshevsky `` Fast Gaussian Elimination with Partial Pivoting for Matrices with Displacement Structure '' Mathematics of Computation , 64 , 212 ( 1995 ) , pp 1557-1576 which can be unstable for levinson recursion . ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/scipy/linalg/tests/test_solve_toeplitz.py#L96,Python,yes,journal,numbers,method,reference,other,numbers to hard coded values,no
"`` A < code > RangeDifferencer < code > finds the differences between two or three < code > IRangeComparator < code > s . < p > To use the differencer , clients provide an < code > IRangeComparator < code > that breaks their input data into a sequence of comparable entities . The differencer returns the differences among these sequences as an array of < code > RangeDifference < code > objects ( < code > findDifferences < code > methods ) . Every < code > RangeDifference < code > represents a single kind of difference and the corresponding ranges of the underlying comparable entities in the left , right , and optionally ancestor sides . < p > Alternatively , the < code > findRanges < code > methods not only return objects for the differing ranges but for non-differing ranges too . < p > The algorithm used is an objectified version of one described in : < it > A File Comparison Program , < it > by Webb Miller and Eugene W. Myers , Software Practice and Experience , Vol . 15 , Nov. 1985 . @ see IRangeComparator @ see RangeDifference ``",https://github.com/plutext/docx4j/blob/618830dd6c86d99d9173462ad00ebdcc3eac3fbb/src/diffx/org/eclipse/compare/rangedifferencer/RangeDifferencer.java#L19,Java,yes,journal,algorithm,class,reference,other,formulas to source code,no
"`` References : Lin , S. F. , Chang , Y. L. , & Chen , L. G. ( 2003 ) . Motion adaptive interpolation with horizontal motion detection for deinterlacing . Consumer Electronics , IEEE Transactions on , 49 ( 4 ) , 1256-1265 . Pei-Yin , C. H. E. N. , & Yao-Hsien , L. A. I . ( 2007 ) . A low-complexity interpolation method for deinterlacing . IEICE transactions on information and systems , 90 ( 2 ) , 606-608. ``",https://github.com/tarceri/Mesa/blob/e01af38d6faf5dfd0f4ac6548ae03c27cca1dede/src/gallium/auxiliary/vl/vl_deint_filter.c#L28,C,404,,,,,,,
"`` Implements the < em > divergence from randomness ( DFR ) < em > framework introduced in Gianni Amati and Cornelis Joost Van Rijsbergen . 2002 . Probabilistic models of information retrieval based on measuring the divergence from randomness . ACM Trans . Inf . Syst . 20 , 4 ( October 2002 ) , 357-389 . < p > The DFR scoring formula is composed of three separate components : the < em > basic model < em > , the < em > aftereffect < em > and an additional < em > normalization < em > component , represented by the classes { @ code BasicModel } , { @ code AfterEffect } and { @ code Normalization } , respectively . The names of these classes were chosen to match the names of their counterparts in the Terrier IR engine. < p > < p > To construct a DFRSimilarity , you must specify the implementations for all three components of DFR : < ol > < li > { @ link BasicModel } : Basic model of information content : < ul > < li > { @ link BasicModelG } : Geometric approximation of Bose-Einstein < li > { @ link BasicModelIn } : Inverse document frequency < li > { @ link BasicModelIne } : Inverse expected document frequency [ mixture of Poisson and IDF ] < li > { @ link BasicModelIF } : Inverse term frequency [ approximation of I ( ne ) ] < ul > < li > { @ link AfterEffect } : First normalization of information gain : < ul > < li > { @ link AfterEffectL } : Laplace 's law of succession < li > { @ link AfterEffectB } : Ratio of two Bernoulli processes < ul > < li > { @ link Normalization } : Second ( length ) normalization : < ul > < li > { @ link NormalizationH1 } : Uniform distribution of term frequency < li > { @ link NormalizationH2 } : term frequency density inversely related to length < li > { @ link NormalizationH3 } : term frequency normalization provided by Dirichlet prior < li > { @ link NormalizationZ } : term frequency normalization provided by a Zipfian relation < li > { @ link NoNormalization } : no second normalization < ul > < ol > < p > Note that < em > qtf < em > , the multiplicity of term-occurrence in the query , is not handled by this implementation. < p > < p > Note that basic models BE ( Limiting form of Bose-Einstein ) , P ( Poisson approximation of the Binomial ) and D ( Divergence approximation of the Binomial ) are not implemented because their formula could n't be written in a way that makes scores non-decreasing with the normalized term frequency . @ see BasicModel @ see AfterEffect @ see Normalization @ lucene.experimental ``",https://github.com/apache/lucene-solr/blob/4a9a8397e458a5805c55fe494ba4b6de18233f90/lucene/core/src/java/org/apache/lucene/search/similarities/DFRSimilarity.java#L26,Java,yes,journal,algorithm,class,reference,data science,formulas to source code,no
"`` Implement addition , subtraction , multiplication and division based on : `` Software for Doubled-Precision Floating-Point Computations '' , by Seppo Linnainmaa , ACM TOMS vol 7 no 3 , September 1981 , pages 272-283 . ''",https://github.com/strejda/tegra/blob/c018f0f7e1b1741efd007d737f0acc34c3633c2e/contrib/llvm/lib/Support/APFloat.cpp#L3905,C,yes,journal,algorithm,method,reference,networks and os,pseudocode to source code,no
"`` `` '' '' Heagy et al. , 2017 Load and Plot Bookpurnong Data=================================================In this example , we load and plot the SkyTEM ( 2006 ) and RESOLVE ( 2008 ) Bookpurnong data , available at ` https : storage.googleapis.com simpeg bookpurnong bookpurnong.tar.gz < https : storage.googleapis.com simpeg bookpurnong bookpurnong.tar.gz > ` _This is published in Lindsey J. Heagy , Rowan Cockett , Seogi Kang , Gudni K. Rosenkjaer , Douglas W. Oldenburg , A framework for simulation and inversion in electromagnetics , Computers & Geosciences , Volume 107 , 2017 , Pages 1-19 , ISSN 0098-3004 , http : dx.doi.org 10.1016 j.cageo.2017.06.018.The script and figures are also on figshare : https : doi.org 10.6084 m9.figshare.5107711 '' '' '' ''",https://github.com/simpeg/simpeg/blob/94295102afc664c001f77c88f902772e06a467c0/examples/00_published/plot_load_booky.py#L1,Python,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` `` '' '' Kearsley , S.K . ( 1989 ) . Acta Cryst . A45 , 208-210 . On the orthogonal transformation used for structural comparison Added by Peter H. Zwart , Nov 3rd , 2006 . Converted to C++ by Gabor Bunkoczi , Apr 2008. `` '' '' ''",https://github.com/cctbx/cctbx_project/blob/f745f1d39982d3473340cdca6758eb98e0190c67/scitbx/math/superpose.py#L11,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` logGamma function . @ version 1.1 @ author Jaco van Kooten Original author was Jaco van Kooten . Ported to PHP by Paul Meagher . The natural logarithm of the gamma function . < br > Based on public domain NETLIB ( Fortran ) code by W. J. Cody and L. Stoltz < br > Applied Mathematics Division < br > Argonne National Laboratory < br > Argonne , IL 60439 < br > < p > References : < ol > < li > W . J. Cody and K. E. Hillstrom , 'Chebyshev Approximations for the Natural Logarithm of the Gamma Function , ' Math . Comp . 21 , 1967 , pp . 198-203. < li > < li > K . E. Hillstrom , ANL AMD Program ANLC366S , DGAMMA DLGAMA , May , 1969. < li > < li > Hart , Et . Al. , Computer Approximations , Wiley and sons , New York , 1968. < li > < ol > < p > < p > From the original documentation : < p > < p > This routine calculates the LOG ( GAMMA ) function for a positive real argument X. Computation is based on an algorithm outlined in references 1 and 2 . The program uses rational functions that theoretically approximate LOG ( GAMMA ) to at least 18 significant decimal digits . The approximation for X > 12 is from reference 3 , while approximations for X < 12.0 are similar to those in reference 1 , but are unpublished . The accuracy achieved depends on the arithmetic system , the compiler , the intrinsic functions , and proper selection of the machine-dependent constants . < p > < p > Error returns : < br > The program returns the value XINF for X .LE . 0.0 or when overflow would occur . The computation is believed to be free of underflow and overflow . < p > @ return float MAX_VALUE for x < 0.0 or when overflow would occur , i.e . x > 2.55E305 ``",https://github.com/openemr/openemr/blob/5307e6340c16e9a53bb1d750d6675eea729cccce/vendor/phpoffice/phpspreadsheet/src/PhpSpreadsheet/Calculation/Statistical.php#L165,PHP,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` 80 Purpose : DUNAVANT_SUBRULE_15 returns a compressed Dunavant rule 15 . Licensing : This code is distributed under the GNU LGPL license . Modified : 11 December 2006 Author : John Burkardt Reference : David Dunavant , High Degree Efficient Symmetrical Gaussian Quadrature Rules for the Triangle , International Journal for Numerical Methods in Engineering , Volume 21 , 1985 , pages 1129-1148 . James Lyness , Dennis Jespersen , Moderate Degree Symmetric Quadrature Rules for the Triangle , Journal of the Institute of Mathematics and its Applications , Volume 15 , Number 1 , February 1975 , pages 19-32 . Parameters : Input , int SUBORDER_NUM , the number of suborders of the rule . Output , double SUBORDER_XYZ [ 3 SUBORDER_NUM ] , the barycentric coordinates of the abscissas . Output , double SUBORDER_W [ SUBORDER_NUM ] , the suborder weights. ``",https://github.com/live-clones/dolfin/blob/474ca26489403e8fe8e5006a836bc9c9dc5194a3/dolfin/geometry/SimplexQuadrature.cpp#L2470,C++,yes,journal,numbers,method,reference,other,numbers to hard coded values,no
"`` `` '' '' Set the iterative-fitting scheme used in the fit . Control whether an iterative scheme should be applied to the fit . Parameters -- -- -- -- -- meth : { 'none ' , 'primini ' , 'sigmarej ' } The name of the scheme used during the fit ; 'none ' means no scheme is used . It is only valid to change the scheme when a chi-square statistic is in use . Raises -- -- -- TypeError When the `` meth `` argument is not recognized . See Also -- -- -- -- fit : Fit a model to one or more data sets . get_iter_method_name : Return the name of the iterative fitting scheme . get_iter_method_opt : Return one or all options for the iterative-fitting scheme . list_iter_methods : List the iterative fitting schemes . set_iter_method_opt : Set an option for the iterative-fitting scheme . set_stat : Set the statistical method . Notes -- -- - The parameters of each scheme are described in ` set_iter_method_opt ` . The `` primini `` scheme is used for re-calculating statistical errors , using the best-fit model parameters from the previous fit , until the fit can no longer be improved . This is a chi-square statistic where the variance is computed from model amplitudes derived in the previous iteration of the fit . This 'Iterative Weighting ' ( [ 1 ] _ ) attempts to remove biased estimates of model parameters which is inherent in chi-square2 statistics ( [ 2 ] _ ) . The variance in bin i is estimated to be : : sigma^2_i^j = S ( i , t_s^ ( j-1 ) ) + ( A_s A_b ) ^2 B_off ( i , t_b^ ( j-1 ) ) where j is the number of iterations that have been carried out in the fitting process , B_off is the background model amplitude in bin i of the off-source region , and t_s^ ( j-1 ) and t_b^ ( j-1 ) are the set of source and background model parameter values derived during the iteration previous to the current one . The variances are set to an array of ones on the first iteration . In addition to reducing parameter estimate bias , this statistic can be used even when the number of counts in each bin is small ( < 5 ) , although the user should proceed with caution . The `` sigmarej `` scheme is based on the IRAF `` sfit `` function [ 3 ] _ , where after a fit data points are excluded if the value of `` ( data-model ) error ) `` exceeds a threshold , and the data re-fit . This removal of data points continues until the fit has converged . The error removal can be asymmetric , since there are separate parameters for the lower and upper limits . References -- -- -- -- -- .. [ 1 ] `` Multiparameter linear least-squares fitting to Poisson data one count at a time '' , Wheaton et al . 1995 , ApJ 438 , 322 http : adsabs.harvard.edu abs 1995ApJ ... 438..322W .. [ 2 ] `` Bias-Free Parameter Estimation with Few Counts , by Iterative Chi-Squared Minimization '' , Kearns , Primini , & Alexander , 1995 , ADASS IV , 331 http : adsabs.harvard.edu abs 1995ASPC ... 77..331K .. [ 3 ] http : iraf.net irafhelp.php ? val=sfit `` '' '' ''",https://github.com/sherpa/sherpa/blob/a960597a2dda99458867f386d2993d8159450a80/sherpa/ui/utils.py#L1837,Python,yes,journal,background,n/a,related,machine learning,no transfer,no
"`` Exploration algorithms based on an extended version of the General State Exploring Algorithm ( GSEA ) framework : @ article { DBLP : journals sttt BosnackiLL09 , author = { Dragan Bosnacki and Stefan Leue and Alberto Lluch-Lafuente } , title = { Partial-order reduction for general state exploring algorithms } , journal = { STTT } , volume = { 11 } , number = { 1 } , year = { 2009 } , pages = { 39-51 } , ee = { http : dx.doi.org 10.1007 s10009-008-0093-y } , bibsource = { DBLP , http : dblp.uni-trier.de } } ``",https://github.com/utwente-fmt/ltsmin/blob/754461105b22bed3b53d3c7197653a5d598edffe/src/pins2lts-seq/pins2lts-seq.c#L33,C,yes,report,algorithm,file,reference,other,pseudocode to source code,no
"`` Copyright ( c ) 2011-2013 : G-CSC , Goethe University Frankfurt Author : Martin Rupp This file is part of UG4 . UG4 is free software : you can redistribute it and or modify it under the terms of the GNU Lesser General Public License version 3 ( as published by the Free Software Foundation ) with the following additional attribution requirements ( according to LGPL GPL v3 ¬ß7 ) : ( 1 ) The following notice must be displayed in the Appropriate Legal Notices of covered and combined works : `` Based on UG4 ( www.ug4.org license ) '' . ( 2 ) The following notice must be displayed at a prominent place in the terminal output of covered works : `` Based on UG4 ( www.ug4.org license ) '' . ( 3 ) The following bibliography is recommended for citation and must be preserved in all covered files : `` Reiter , S. , Vogel , A. , Heppner , I. , Rupp , M. , and Wittum , G. A massively parallel geometric multigrid solver on hierarchically distributed grids . Computing and visualization in science 16 , 4 ( 2013 ) , 151-164 '' `` Vogel , A. , Reiter , S. , Rupp , M. , N√§gel , A. , and Wittum , G. UG4 -- a novel flexible software system for simulating pde based models on high performance computers . Computing and visualization in science 16 , 4 ( 2013 ) , 165-179 '' This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU Lesser General Public License for more details. ``",https://github.com/UG4/ugcore/blob/fd783d5abfa67f0bbaae380cd52cbb2beff8e0d4/ugbase/lib_algebra/common/stl_debug.h#L1,C++,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` Implementations of floating-point long double basic arithmetic functions called by the IBM C compiler when generating code for PowerPC platforms . In particular , the following functions are implemented : __gcc_qadd , __gcc_qsub , __gcc_qmul , and __gcc_qdiv . Double-double algorithms are based on the paper `` Doubled-Precision IEEE Standard 754 Floating-Point Arithmetic '' by W. Kahan , February 26 , 1987 . An alternative published reference is `` Software for Doubled-Precision Floating-Point Computations '' , by Seppo Linnainmaa , ACM TOMS vol 7 no 3 , September 1981 , pages 272-283. ``",https://github.com/trini/u-boot/blob/e3396ffd720877976141fa0b76a0b8ee9643d7d1/post/lib_powerpc/fpu/darwin-ldouble.c#L12,C,yes,report,algorithm,file,reference,other,description to source code,no
"`` This is an implementation of the binary classifier version of the relevance vector machine algorithm described in the paper : Tipping , M. E. and A. C. Faul ( 2003 ) . Fast marginal likelihood maximisation for sparse Bayesian models . In C. M. Bishop and B. J. Frey ( Eds . ) , Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics , Key West , FL , Jan 3-6 . This code mostly does what is described in the above paper with the exception that here we use a different stopping condition as well as a modified alpha selection rule . See the code for the exact details. ``",https://github.com/davisking/dlib/blob/2b55b996406eebd6abcbdb0126048ca09fdd8813/dlib/svm/rvm.h#L129,C++,yes,conference,algorithm,class,reference,machine learning,formulas to source code,no
"`` An implementation of the Vegas congestion control algorithm for FreeBSD , based on L. S. Brakmo and L. L. Peterson , `` TCP Vegas : end to end congestion avoidance on a global internet '' , IEEE J. Sel . Areas Commun. , vol . 13 , no . 8 , pp . 1465-1480 , Oct. 1995 . The original Vegas duplicate ack policy has not been implemented , since clock ticks are not as coarse as they were ( i.e . 500ms ) when Vegas was designed . Also , packets are timed once per RTT as in the original paper . Originally released as part of the NewTCP research project at Swinburne University of Technology 's Centre for Advanced Internet Architectures , Melbourne , Australia , which was made possible in part by a grant from the Cisco University Research Program Fund at Community Foundation Silicon Valley . More details are available at : http : caia.swin.edu.au urp newtcp ``",https://github.com/bukinr/freebsd-head/blob/6be0582ae51fc3a3ba7b037d79d3be65c2ca6f26/sys/netinet/cc/cc_vegas.c#L41,C,yes,journal,algorithm,file,reference,networks and os,description to source code,no
"`` class RegionBasedLevelSetFunctionSharedData brief Helper class used to share data in the ScalarChanAndVeseLevelSetFunction . This class holds cache data used during the computation of the LevelSet updates . Based on the paper : `` An active contour model without edges '' T. Chan and L. Vese . In Scale-Space Theories in Computer Vision , pages 141-151 , 1999. author Mosaliganti K. , Smith B. , Gelas A. , Gouaillard A. , Megason S. This code was taken from the Insight Journal paper : `` Cell Tracking using Coupled Active Surfaces for Nuclei and Membranes '' http : www.insight-journal.org browse publication 642 https : hdl.handle.net 10380 3055 That is based on the papers : `` Level Set Segmentation : Active Contours without edge '' http : www.insight-journal.org browse publication 322 https : hdl.handle.net 1926 1532 and `` Level set segmentation using coupled active surfaces '' http : www.insight-journal.org browse publication 323 https : hdl.handle.net 1926 1533 ingroup ITKReview ``",https://github.com/InsightSoftwareConsortium/ITK/blob/a077e98358a87c2b507e5ee9c27939918a9120ea/Modules/Nonunit/Review/include/itkRegionBasedLevelSetFunctionSharedData.h#L32,C++,yes,conference,background,n/a,related,computer vision,no transfer,no
"`` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- decNumberSquareRoot -- square root operator This computes C = squareroot ( A ) res is C , the result . C may be A rhs is A set is the context ; note that rounding mode has no effect C must have space for set- > digits digits . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- This uses the following varying-precision algorithm in : Properly Rounded Variable Precision Square Root , T. E. Hull and A. Abrham , ACM Transactions on Mathematical Software , Vol 11 3 , pp229-237 , ACM , September 1985 . % [ Reformatted original Numerical Turing source code follows . ] function sqrt ( x : real ) : real % sqrt ( x ) returns the properly rounded approximation to the square % root of x , in the precision of the calling environment , or it % fails if x < 0 . % t e hull and a abrham , august , 1984 if x < = 0 then if x < 0 then assert false else result 0 end if end if var f : = setexp ( x , 0 ) % fraction part of x [ 0.1 < = x < 1 ] var e : = getexp ( x ) % exponent part of x var approx : real if e mod 2 = 0 then approx : = .259 + .819 f % approx to root of f else f : = f l0 % adjustments e : = e + 1 % for odd approx : = .0819 + 2.59 f % exponent end if var p : = 3 const maxp : = currentprecision + 2 loop p : = min ( 2 p - 2 , maxp ) % p = 4,6,10 , . . . , maxp precision p approx : = .5 ( approx + f approx ) exit when p = maxp end loop % approx is now within 1 ulp of the properly rounded square root % of f ; to ensure proper rounding , compare squares of ( approx - % l 2 ulp ) and ( approx + l 2 ulp ) with f. p : = currentprecision begin precision p + 2 const approxsubhalf : = approx - setexp ( .5 , -p ) if mulru ( approxsubhalf , approxsubhalf ) > f then approx : = approx - setexp ( .l , -p + 1 ) else const approxaddhalf : = approx + setexp ( .5 , -p ) if mulrd ( approxaddhalf , approxaddhalf ) < f then approx : = approx + setexp ( .l , -p + 1 ) end if end if end result setexp ( approx , e div 2 ) % fix exponent end sqrt -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ``",https://github.com/netzbasis/openbsd-src/blob/2476b636c89c988f6ec4abd6fab50cec08a025d7/gnu/gcc/libdecnumber/decNumber.c#L1385,C,yes,journal,algorithm,method,reference,networks and os,pseudocode to source code,no
"`` NAME MOLLWEIDEPURPOSE : Transforms input longitude and latitude to Easting and Northing for the MOllweide projection . The longitude and latitude must be in radians . The Easting and Northing values will be returned in meters.PROGRAMMER DATE -- -- -- -- -- -- -- D. Steinwand , EROS May , 1991 ; Updated Sept , 1992 ; Updated Feb , 1993S . Nelson , EDC Jun , 2993 ; Made corrections in precision and number of iterations.ALGORITHM REFERENCES1 . Snyder , John P. and Voxland , Philip M. , `` An Album of Map Projections '' , U.S. Geological Survey Professional Paper 1453 , United State Government Printing Office , Washington D.C. , 1989.2 . Snyder , John P. , `` Map Projections -- A Working Manual '' , U.S. Geological Survey Professional Paper 1395 ( Supersedes USGS Bulletin 1532 ) , United State Government Printing Office , Washington D.C. , 1987. ``",https://github.com/GCRC/nunaliit/blob/04938bf44e667dd1f69dffd399b754356e1f673f/nunaliit2-js-external/src/main/js/js-external/js/proj4js/lib/projCode/moll.js#L1,JavaScript,yes,report,algorithm,file,reference,other,formulas to source code,no
"`` Subfunction for Hypergeometric distribution . This method is valid only for mode > = 10 and 0 < = n < = m < = N 2 . This method is fast when called repeatedly with the same parameters , but slow when the parameters change due to a high setup time . The computation time hardly depends on the parameters , except that it matters a lot whether parameters are within the range where the LnFac function is tabulated . Uses the Patchwork Rejection method of Heinz Zechner ( HPRS ) . The area below the histogram function f ( x ) in its body is rearranged by two point reflections . Within a large center interval variates are sampled efficiently by rejection from uniform hats . Rectangular immediate acceptance regions speed up the generation . The remaining tails are covered by exponential functions . For detailed explanation , see : Stadlober , E & Zechner , H : `` The Patchwork Rejection Technique for Sampling from Unimodal Distributions '' . ACM Transactions on Modeling and Computer Simulation , vol . 9 , no . 1 , 1999 , p. 59-83. ``",https://github.com/BlastTNG/flight/blob/b6bba1afef0af9e4907ec49ef2ae4425086ab29c/stars/code/tools/stocc/stoc2.cpp#L594,C,yes,journal,algorithm,method,reference,science,pseudocode to source code,no
"`` 80 Purpose : LEGENDRE_SET sets abscissas and weights for Gauss-Legendre quadrature . Discussion : The integral : Integral ( -1 < = X < = 1 ) F ( X ) dX Quadrature rule : Sum ( 1 < = I < = N ) W ( I ) F ( X ( I ) ) The quadrature rule is exact for all polynomials through degree 2 N-1 . The abscissas are the zeroes of the Legendre polynomial P ( N ) ( X ) . Mathematica can compute the abscissas and weights of a Gauss-Legendre rule of order N for the interval [ A , B ] with P digits of precision by the commands : Needs [ `` NumericalDifferentialEquationAnalysis ` `` ] GaussianQuadratureWeights [ n , a , b , p ] Licensing : This code is distributed under the GNU LGPL license . Modified : 20 April 2010 Author : John Burkardt Reference : Milton Abramowitz , Irene Stegun , Handbook of Mathematical Functions , National Bureau of Standards , 1964 , ISBN : 0-486-61272-4 , LC : QA47.A34 . Vladimir Krylov , Approximate Calculation of Integrals , Dover , 2006 , ISBN : 0486445798 . LC : QA311.K713 . Arthur Stroud , Don Secrest , Gaussian Quadrature Formulas , Prentice Hall , 1966 , LC : QA299.4G3S7 . Stephen Wolfram , The Mathematica Book , Fourth Edition , Cambridge University Press , 1999 , ISBN : 0-521-64314-7 , LC : QA76.95.W65 . Daniel Zwillinger , editor , CRC Standard Mathematical Tables and Formulae , 30th Edition , CRC Press , 1996 , ISBN : 0-8493-2479-3 , LC : QA47.M315 . Parameters : Input , int N , the order . N must be between 1 and 33 or 63 64 65 , 127 128 129 , 255 256 257 . Output , double X [ N ] , the abscissas . Output , double W [ N ] , the weights. ``",https://github.com/davidshepherd7/oomph-lib-micromagnetics/blob/cb028543d972ad304f042527c03a2f5df8ec79d7/etc/generate_quadrature_rules/quadrule.cc#L11913,C++,yes,book,numbers,method,reference,science,numbers to hard coded values,no
"`` Copyright ( c ) 2009-2015 : G-CSC , Goethe University Frankfurt Author : Sebastian Reiter , Martin Stepniewski This file is part of UG4 . UG4 is free software : you can redistribute it and or modify it under the terms of the GNU Lesser General Public License version 3 ( as published by the Free Software Foundation ) with the following additional attribution requirements ( according to LGPL GPL v3 ¬ß7 ) : ( 1 ) The following notice must be displayed in the Appropriate Legal Notices of covered and combined works : `` Based on UG4 ( www.ug4.org license ) '' . ( 2 ) The following notice must be displayed at a prominent place in the terminal output of covered works : `` Based on UG4 ( www.ug4.org license ) '' . ( 3 ) The following bibliography is recommended for citation and must be preserved in all covered files : `` Reiter , S. , Vogel , A. , Heppner , I. , Rupp , M. , and Wittum , G. A massively parallel geometric multigrid solver on hierarchically distributed grids . Computing and visualization in science 16 , 4 ( 2013 ) , 151-164 '' `` Vogel , A. , Reiter , S. , Rupp , M. , N√§gel , A. , and Wittum , G. UG4 -- a novel flexible software system for simulating pde based models on high performance computers . Computing and visualization in science 16 , 4 ( 2013 ) , 165-179 '' This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU Lesser General Public License for more details. ``",https://github.com/UG4/ugcore/blob/fd783d5abfa67f0bbaae380cd52cbb2beff8e0d4/ugbase/lib_grid/file_io/file_io_ug.h#L1,C++,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` This example shows how to adaptively learn the parameters of a Bayesian network from a stream of data using exponential forgetting with a given fading factor , directly inspired by the approach presented in < i > Olesen , K. G. , Lauritzen , S. L. , and Jensen , F. V. ( 1992 , July ) . aHUGIN : A system creating adaptive causal probabilistic networks . In Proceedings of the Eighth international conference on Uncertainty in Artificial Intelligence ( pp . 223-229 ) . Morgan Kaufmann Publishers Inc. < i > Created by andresmasegosa on 18 6 15. ``",https://github.com/amidst/toolbox/blob/3e45c32fc7cf0bf5726a78a171cc2150dc9ea71f/examples/src/main/java/eu/amidst/core/examples/conceptdrift/MaximimumLikelihoodFadingExample.java#L27,Java,yes,conference,background,n/a,related,machine learning,no transfer,no
"`` - - Mode : python ; tab-width : 4 ; indent-tabs-mode : nil ; coding : utf-8 - - vim : tabstop=4 expandtab shiftwidth=4 softtabstop=4 MDAnalysis -- - https : www.mdanalysis.org Copyright ( c ) 2006-2017 The MDAnalysis Development Team and contributors ( see the file AUTHORS for the full list of names ) Released under the GNU Public Licence , v2 or any higher version Please cite your use of MDAnalysis in published work : R. J. Gowers , M. Linke , J. Barnoud , T. J. E. Reddy , M. N. Melo , S. L. Seyler , D. L. Dotson , J. Domanski , S. Buchoux , I. M. Kenney , and O. Beckstein . MDAnalysis : A Python package for the rapid analysis of molecular dynamics simulations . In S. Benthall and S. Rostrup editors , Proceedings of the 15th Python in Science Conference , pages 102-109 , Austin , TX , 2016 . SciPy . N. Michaud-Agrawal , E. J. Denning , T. B. Woolf , and O. Beckstein . MDAnalysis : A Toolkit for the Analysis of Molecular Dynamics Simulations . J. Comput . Chem . 32 ( 2011 ) , 2319 -- 2327 , doi:10.1002 jcc.21787 Python implementation of FORTRAN HELANAL [ Bansal et al , J Biomol Struct Dyn . 17 ( 2000 ) , 811 ] Copyright ( c ) 2009 Benjamin Hall < benjamin.hall @ bioch.ox.ac.uk > Published under the GNU General Public License v2 or higher Copyright ( c ) 2011 Oliver Beckstein < orbeckst @ gmail.com > Integrated into MDAnalysis and NumPy-fied ''",https://github.com/MDAnalysis/mdanalysis/blob/bdb1844d17d1be6340452a25e95ed39d614d9edf/package/MDAnalysis/analysis/helanal.py#L1,Python,yes,conference,background,n/a,related,simulation,no transfer,yes
"`` Given || [ dA , db ] ||_2 < = rho , minimize the worst-case error of || ( A+dA ) x - ( b+db ) ||_2 , which can be shown to be equal to || A x - b ||_2 + rho || [ x ; 1 ] ||_2 , subject to x > = 0 , which can be formulated as the SOCP min t + rho s s.t . || A x - b ||_2 < = t , || [ x ; 1 ] ||_2 < = s , x > = 0 . ( See [ 1 ] or Subsection 2.7 of [ 2 ] . ) [ 1 ] L. El Ghaoui and H. Lebret , `` Robust solutions to least-squares problems with uncertain data '' , SIAM J. Matrix Anal . and Appl. , Vol . 18 , No . 4 , 1997 . DOI : http : epubs.siam.org doi abs 10.1137 S0895479896298130 [ 2 ] M.S . Lobo , L. Vandenberghe , S. Boyd , and H. Lebret , `` Applications of second-order cone programming '' , Linear Algebra and its Applications , Vol . 284 , Issues 1-3 , 1998 . DOI : http : www.sciencedirect.com science article pii S0024379598100320 ``",https://github.com/elemental/Elemental/blob/bc9c018eafa2b31ddd578f91474356d88874aa41/src/optimization/models/RNNLS.cpp#L13,C++,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` `` '' '' Find the B-spline representation of 1-D curve . Given the set of data points `` ( x [ i ] , y [ i ] ) `` determine a smooth spline approximation of degree k on the interval `` xb < = x < = xe `` . Parameters -- -- -- -- -- x , y : array_like The data points defining a curve y = f ( x ) . w : array_like , optional Strictly positive rank-1 array of weights the same length as x and y . The weights are used in computing the weighted least-squares spline fit . If the errors in the y values have standard-deviation given by the vector d , then w should be 1 d. Default is ones ( len ( x ) ) . xb , xe : float , optional The interval to fit . If None , these default to x [ 0 ] and x [ -1 ] respectively . k : int , optional The degree of the spline fit . It is recommended to use cubic splines . Even values of k should be avoided especially with small s values . 1 < = k < = 5 task : { 1 , 0 , -1 } , optional If task==0 find t and c for a given smoothing factor , s. If task==1 find t and c for another value of the smoothing factor , s. There must have been a previous call with task=0 or task=1 for the same set of data ( t will be stored an used internally ) If task=-1 find the weighted least square spline for a given set of knots , t. These should be interior knots as knots on the ends will be added automatically . s : float , optional A smoothing condition . The amount of smoothness is determined by satisfying the conditions : sum ( ( w ( y - g ) ) 2 , axis=0 ) < = s where g ( x ) is the smoothed interpolation of ( x , y ) . The user can use s to control the tradeoff between closeness and smoothness of fit . Larger s means more smoothing while smaller values of s indicate less smoothing . Recommended values of s depend on the weights , w. If the weights represent the inverse of the standard-deviation of y , then a good s value should be found in the range ( m-sqrt ( 2 m ) , m+sqrt ( 2 m ) ) where m is the number of datapoints in x , y , and w. default : s=m-sqrt ( 2 m ) if weights are supplied . s = 0.0 ( interpolating ) if no weights are supplied . t : array_like , optional The knots needed for task=-1 . If given then task is automatically set to -1. full_output : bool , optional If non-zero , then return optional outputs . per : bool , optional If non-zero , data points are considered periodic with period x [ m-1 ] - x [ 0 ] and a smooth periodic spline approximation is returned . Values of y [ m-1 ] and w [ m-1 ] are not used . quiet : bool , optional Non-zero to suppress messages . This parameter is deprecated ; use standard Python warning filters instead . Returns -- -- -- - tck : tuple ( t , c , k ) a tuple containing the vector of knots , the B-spline coefficients , and the degree of the spline . fp : array , optional The weighted sum of squared residuals of the spline approximation . ier : int , optional An integer flag about splrep success . Success is indicated if ier < =0 . If ier in [ 1,2,3 ] an error occurred but was not raised . Otherwise an error is raised . msg : str , optional A message corresponding to the integer flag , ier . Notes -- -- - See splev for evaluation of the spline and its derivatives . The user is responsible for assuring that the values of x are unique . Otherwise , splrep will not return sensible results . See Also -- -- -- -- UnivariateSpline , BivariateSpline splprep , splev , sproot , spalde , splint bisplrep , bisplev Notes -- -- - See splev for evaluation of the spline and its derivatives . Uses the FORTRAN routine curfit from FITPACK . If provided , knots ` t ` must satisfy the Schoenberg-Whitney conditions , i.e. , there must be a subset of data points `` x [ j ] `` such that `` t [ j ] < x [ j ] < t [ j+k+1 ] `` , for `` j=0 , 1 , ... , n-k-2 `` . References -- -- -- -- -- Based on algorithms described in [ 1 ] _ , [ 2 ] _ , [ 3 ] _ , and [ 4 ] _ : .. [ 1 ] P. Dierckx , `` An algorithm for smoothing , differentiation and integration of experimental data using spline functions '' , J.Comp.Appl.Maths 1 ( 1975 ) 165-184 . .. [ 2 ] P. Dierckx , `` A fast algorithm for smoothing data on a rectangular grid while using spline functions '' , SIAM J.Numer.Anal . 19 ( 1982 ) 1286-1304 . .. [ 3 ] P. Dierckx , `` An improved algorithm for curve fitting with spline functions '' , report tw54 , Dept . Computer Science , K.U . Leuven , 1981 . .. [ 4 ] P. Dierckx , `` Curve and surface fitting with splines '' , Monographs on Numerical Analysis , Oxford University Press , 1993 . Examples -- -- -- -- > > > import matplotlib.pyplot as plt > > > from scipy.interpolate import splev , splrep > > > x = np.linspace ( 0 , 10 , 10 ) > > > y = np.sin ( x ) > > > tck = splrep ( x , y ) > > > x2 = np.linspace ( 0 , 10 , 200 ) > > > y2 = splev ( x2 , tck ) > > > plt.plot ( x , y , ' o ' , x2 , y2 ) > > > plt.show ( ) `` '' '' ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/scipy/interpolate/fitpack.py#L320,Python,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` Preconditioned Lanczos Note that there are 4 variants on the Lanczos algorithm . These are described in Paige , C. C. ( 1972 ) . Computational variants of the Lanczos method for the eigenproblem . IMA Journal of Applied Mathematics , 10 ( 3 ) , 373‚Äì381 . The current implementation corresponds to the case A ( 2,7 ) in the paper . It also corresponds to algorithm 6.14 in Y. Saad , Iterative Methods ÔøºÔøºÔøºfor Sparse Linear Systems , 2003 p.173 . For the preconditioned version see A. Greenbaum , Iterative Methods for Solving Linear Systems , SIAM ( 1987 ) . ``",https://github.com/kaskr/adcomp/blob/3e23da9528701545cbc7c33877b7abf2809e5842/TMB/inst/include/unsupported/Eigen/src/IterativeSolvers/MINRES.h#L82,C++,yes,journal,algorithm,method,reference,science,paper not available,no
"`` `` '' '' Compute finite difference approximation of the derivatives of a vector-valued function . If a function maps from R^n to R^m , its derivatives form m-by-n matrix called the Jacobian , where an element ( i , j ) is a partial derivative of f [ i ] with respect to x [ j ] . Parameters -- -- -- -- -- fun : callable Function of which to estimate the derivatives . The argument x passed to this function is ndarray of shape ( n , ) ( never a scalar even if n=1 ) . It must return 1-d array_like of shape ( m , ) or a scalar . x0 : array_like of shape ( n , ) or float Point at which to estimate the derivatives . Float will be converted to a 1-d array . method : { ' 3-point ' , ' 2-point ' } , optional Finite difference method to use : - ' 2-point ' - use the fist order accuracy forward or backward difference . - ' 3-point ' - use central difference in interior points and the second order accuracy forward or backward difference near the boundary . - 'cs ' - use a complex-step finite difference scheme . This assumes that the user function is real-valued and can be analytically continued to the complex plane . Otherwise , produces bogus results . rel_step : None or array_like , optional Relative step size to use . The absolute step size is computed as `` h = rel_step sign ( x0 ) max ( 1 , abs ( x0 ) ) `` , possibly adjusted to fit into the bounds . For `` method= ' 3-point ' `` the sign of ` h ` is ignored . If None ( default ) then step is selected automatically , see Notes . f0 : None or array_like , optional If not None it is assumed to be equal to `` fun ( x0 ) `` , in this case the `` fun ( x0 ) `` is not called . Default is None . bounds : tuple of array_like , optional Lower and upper bounds on independent variables . Defaults to no bounds . Each bound must match the size of ` x0 ` or be a scalar , in the latter case the bound will be the same for all variables . Use it to limit the range of function evaluation . sparsity : { None , array_like , sparse matrix , 2-tuple } , optional Defines a sparsity structure of the Jacobian matrix . If the Jacobian matrix is known to have only few non-zero elements in each row , then it 's possible to estimate its several columns by a single function evaluation [ 3 ] _ . To perform such economic computations two ingredients are required : structure : array_like or sparse matrix of shape ( m , n ) . A zero element means that a corresponding element of the Jacobian identically equals to zero . groups : array_like of shape ( n , ) . A column grouping for a given sparsity structure , use ` group_columns ` to obtain it . A single array or a sparse matrix is interpreted as a sparsity structure , and groups are computed inside the function . A tuple is interpreted as ( structure , groups ) . If None ( default ) , a standard dense differencing will be used . Note , that sparse differencing makes sense only for large Jacobian matrices where each row contains few non-zero elements . args , kwargs : tuple and dict , optional Additional arguments passed to ` fun ` . Both empty by default . The calling signature is `` fun ( x , args , kwargs ) `` . Returns -- -- -- - J : ndarray or csr_matrix Finite difference approximation of the Jacobian matrix . If ` sparsity ` is None then ndarray with shape ( m , n ) is returned . Although if m=1 it is returned as a gradient with shape ( n , ) . If ` sparsity ` is not None , csr_matrix with shape ( m , n ) is returned . See Also -- -- -- -- check_derivative : Check correctness of a function computing derivatives . Notes -- -- - If ` rel_step ` is not provided , it assigned to `` EPS ( 1 s ) `` , where EPS is machine epsilon for float64 numbers , s=2 for ' 2-point ' method and s=3 for ' 3-point ' method . Such relative step approximately minimizes a sum of truncation and round-off errors , see [ 1 ] _ . A finite difference scheme for ' 3-point ' method is selected automatically . The well-known central difference scheme is used for points sufficiently far from the boundary , and 3-point forward or backward scheme is used for points near the boundary . Both schemes have the second-order accuracy in terms of Taylor expansion . Refer to [ 2 ] _ for the formulas of 3-point forward and backward difference schemes . For dense differencing when m=1 Jacobian is returned with a shape ( n , ) , on the other hand when n=1 Jacobian is returned with a shape ( m , 1 ) . Our motivation is the following : a ) It handles a case of gradient computation ( m=1 ) in a conventional way . b ) It clearly separates these two different cases . b ) In all cases np.atleast_2d can be called to get 2-d Jacobian with correct dimensions . References -- -- -- -- -- .. [ 1 ] W. H. Press et . al . `` Numerical Recipes . The Art of Scientific Computing . 3rd edition '' , sec . 5.7 . .. [ 2 ] A. Curtis , M. J. D. Powell , and J. Reid , `` On the estimation of sparse Jacobian matrices '' , Journal of the Institute of Mathematics and its Applications , 13 ( 1974 ) , pp . 117-120 . .. [ 3 ] B. Fornberg , `` Generation of Finite Difference Formulas on Arbitrarily Spaced Grids '' , Mathematics of Computation 51 , 1988 . Examples -- -- -- -- > > > import numpy as np > > > from scipy.optimize import approx_derivative > > > > > > def f ( x , c1 , c2 ) : ... return np.array ( [ x [ 0 ] np.sin ( c1 x [ 1 ] ) , ... x [ 0 ] np.cos ( c2 x [ 1 ] ) ] ) ... > > > x0 = np.array ( [ 1.0 , 0.5 np.pi ] ) > > > approx_derivative ( f , x0 , args= ( 1 , 2 ) ) array ( [ [ 1. , 0 . ] , [ -1. , 0 . ] ] ) Bounds can be used to limit the region of function evaluation . In the example below we compute left and right derivative at point 1.0 . > > > def g ( x ) : ... return x 2 if x > = 1 else x ... > > > x0 = 1.0 > > > approx_derivative ( g , x0 , bounds= ( -np.inf , 1.0 ) ) array ( [ 1 . ] ) > > > approx_derivative ( g , x0 , bounds= ( 1.0 , np.inf ) ) array ( [ 2 . ] ) `` `` '' ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/scipy/optimize/_numdiff.py#L182,Python,yes,book,background,method,related,other,no transfer,no
"`` < -- globalinfo-start -- > An instance filter that discretizes a range of numeric attributes in the dataset into nominal attributes . Discretization is by Fayyad & amp ; Irani 's MDL method ( the default ) . < br > < br > For more information , see : < br > < br > Usama M. Fayyad , Keki B. Irani : Multi-interval discretization of continuousvalued attributes for classification learning . In : Thirteenth International Joint Conference on Articial Intelligence , 1022-1027 , 1993. < br > < br > Igor Kononenko : On Biases in Estimating Multi-Valued Attributes . In : 14th International Joint Conference on Articial Intelligence , 1034-1040 , 1995 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Fayyad1993 , author = { Usama M. Fayyad and Keki B. Irani } , booktitle = { Thirteenth International Joint Conference on Articial Intelligence } , pages = { 1022-1027 } , publisher = { Morgan Kaufmann Publishers } , title = { Multi-interval discretization of continuousvalued attributes for classification learning } , volume = { 2 } , year = { 1993 } } & 64 ; inproceedings { Kononenko1995 , author = { Igor Kononenko } , booktitle = { 14th International Joint Conference on Articial Intelligence } , pages = { 1034-1040 } , title = { On Biases in Estimating Multi-Valued Attributes } , year = { 1995 } , PS = { http : ai.fri.uni-lj.si papers kononenko95-ijcai.ps.gz } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -R & lt ; col1 , col2-col4 , ... & gt ; Specifies list of columns to Discretize . First and last are valid indexes . ( default none ) < pre > < pre > -V Invert matching sense of column indexes. < pre > < pre > -D Output binary attributes for discretized attributes. < pre > < pre > -E Use better encoding of split point for MDL. < pre > < pre > -K Use Kononenko 's MDL criterion. < pre > < -- options-end -- > @ author Len Trigg ( trigg @ cs.waikato.ac.nz ) @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ version $ Revision : 1.7.2.1 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_propertydialog_becoming_JDialog-waikato/weka/filters/supervised/attribute/Discretize.java#L51,Java,yes,conference,algorithm,class,reference,machine learning,formulas to source code,no
"`` Returns the thermal conductivity of water at the current conditions ( W m K ) This function calculates the value of the thermal conductivity of water at the current T and P. The formulas used are from the paper : J. V. Sengers , J. T. R. Watson , `` Improved International Formulations for the Viscosity and Thermal Conductivity of Water Substance '' , J. Phys . Chem . Ref . Data , 15 , 1291 ( 1986 ) . The formulation is accurate for all temperatures and pressures , for steam and for water , even near the critical point . Pressures above 500 MPa and temperature above 900 C are suspect. ``",https://github.com/Cantera/cantera/blob/a9ad75e974b7d1aed642a23f5b58ff7bd27a2d8d/include/cantera/transport/WaterTransport.h#L67,C++,yes,journal,background,n/a,related,science,no transfer,no
"`` `` '' '' This tutorial introduces the LeNet5 neural network architectureusing Theano . LeNet5 is a convolutional neural network , good forclassifying images . This tutorial shows how to build the architecture , and comes with all the hyper-parameters you need to reproduce thepaper 's MNIST results.This implementation simplifies the model in the following ways : - LeNetConvPool does n't implement location-specific gain and bias parameters - LeNetConvPool does n't implement pooling by average , it implements pooling by max . - Digit classification is implemented with a logistic regression rather than an RBF network - LeNet5 was not fully-connected convolutions at second layerReferences : - Y. LeCun , L. Bottou , Y. Bengio and P. Haffner : Gradient-Based Learning Applied to Document Recognition , Proceedings of the IEEE , 86 ( 11 ) :2278-2324 , November 1998. http : yann.lecun.com exdb publis pdf lecun-98.pdf '' '' '' ''",https://github.com/gt-ros-pkg/hrl-assistive/blob/03cf48c417cbc151f2d5441633da72fec1ca9f69/hrl_anomaly_detection/src/hrl_anomaly_detection/feature_extractors/SDA/convolutional_mlp.py#L1,Python,yes,conference,algorithm,file,reference,other,formulas to source code,no
"`` `` '' '' Test for no-cointegration of a univariate equation The null hypothesis is no cointegration . Variables in y0 and y1 are assumed to be integrated of order 1 , I ( 1 ) . This uses the augmented Engle-Granger two-step cointegration test . Constant or trend is included in 1st stage regression , i.e . in cointegrating equation . Warning : The autolag default has changed compared to statsmodels 0.8 . In 0.8 autolag was always None , no the keyword is used and defaults to 'aic ' . Use ` autolag=None ` to avoid the lag search . Parameters -- -- -- -- -- y1 : array_like , 1d first element in cointegrating vector y2 : array_like remaining elements in cointegrating vector trend : str { ' c ' , 'ct ' } trend term included in regression for cointegrating equation ' c ' : constant 'ct ' : constant and linear trend also available quadratic trend 'ctt ' , and no constant 'nc ' method : string currently only 'aeg ' for augmented Engle-Granger test is available . default might change . maxlag : None or int keyword for ` adfuller ` , largest or given number of lags autolag : string keyword for ` adfuller ` , lag selection criterion . if None , then maxlag lags are used without lag search if 'AIC ' ( default ) or 'BIC ' , then the number of lags is chosen to minimize the corresponding information criterion 't-stat ' based choice of maxlag . Starts with maxlag and drops a lag until the t-statistic on the last lag length is significant using a 5 % -sized test return_results : bool for future compatibility , currently only tuple available . If True , then a results instance is returned . Otherwise , a tuple with the test outcome is returned . Set ` return_results=False ` to avoid future changes in return . Returns -- -- -- - coint_t : float t-statistic of unit-root test on residuals pvalue : float MacKinnon 's approximate , asymptotic p-value based on MacKinnon ( 1994 ) crit_value : dict Critical values for the test statistic at the 1 % , 5 % , and 10 % levels based on regression curve . This depends on the number of observations . Notes -- -- - The Null hypothesis is that there is no cointegration , the alternative hypothesis is that there is cointegrating relationship . If the pvalue is small , below a critical size , then we can reject the hypothesis that there is no cointegrating relationship . P-values and critical values are obtained through regression surface approximation from MacKinnon 1994 and 2010 . If the two series are almost perfectly collinear , then computing the test is numerically unstable . However , the two series will be cointegrated under the maintained assumption that they are integrated . In this case the t-statistic will be set to -inf and the pvalue to zero . TODO : We could handle gaps in data by dropping rows with nans in the auxiliary regressions . Not implemented yet , currently assumes no nans and no gaps in time series . References -- -- -- -- -- MacKinnon , J.G . 1994 `` Approximate Asymptotic Distribution Functions for Unit-Root and Cointegration Tests . '' Journal of Business & Economics Statistics , 12.2 , 167-76 . MacKinnon , J.G . 2010 . `` Critical Values for Cointegration Tests . '' Queen 's University , Dept of Economics Working Papers 1227. http : ideas.repec.org p qed wpaper 1227.html `` '' '' ''",https://github.com/statsmodels/statsmodels/blob/d465ebc54533afe2d003ca72287ca866498eb399/statsmodels/tsa/stattools.py#L927,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` < -- globalinfo-start -- > Generates a people database and is based on the paper by Agrawal et al . : < br > R. Agrawal , T. Imielinski , A. Swami ( 1993 ) . Database Mining : A Performance Perspective . IEEE Transactions on Knowledge and Data Engineering . 5 ( 6 ) :914-925 . URL http : www.almaden.ibm.com software quest Publications ByDate.html . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Agrawal1993 , author = { R. Agrawal and T. Imielinski and A. Swami } , journal = { IEEE Transactions on Knowledge and Data Engineering } , note = { Special issue on Learning and Discovery in Knowledge-Based Databases } , number = { 6 } , pages = { 914-925 } , title = { Database Mining : A Performance Perspective } , volume = { 5 } , year = { 1993 } , URL = { http : www.almaden.ibm.com software quest Publications ByDate.html } , PDF = { http : www.almaden.ibm.com software quest Publications papers tkde93.pdf } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -h Prints this help . < pre > < pre > -o & lt ; file & gt ; The name of the output file , otherwise the generated data is printed to stdout . < pre > < pre > -r & lt ; name & gt ; The name of the relation . < pre > < pre > -d Whether to print debug informations . < pre > < pre > -S The seed for random function ( default 1 ) < pre > < pre > -n & lt ; num & gt ; The number of examples to generate ( default 100 ) < pre > < pre > -F & lt ; num & gt ; The function to use for generating the data . ( default 1 ) < pre > < pre > -B Whether to balance the class . < pre > < pre > -P & lt ; num & gt ; The perturbation factor . ( default 0.05 ) < pre > < -- options-end -- > @ author Richard Kirkby ( rkirkby at cs dot waikato dot ac dot nz ) @ author FracPete ( fracpete at waikato dot ac dot nz ) @ version $ Revision : 10203 $ ``",https://github.com/umple/umple/blob/67002649b1a9db2c15b6e9bb5de7b803204e1b14/Umplificator/UmplifiedProjects/weka-umplified-0/src/main/java/weka/datagenerators/classifiers/classification/Agrawal.java#L45,Java,yes,journal,algorithm,class,reference,other,formulas to source code,no
"`` < -- globalinfo-start -- > SVMreg implements the support vector machine for regression . The parameters can be learned using various algorithms . The algorithm is selected by setting the RegOptimizer . The most popular algorithm ( RegSMOImproved ) is due to Shevade , Keerthi et al and this is the default RegOptimizer. < br > < br > For more information see : < br > < br > S.K . Shevade , S.S. Keerthi , C. Bhattacharyya , K.R.K . Murthy : Improvements to the SMO Algorithm for SVM Regression . In : IEEE Transactions on Neural Networks , 1999. < br > < br > A.J . Smola , B. Schoelkopf ( 1998 ) . A tutorial on support vector regression . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Shevade1999 , author = { S.K . Shevade and S.S. Keerthi and C. Bhattacharyya and K.R.K . Murthy } , booktitle = { IEEE Transactions on Neural Networks } , title = { Improvements to the SMO Algorithm for SVM Regression } , year = { 1999 } , PS = { http : guppy.mpe.nus.edu.sg ~mpessk svm ieee_smo_reg.ps.gz } } & 64 ; techreport { Smola1998 , author = { A.J . Smola and B. Schoelkopf } , note = { NeuroCOLT2 Technical Report NC2-TR-1998-030 } , title = { A tutorial on support vector regression } , year = { 1998 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -C & lt ; double & gt ; The complexity constant C. ( default 1 ) < pre > < pre > -N Whether to 0=normalize 1=standardize 2=neither . ( default 0=normalize ) < pre > < pre > -I & lt ; classname and parameters & gt ; Optimizer class used for solving quadratic optimization problem ( default weka.classifiers.functions.supportVector.RegSMOImproved ) < pre > < pre > -K & lt ; classname and parameters & gt ; The Kernel to use . ( default : weka.classifiers.functions.supportVector.PolyKernel ) < pre > < pre > Options specific to optimizer ( '-I ' ) weka.classifiers.functions.supportVector.RegSMOImproved : < pre > < pre > -T & lt ; double & gt ; The tolerance parameter for checking the stopping criterion . ( default 0.001 ) < pre > < pre > -V Use variant 1 of the algorithm when true , otherwise use variant 2 . ( default true ) < pre > < pre > -P & lt ; double & gt ; The epsilon for round-off error . ( default 1.0e-12 ) < pre > < pre > -L & lt ; double & gt ; The epsilon parameter in epsilon-insensitive loss function . ( default 1.0e-3 ) < pre > < pre > -W & lt ; double & gt ; The random number seed . ( default 1 ) < pre > < pre > Options specific to kernel ( '-K ' ) weka.classifiers.functions.supportVector.PolyKernel : < pre > < pre > -D Enables debugging output ( if available ) to be printed . ( default : off ) < pre > < pre > -no-checks Turns off all checks - use with caution ( default : checks on ) < pre > < pre > -C & lt ; num & gt ; The size of the cache ( a prime number ) . ( default : 250007 ) < pre > < pre > -E & lt ; num & gt ; The Exponent to use . ( default : 1.0 ) < pre > < pre > -L Use lower-order terms . ( default : no ) < pre > < -- options-end -- > @ author Remco Bouckaert ( remco @ cs.waikato.ac.nz , rrb @ xm.co.nz ) @ version $ Revision : 1.3 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-5-6/weka/classifiers/functions/SVMreg.java#L54,Java,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` galsgl ( modification of equgad ) to compute supergalactic coordinates from galactic coordinates . j.bennett 30oct-05nov90 inputs : rad = gal lon ( l ) in degrees . double precision . decd = gal lat ( b ) in degrees . double precision . returned : xb = supergalactic latitude ( SGB ) ( -90. to + 90. deg ) . double precision . xl = supergalactic longitude ( SGL ) ( 0-360. deg ) . double precision . Computed values of rotation used for the Eulerian angles : phi = 137.37 deg . ; theta = 83.68 deg . ; psi = 0. deg . Based on gal l , b = 137.37 , 0 deg . at sg SGL , SGB = 0. , 0. and SG North Pole ( 0 , +90 ) at gal l , b = 47.37 , +6.32 deg . References : de Vaucouleurs , G. , A. de Vaucouleurs , and G.H.Corwin , Second Reference Catalog of Bright Galaxies , ( 1976 ) , p.8 Tully , B. , Nearby Galaxies Catalog , ( 1988 ) p. 1,4-5 Note : de Vaucouleurs gives gal l , b 137.29 , 0 for SGL , SGB= 0,0 ; this is not 90 degrees from Galactic North Pole.- used Tully 's value of 137.37 , 0 deg . gal < == > equ equations from tom soifer . see also : Classical Mechanics by Herbert Goldstein ( Addison-Wesley Publ.Co. , c.1950 , 7th printing 1965 ) Section 4-4 on Eulerian Angles ( p.107-109 ) note : such equations also appropriate for gal < == > sgl . See code below for def . of arrays a , b , c . x = cos ( lon ) cos ( lat ) y = sin ( lon ) cos ( lat ) z = sin ( lat ) eq1 = sin ( sgb ) = a ( 1 ) x + a ( 2 ) y + a ( 3 ) z eq2 = cos ( sgb ) cos ( sgl ) = b ( 1 ) x + b ( 2 ) y + b ( 3 ) z eq3 = cos ( sgb ) sin ( sgl ) = c ( 1 ) x + c ( 2 ) y + c ( 3 ) z sgb = arsin ( sin ( sgb ) ) cos ( sgb ) = sqrt ( 1.-sin ( sgb ) sin ( sgb ) ) cos ( sgl ) = eq2 cos ( sgb ) sin ( sgl ) = eq3 cos ( sgb ) sgl = atan2 ( sin ( sgl ) cos ( sgl ) : if ( sgl.lt.0 ) sgl = sgl+2 pi if radians. ``",https://github.com/Caltech-IPAC/firefly/blob/b34374bd3de937e06b4d7682ade0ecb0e555377b/src/firefly/java/edu/caltech/ipac/astro/conv/Galeqd.java#L251,Java,yes,book,algorithm,method,reference,other,formulas to source code,no
"`` `` '' '' Calculates temperature from GOES flux ratio . This function calculates the isothermal temperature of the solar soft X-ray emitting plasma observed by the GOES XRS from the observed flux ratio of the short ( 0.5-4 angstrom ) to long ( 1-8 angstrom ) channels . This function is not intended to be called directly but by _goes_chianti_tem ( ) , although it can be used independently . However , if used independently data preparation , such as correctly rescaling fluxes for some satellites etc . will not be carried out . This is done in _goes_chianti_tem ( ) . Parameters -- -- -- -- -- fluxratio : ` ~astropy.units.Quantity ` Array containing the ratio of short channel to long channel GOES XRS flux measurements . satellite : int ( optional ) Number of GOES satellite used to make observations . Important for correct calibration of data . Default=8 abundances : ( optional ) string equalling 'coronal ' or 'photospheric ' States whether photospheric or coronal abundances should be assumed . Default='coronal ' download : ( optional ) bool If True , the GOES temperature data files are downloaded . It is important to do this if a new version of the files has been generated due to a new CHIANTI version being released or the launch of new GOES satellites since these files were last downloaded . Default=False download_dir : ( optional ) string The directory to download the GOES temperature data file to . Default=SunPy default download directory Returns -- -- -- - temp : ` ~astropy.units.Quantity ` Array of temperature values of same length as longflux and shortflux . Units= [ MK ] Notes -- -- - This function uses csv files representing the modelled relationship between temperature of the soft X-ray emitting plasma and the short to long channel GOES flux ratio . goes_chianti_temp_cor.csv is used when coronal abundances are assumed while goes_chianti_temp_pho.csv is used when photospheric abundances are assumed . ( See make_goes_chianti_temp.py for more detail . ) These files were calculated using the methods of [ 1 ] _ who used the CHIANTI atomic physics database to model the response of the ratio of the short ( 0.5-4 angstrom ) to long ( 1-8 angstrom ) channels of the XRSs onboard various GOES satellites . This method assumes an isothermal plasma , the ionisation equilibria of ( See White et al . 2005 for justification of this last assumption . ) This function is based on goes_get_chianti_temp.pro in SolarSoftWare written in IDL by Stephen White . For correct preparation of GOES data before calculating temperature see _goes_chianti_tem ( ) ( Notes section of docstring ) . References -- -- -- -- -- .. [ 1 ] White , S. M. , Thomas , R. J. , & Schwartz , R. A . 2005 , Sol . Phys. , 227 , 231 , DOI : 10.1007 s11207-005-2445-z .. [ 2 ] Mazzotta , P. , Mazzitelli , G. , Colafrancesco , S. , & Vittorio , N. 1998 , A & AS , 133 , 339 , DOI : 10.1051 aas:1998330 Examples -- -- -- -- > > > from astropy.units import Quantity > > > from sunpy.instr.goes import _goes_get_chianti_temp > > > fluxratio = Quantity ( [ 0.1,0.1 ] ) > > > temp = _goes_get_chianti_temp ( fluxratio , satellite=15 , ... abundances= '' coronal '' ) doctest : +REMOTE_DATA > > > temp doctest : +REMOTE_DATA < Quantity [ 12.27557778 , 12.27557778 ] MK > `` '' '' ''",https://github.com/sunpy/sunpy/blob/cf6df141e0c9a66b94818318eebfa6f906ec9b2f/sunpy/instr/goes.py#L427,Python,yes,journal,algorithm,method,reference,science,paper not available,no
"`` `` '' '' Return nodes in strongly connected components of graph . Parameters -- -- -- -- -- G : NetworkX Graph An directed graph . Returns -- -- -- - comp : list of lists A list of nodes for each component of G. The list is ordered from largest connected component to smallest . Raises -- -- -- NetworkXError : If G is undirected . See Also -- -- -- -- connected_components , weakly_connected_components Notes -- -- - Uses Tarjan 's algorithm with Nuutila 's modifications . Nonrecursive version of algorithm . References -- -- -- -- -- .. [ 1 ] Depth-first search and linear graph algorithms , R. Tarjan SIAM Journal of Computing 1 ( 2 ) :146-160 , ( 1972 ) . .. [ 2 ] On finding the strongly connected components in a directed graph . E. Nuutila and E. Soisalon-Soinen Information Processing Letters 49 ( 1 ) : 9-14 , ( 1994 ) .. `` '' '' ''",https://github.com/dkratzert/DSR/blob/efbb09e1a37268f98b9044ebed880a150c2fa0c8/networkx/algorithms/components/strongly_connected.py#L26,Python,yes,journal,algorithm,method,reference,science,pseudocode to source code,no
"`` Enhance the solution in the convex hull of current atoms with atom norm constraint tau . Used in UpdateFullCorrection class for update step . Minimize the function in the atom domain defined by current atoms , where the solution still need to have atom norm ( defined by current atoms ) less than or equal to tau . We use projected gradient method to solve it , see the `` Enhancement step '' of the following paper : @ code @ article { RaoShaWri:2015Forward -- backward , Author = { Rao , Nikhil and Shah , Parikshit and Wright , Stephen } , Journal = { IEEE Transactions on Signal Processing } , Number = { 21 } , Pages = { 5798 -- 5811 } , Publisher = { IEEE } , Title = { Forward -- backward greedy algorithms for atomic norm regularization } , Volume = { 63 } , Year = { 2015 } } @ endcode @ param function function to be minimized . @ param tau atom norm constraint . @ param stepSize step size for projected gradient method . @ param maxIteration maximum iteration number . @ param tolerance tolerance for projected gradient method. ``",https://github.com/mlpack/mlpack/blob/095f784a30742ff64f4616c362475d6c8ec8ea15/src/mlpack/core/optimizers/fw/atoms.hpp#L138,C++,yes,journal,algorithm,method,reference,machine learning,description to source code,no
"`` `` '' '' The following contains a database of small moleculesData for the G2 97 database are fromRaghavachari , Redfern , and Pople , J. Chem . Phys . Vol . 106 , 1063 ( 1997 ) .See http : www.cse.anl.gov Catalysis_and_Energy_Conversion Computational_Thermochemistry.shtml for the original files.All numbers are experimental values , except for coordinates , which areMP2 ( full ) 6-31G ( d ) optimized geometries ( from http : www.cse.anl.gov OldCHMwebsiteContent compmat G2-97.htm ) Atomic species : ref : Curtiss et al . JCP 106 , 1063 ( 1997 ) . 'Enthalpy ' is the experimental enthalpies of formation at 0K'thermal correction ' is the thermal corrections H ( 298 ) -H ( 0 ) Molecular species : ref : Staroverov et al . JCP 119 , 12129 ( 2003 ) 'Enthalpy ' is the experimental enthalpies of formation at 298K'ZPE ' is the zero-point energies'thermal correction ' is the thermal enthalpy corrections H ( 298K ) - H_exp ( 0K ) ZPE and thermal corrections are estimated from B3LYP geometries and vibrations.Experimental ionization potentials are from http : srdata.nist.gov cccbdb .For details about G2-1 and G2-2 sets see doi:10.1063 1.477422 . '' '' '' ''",https://github.com/svn2github/ASE-DB/blob/adfc97064e8b204548b1b57c5b09a024efe33bca/ase/data/g2.py#L1,Python,yes,journal,background,n/a,related,machine learning,no transfer,no
"`` brief Spherical harmonic series This class evaluates the spherical harmonic sum verbatim V ( x , y , z ) = sum ( n = 0..N ) [ q^ ( n+1 ) sum ( m = 0..n ) [ ( C [ n , m ] cos ( m lambda ) + S [ n , m ] sin ( m lambda ) ) P [ n , m ] ( cos ( theta ) ) ] ] endverbatim where - < i > p < i > < sup > 2 < sup > = < i > x < i > < sup > 2 < sup > + < i > y < i > < sup > 2 < sup > , - < i > r < i > < sup > 2 < sup > = < i > p < i > < sup > 2 < sup > + < i > z < i > < sup > 2 < sup > , - e q = < i > a < i > < i > r < i > , - & theta ; = atan2 ( e p , e z ) = the spherical e colatitude , - & lambda ; = atan2 ( e y , e x ) = the longitude . - P < sub > < i > nm < i > < sub > ( e t ) is the associated Legendre polynomial of degree e n and order e m. Two normalizations are supported for P < sub > < i > nm < i > < sub > - fully normalized denoted by SphericalHarmonic : :FULL . - Schmidt semi-normalized denoted by SphericalHarmonic : :SCHMIDT . Clenshaw summation is used for the sums over both e n and e m. This allows the computation to be carried out without the need for any temporary arrays . See SphericalEngine.cpp for more information on the implementation . References : - C. W. Clenshaw , < a href= '' https : dx.doi.org 10.1090 S0025-5718-1955-0071856-0 '' > A note on the summation of Chebyshev series < a > , % Math . Tables Aids Comput . 9 ( 51 ) , 118 -- 120 ( 1955 ) . - R. E. Deakin , Derivatives of the earth 's potentials , Geomatics Research Australasia 68 , 31 -- 60 , ( June 1998 ) . - W. A. Heiskanen and H. Moritz , Physical Geodesy , ( Freeman , San Francisco , 1967 ) . ( See Sec . 1-14 , for a definition of Pbar . ) - S. A. Holmes and W. E. Featherstone , < a href= '' https : dx.doi.org 10.1007 s00190-002-0216-2 '' > A unified approach to the Clenshaw summation and the recursive computation of very high degree and order normalised associated Legendre functions < a > , J. Geodesy 76 ( 5 ) , 279 -- 299 ( 2002 ) . - C. C. Tscherning and K. Poder , < a href= '' http : cct.gfy.ku.dk publ_cct cct80.pdf '' > Some geodetic applications of Clenshaw summation < a > , Boll . Geod . Sci . Aff . 41 ( 4 ) , 349 -- 375 ( 1982 ) . Example of use : include example-SphericalHarmonic.cpp ``",https://github.com/NREL/OpenStudio/blob/8a27a9ffa53362d1fe60079b4637cde87cca92b7/openstudiocore/src/geographic_lib/include/GeographicLib/SphericalHarmonic.hpp#L20,C++,yes,journal,algorithm,class,reference,science,formulas to source code,no
"`` An implementation of the Vegas congestion control algorithm for FreeBSD , based on L. S. Brakmo and L. L. Peterson , `` TCP Vegas : end to end congestion avoidance on a global internet '' , IEEE J. Sel . Areas Commun. , vol . 13 , no . 8 , pp . 1465-1480 , Oct. 1995 . The original Vegas duplicate ack policy has not been implemented , since clock ticks are not as coarse as they were ( i.e . 500ms ) when Vegas was designed . Also , packets are timed once per RTT as in the original paper . Originally released as part of the NewTCP research project at Swinburne University of Technology 's Centre for Advanced Internet Architectures , Melbourne , Australia , which was made possible in part by a grant from the Cisco University Research Program Fund at Community Foundation Silicon Valley . More details are available at : http : caia.swin.edu.au urp newtcp ``",https://github.com/lattera/freebsd/blob/401a161083850a9a4ce916f37520c084cff1543b/sys/netinet/cc/cc_vegas.c#L39,C,yes,journal,algorithm,file,reference,networks and os,description to source code,no
"`` r '' '' '' This incremental pressure correction scheme ( IPCS ) is an operator splitting scheme that follows the idea of Goda [ 1 ] _ and Simo [ 2 ] _ . This scheme preserves the exact same stability properties as Navier-Stokes and hence does not introduce additional dissipation in the flow ( FIXME : needs verification ) . The idea is to replace the unknown free-surface with an approximation . This is chosen as the free-surface solution from the previous solution . The time discretization is done using a : math : ` theta ` -scheme , the convection , friction and divergence are handled semi-implicitly . Thus , we have a discretized version of the shallow water equations as .. math : : frac { 1 } { Delta t } left ( u^ { n+1 } -u^ { n } right ) - abla cdot u abla u^ { n+ theta } +u^ cdot abla u^ { n+ theta } +g abla eta^ { n+ theta } + frac { c_b + c_t } { H^n } | u^n | u^ { n+ theta } & = f_u^ { n+ theta } , frac { 1 } { Delta t } left ( eta^ { n+1 } - eta^ { n } right ) + abla cdot left ( H^ { n } u^ { n+ theta } right ) & = 0 , where : math : ` square^ { n+ theta } = theta { square } ^ { n+1 } + ( 1- theta ) square^n , theta in [ 0 , 1 ] ` and : math : ` u^ = frac { 3 } { 2 } u^n - frac { 1 } { 2 } u^ { n-1 } ` . This convection term is unconditionally stable , and with : math : ` theta=0.5 ` , this equation is second order in time and space [ 2 ] _ ( FIXME : Needs verification ) . For the operator splitting , we use the free-surface solution from the previous timestep as an estimation , giving an equation for a tentative velocity , : math : ` tilde { u } ^ { n+1 } ` : .. math : : frac { 1 } { Delta t } left ( tilde { u } ^ { n+1 } -u^ { n } right ) - abla cdot u abla tilde { u } ^ { n+ theta } + u^ cdot abla tilde u^ { n+ theta } +g abla eta^ { n } + frac { c_b + c_t } { H^n } | u^n | tilde u^ { n+ theta } = f_u^ { n+ theta } . This tenative velocity does not satisfy the divergence equation , and thus we define a velocity correction : math : ` u^c=u^ { n+1 } - tilde { u } ^ { n+1 } ` . Substracting the second equation from the first , we see that .. math : : frac { 1 } { Delta t } u^c - theta abla cdot u abla u^c + theta u^ cdot abla u^ { c } + g theta abla left ( eta^ { n+1 } - eta^n right ) + theta frac { c_b + c_t } { H^n } | u^n | u^ { c } & = 0 , frac { 1 } { Delta t } left ( eta^ { n+1 } - eta^ { n } right ) + theta abla cdot left ( H^ { n } u^c right ) & = - abla cdot left ( H^ { n } tilde { u } ^ { n+ theta } right ) . The operator splitting is a first order approximation , : math : ` O ( Delta t ) ` , so we can , without reducing the order of the approximation simplify the above to .. math : : frac { 1 } { Delta t } u^c + g theta abla left ( eta^ { n+1 } - eta^n right ) & = 0 , frac { 1 } { Delta t } left ( eta^ { n+1 } - eta^ { n } right ) + theta abla cdot left ( H^ { n } u^c right ) & = - abla cdot left ( H^ { n } tilde { u } ^ { n+ theta } right ) , which is reducible to the problem : .. math : : eta^ { n+1 } - eta^ { n } - g Delta t^2 theta^2 abla cdot left ( H^ { n+1 } abla left ( eta^ { n+1 } - eta^n right ) right ) = - Delta t abla cdot left ( H^ { n } tilde { u } ^ { n+ theta } right ) . The corrected velocity is then easily calculated from .. math : : u^ { n+1 } = tilde { u } ^ { n+1 } - Delta tg theta abla left ( eta^ { n+1 } - eta^n right ) The scheme can be summarized in the following steps : . Replace the pressure with a known approximation and solve for a tenative velocity : math : ` tilde u^ { n+1 } ` . . Solve a free-surface correction equation for the free-surface , : math : ` eta^ { n+1 } ` . Use the corrected pressure to find the velocity correction and calculate : math : ` u^ { n+1 } ` . Update t , and repeat . Remarks : - This solver only works with transient problems , that is with : class : ` opentidalfarm.problems.sw.SWProblem ` . - This solver supports large eddy simulation ( LES ) . The LES model is implemented via the : class : ` opentidalfarm.solvers.les.LES ` class . .. [ 1 ] Goda , Katuhiko . A multistep technique with implicit difference schemes for calculating two-or three-dimensional cavity flows . Journal of Computational Physics 30.1 ( 1979 ) : 76-95 . .. [ 2 ] Simo , J. C. , and F. Armero . Unconditional stability and long-term behavior of transient algorithms for the incompressible Navier-Stokes and Euler equations . Computer Methods in Applied Mechanics and Engineering 111.1 ( 1994 ) : 111-154. `` '' '' ''",https://github.com/OpenTidalFarm/OpenTidalFarm/blob/b19848f6dbaa129e89c3fb29bbb6829284f7a419/opentidalfarm/solvers/ipcs_sw_solver.py#L63,Python,yes,journal,algorithm,class,reference,other,paper not available,no
"`` `` '' '' Internal function for isolation positive roots up to given precision . References ========== .. [ 1 ] Alkiviadis G. Akritas and Adam W. Strzebonski : A Comparative Study of Two Real Root Isolation Methods . Nonlinear Analysis : Modelling and Control , Vol . 10 , No . 4 , 297-304 , 2005 . .. [ 2 ] Alkiviadis G. Akritas , Adam W. Strzebonski and Panagiotis S. Vigklas : Improving the Performance of the Continued Fractions Method Using new Bounds of Positive Roots . Nonlinear Analysis : Modelling and Control , Vol . 13 , No . 3 , 265-279 , 2008. `` '' '' ''",https://github.com/diofant/diofant/blob/4d9f43afde5cae9c6c20890e4ee7352f3f296166/diofant/polys/rootisolation.py#L277,Python,yes,journal,algorithm,method,reference,science,pseudocode to source code,no
"`` This file implements moduluar exponentiation using Montgomery 's method for modular reduction . This file implements the method described as `` Improvement 2 '' in the paper `` A Cryptogrpahic Library for the Motorola DSP56000 '' by Stephen R. Dusse ' and Burton S. Kaliski Jr. published in `` Advances in Cryptology : Proceedings of EUROCRYPT '90 '' `` Lecture Notes in Computer Science '' volume 473 , 1991 , pg 230-244 , published by Springer Verlag. ``",https://github.com/qtproject/qtwebengine-chromium/blob/b45f07bfbe74c333f1017810c2409e1aa6077a1b/chromium/third_party/nss/nss/lib/freebl/mpi/mpmontg.c#L5,C++,yes,conference,algorithm,file,reference,other,pseudocode to source code,no
"`` Algorithm for detecting subspace hierarchies . Reference : < p > E. Achtert , C. B√∂hm , H.-P. Kriegel , P. Kr√∂ger , I. M√ºller-Gorman , A. Zimek : < br > Detection and Visualization of Subspace Cluster Hierarchies . < br > In Proc . 12th International Conference on Database Systems for Advanced Applications ( DASFAA ) , Bangkok , Thailand , 2007 . < p > @ author Elke Achtert @ since 0.2 @ apiviz.uses DiSHPreferenceVectorIndex @ apiviz.has SubspaceModel @ apiviz.has DiSHClusterOrder @ param < V > the type of NumberVector handled by this Algorithm ``",https://github.com/elki-project/elki/blob/3e820f9f6382cb82e40955692337737c24f94b54/elki/src/main/java/de/lmu/ifi/dbs/elki/algorithm/clustering/subspace/DiSH.java#L83,Java,yes,conference,algorithm,class,reference,data science,pseudocode to source code,no
"`` Budget Fair Queueing ( BFQ ) I O scheduler . Based on ideas and code from CFQ : Copyright ( C ) 2003 Jens Axboe < axboe @ kernel.dk > Copyright ( C ) 2008 Fabio Checconi < fabio @ gandalf.sssup.it > Paolo Valente < paolo.valente @ unimore.it > Copyright ( C ) 2010 Paolo Valente < paolo.valente @ unimore.it > Arianna Avanzini < avanzini @ google.com > Copyright ( C ) 2017 Paolo Valente < paolo.valente @ linaro.org > This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation ; either version 2 of the License , or ( at your option ) any later version . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . BFQ is a proportional-share I O scheduler , with some extra low-latency capabilities . BFQ also supports full hierarchical scheduling through cgroups . Next paragraphs provide an introduction on BFQ inner workings . Details on BFQ benefits , usage and limitations can be found in Documentation block bfq-iosched.txt . BFQ is a proportional-share storage-I O scheduling algorithm based on the slice-by-slice service scheme of CFQ . But BFQ assigns budgets , measured in number of sectors , to processes instead of time slices . The device is not granted to the in-service process for a given time slice , but until it has exhausted its assigned budget . This change from the time to the service domain enables BFQ to distribute the device throughput among processes as desired , without any distortion due to throughput fluctuations , or to device internal queueing . BFQ uses an ad hoc internal scheduler , called B-WF2Q+ , to schedule processes according to their budgets . More precisely , BFQ schedules queues associated with processes . Each process queue is assigned a user-configurable weight , and B-WF2Q+ guarantees that each queue receives a fraction of the throughput proportional to its weight . Thanks to the accurate policy of B-WF2Q+ , BFQ can afford to assign high budgets to I O-bound processes issuing sequential requests ( to boost the throughput ) , and yet guarantee a low latency to interactive and soft real-time applications . In particular , to provide these low-latency guarantees , BFQ explicitly privileges the I O of two classes of time-sensitive applications : interactive and soft real-time . In more detail , BFQ behaves this way if the low_latency parameter is set ( default configuration ) . This feature enables BFQ to provide applications in these classes with a very low latency . To implement this feature , BFQ constantly tries to detect whether the I O requests in a bfq_queue come from an interactive or a soft real-time application . For brevity , in these cases , the queue is said to be interactive or soft real-time . In both cases , BFQ privileges the service of the queue , over that of non-interactive and non-soft-real-time queues . This privileging is performed , mainly , by raising the weight of the queue . So , for brevity , we call just weight-raising periods the time periods during which a queue is privileged , because deemed interactive or soft real-time . The detection of soft real-time queues applications is described in detail in the comments on the function bfq_bfqq_softrt_next_start . On the other hand , the detection of an interactive queue works as follows : a queue is deemed interactive if it is constantly non empty only for a limited time interval , after which it does become empty . The queue may be deemed interactive again ( for a limited time ) , if it restarts being constantly non empty , provided that this happens only after the queue has remained empty for a given minimum idle time . By default , BFQ computes automatically the above maximum time interval , i.e. , the time interval after which a constantly non-empty queue stops being deemed interactive . Since a queue is weight-raised while it is deemed interactive , this maximum time interval happens to coincide with the ( maximum ) duration of the weight-raising for interactive queues . Finally , BFQ also features additional heuristics for preserving both a low latency and a high throughput on NCQ-capable , rotational or flash-based devices , and to get the job done quickly for applications consisting in many I O-bound processes . NOTE : if the main or only goal , with a given device , is to achieve the maximum-possible throughput at all times , then do switch off all low-latency heuristics for that device , by setting low_latency to 0 . BFQ is described in [ 1 ] , where also a reference to the initial , more theoretical paper on BFQ can be found . The interested reader can find in the latter paper full details on the main algorithm , as well as formulas of the guarantees and formal proofs of all the properties . With respect to the version of BFQ presented in these papers , this implementation adds a few more heuristics , such as the ones that guarantee a low latency to interactive and soft real-time applications , and a hierarchical extension based on H-WF2Q+ . B-WF2Q+ is based on WF2Q+ , which is described in [ 2 ] , together with H-WF2Q+ , while the augmented tree used here to implement B-WF2Q+ with O ( log N ) complexity derives from the one introduced with EEVDF in [ 3 ] . [ 1 ] P. Valente , A. Avanzini , `` Evolution of the BFQ Storage I O Scheduler '' , Proceedings of the First Workshop on Mobile System Technologies ( MST-2015 ) , May 2015. http : algogroup.unimore.it people paolo disk_sched mst-2015.pdf [ 2 ] Jon C.R . Bennett and H. Zhang , `` Hierarchical Packet Fair Queueing Algorithms '' , IEEE ACM Transactions on Networking , 5 ( 5 ) :675-689 , Oct 1997. http : www.cs.cmu.edu ~hzhang papers TON-97-Oct.ps.gz [ 3 ] I. Stoica and H. Abdel-Wahab , `` Earliest Eligible Virtual Deadline First : A Flexible and Accurate Mechanism for Proportional Share Resource Allocation '' , technical report . http : www.cs.berkeley.edu ~istoica papers eevdf-tr-95.pdf ``",https://github.com/sergey-senozhatsky/linux-next-ss/blob/2b226612008ecdd3e9aed83aa886784798c8b335/block/bfq-iosched.c#L1,C,yes,conference,background,n/a,related,networks and os,no transfer,no
"`` `` '' '' Implement equation used by the Geological Survey of Canada ( GSC ) for the 2010 Eastern Canada National Seismic Hazard Model . The equation fits the table values defined by Gail M. Atkinson and David M. Boore in `` Ground-Motion Relations for Eastern North America '' , Bullettin of the Seismological Society of America , Vol . 85 , No . 1 , pp . 17-30 , February 1995 . Table of coefficients were provided by GSC and are associated to the 'Lower Limit ' case ( that is mean value decreased ) . `` '' '' ''",https://github.com/gem/oq-engine/blob/fa0c4be562412d9ab6eaf093db1844fa9662b743/openquake/hazardlib/gsim/atkinson_boore_1995.py#L132,Python,yes,journal,numbers,class,reference,science,numbers to hard coded values,no
"`` `` '' '' ( Used only if `` { TS_GLOBAL } '' is selected for thresholding strategy ) The intensity threshold affects the decision of whether each pixelwill be considered foreground ( objects region ( s ) of interest ) or background . Ahigher threshold value will result in only the brightest regions beingidentified , whereas a lower threshold value will include dim regions.You can have the threshold automatically calculated from a choice ofseveral methods , or you can enter a number manually between 0 and 1for the threshold.Both the automatic and manual options have advantages and disadvantages.|image0| An automatically-calculated threshold adapts to changes inlighting staining conditions between images and is usually morerobust accurate . In the vast majority of cases , an automatic method issufficient to achieve the desired thresholding , once the proper methodis selected . In contrast , an advantage of a manually-entered number isthat it treats every image identically , so use this option when you havea good sense for what the threshold should be across all images . To helpdetermine the choice of threshold manually , you can inspect the pixelintensities in an image of your choice . { HELP_ON_PIXEL_INTENSITIES } |image1| The manual method is not robust with regard to slight changesin lighting staining conditions between images . The automatic methodsmay occasionally produce a poor threshold for unusual or artifactualimages . It also takes a small amount of time to calculate , which can addto processing time for analysis runs on a large number of images.The threshold that is used for each image is recorded as a per-imagemeasurement , so if you are surprised by unusual measurements from one ofyour images , you might check whether the automatically calculatedthreshold was unusually high or low compared to the other images . Seethe FlagImage module if you would like to flag an image based on thethreshold value.There are a number of methods for finding thresholds automatically : - { TM_OTSU } : This approach calculates the threshold separating the two classes of pixels ( foreground and background ) by minimizing the variance within the each class . |image2| This method is a good initial approach if you do not know much about the image characteristics of all the images in your experiment , especially if the percentage of the image covered by foreground varies substantially from image to image . |image3| Our implementation of Otsu ‚Äô s method allows for assigning the threshold value based on splitting the image into either two classes ( foreground and background ) or three classes ( foreground , mid-level , and background ) . See the help below for more details.- { TM_ROBUST_BACKGROUND } : This method assumes that the background distribution approximates a Gaussian by trimming the brightest and dimmest X % of pixel intensities , where you choose a suitable percentage . It then calculates the mean and standard deviation of the remaining pixels and calculates the threshold as the mean + N times the standard deviation , where again you choose the number of standard deviations to suit your images . |image4| This thresholding method can be helpful if the majority of the image is background . It can also be helpful if your images vary in overall brightness , but the objects of interest are consistently N times brighter than the background level of the image.- { TM_LI } : The distributions of intensities that define foreground and background are used as estimates for probability distributions that produce the intensities of foreground and background pixels . For each possible threshold the cross-entropy between the foreground and background distributions is calculated and the lowest cross-entropy value is chosen as the final threshold . The lowest cross-entropy can be interpreted as the value where the information shared between the two probability distributions is the highest . On average , given a pixel of an arbitrary intensity , the likelihood it came from the foreground or background would be at its highest.- { TM_MANUAL } : Enter a single value between zero and one that applies to all images and is thus independent of the input image . |image5| This approach is useful if the input image has a stable or negligible background , or if the input image is the probability map output of a pixel-based classifier ( in which case , a value of 0.5 should be chosen ) . If the input image is already binary ( i.e. , where the foreground is 1 and the background is 0 ) , a manual value of 0.5 will identify the objects.- { TM_MEASUREMENT } : Use a prior image measurement as the threshold . The measurement should have values between zero and one . This strategy can also be used to apply a pre-calculated threshold imported as per-image metadata . References - Sezgin M , Sankur B ( 2004 ) ‚Äú Survey over image thresholding techniques and quantitative performance evaluation. ‚Äù Journal of Electronic Imaging , 13 ( 1 ) , 146-165 . ( ` link ` _ ) .. _link : https : doi.org 10.1117 1.1631315.. |image0| image : : { PROTIP_RECOMMEND_ICON } .. |image1| image : : { PROTIP_AVOID_ICON } .. |image2| image : : { PROTIP_RECOMMEND_ICON } .. |image3| image : : { TECH_NOTE_ICON } .. |image4| image : : { PROTIP_RECOMMEND_ICON } .. |image5| image : : { PROTIP_RECOMMEND_ICON } '' '' '' ''",https://github.com/CellProfiler/CellProfiler/blob/6a349582e5923f69d6c544b60765404995256044/cellprofiler/modules/threshold.py#L115,Python,yes,journal,background,n/a,related,computer vision,no transfer,no
"`` < -- globalinfo-start -- > WrapperSubsetEval : < br > < br > Evaluates attribute sets by using a learning scheme . Cross validation is used to estimate the accuracy of the learning scheme for a set of attributes. < br > < br > For more information see : < br > < br > Ron Kohavi , George H. John ( 1997 ) . Wrappers for feature subset selection . Artificial Intelligence . 97 ( 1-2 ) :273-324 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Kohavi1997 , author = { Ron Kohavi and George H. John } , journal = { Artificial Intelligence } , note = { Special issue on relevance } , number = { 1-2 } , pages = { 273-324 } , title = { Wrappers for feature subset selection } , volume = { 97 } , year = { 1997 } , ISSN = { 0004-3702 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -B & lt ; base learner & gt ; class name of base learner to use for accuracy estimation . Place any classifier options LAST on the command line following a `` -- '' . eg . : -B weka.classifiers.bayes.NaiveBayes ... -- -K ( default : weka.classifiers.rules.ZeroR ) < pre > < pre > -F & lt ; num & gt ; number of cross validation folds to use for estimating accuracy . ( default=5 ) < pre > < pre > -R & lt ; seed & gt ; Seed for cross validation accuracy testimation . ( default = 1 ) < pre > < pre > -T & lt ; num & gt ; threshold by which to execute another cross validation ( standard deviation -- -expressed as a percentage of the mean ) . ( default : 0.01 ( 1 % ) ) < pre > < pre > -E & lt ; acc | rmse | mae | f-meas | auc & gt ; Performance evaluation measure to use for selecting attributes . ( Default = accuracy for discrete class and rmse for numeric class ) < pre > < pre > Options specific to scheme weka.classifiers.rules.ZeroR : < pre > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < -- options-end -- > @ author Mark Hall ( mhall @ cs.waikato.ac.nz ) @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/beforeMoveCodeToPackages/weka/src/main/java/weka/attributeSelection/WrapperSubsetEval.java#L50,Java,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` Glare point-spread-function ( PSF ) based on the human visual system in non-dark-adapted ( `` photopic '' ) state . From [ SSZG95 ] : Spencer , G. , Shirley , P. , Zimmerman , K. , Greenberg , D.P . `` Physically based glare effect for digital images '' Proceedings of SIGGRAPH , pp . 325‚Äì334 . ACM , New York ( 1995 ) ``",https://github.com/snogglethorpe/snogray/blob/7f2ddc92de37697cb72d66d6792444a7e4fc5ac6/glare/photopic-glare-psf.cc#L12,C++,yes,journal,algorithm,file,reference,computer vision,formulas to source code,no
"`` r '' '' '' Calculates compressed-liquid density of a mixture , using the Aalto-Keskinen modification of Chang-Zhao correlation .. math : : T_ { cm } = frac { sum_i sum_jx_ix_j left ( V_i^oT_ { ci } V_j^oT_ { cj } right ) ^ { 1 2 } } { V_m^o } .. math : : V_m^o = frac { sum_i xiV_i^o + 3 sum_i x_iV_i^ { o^ { 2 3 } } sum_i x_iV_i^ { o^ { 1 3 } } } { 4 } .. math : : P_ { cm } = frac { left ( 0.291-0.08 omega_ { SRKm } right ) RT_ { cm } } { V_ { cm } } .. math : : omega_ { SRKm } = left ( sum_i x_i omega_ { SRKi } right ) ^2 Parameters -- -- -- -- -- T : float Temperature , [ K ] P : float Pressure , [ Pa ] xi : list Mole fractions of components , [ - ] Tci : list Critical temperature of components , [ K ] Pci : list Critical pressure of components , [ Pa ] Vci : list Critical volume of components , [ m¬≥ kg ] wi : list Acentric factor ( SRK optimized ) of components , [ - ] Mi : list Molecular weight of components , [ g mol ] rhos : float Boiling point liquid density , [ kg m¬≥ ] Returns -- -- -- - rho : float High-pressure liquid density , [ kg m¬≥ ] Examples -- -- -- -- Example 5-4 from [ 3 ] _ ; 70 % ethane , 30 % nC10 at 344.26K and 10000psi > > > P = unidades.Pressure ( 10000 , `` psi '' ) > > > xi = [ 0.7 , 0.3 ] > > > Tci = [ 305.32 , 617.7 ] > > > Pci = [ 48.72e5 , 21.1e5 ] > > > Vci = [ 145.5 1000 , 624 1000 ] > > > wi = [ 0.099 , 0.491 ] > > > Mi = [ 1 , 1 ] > > > args = ( 344.26 , P , xi , Tci , Pci , Vci , wi , Mi , 1 116.43 1000 ) > > > `` % 0.2f '' % ( 1 RhoL_AaltoKeskinenMix ( args ) .gcc ) '99.05 ' References -- -- -- -- -- [ 11 ] _ Aalto , M. , Keskinen , K.I. , Aittamaa , J. , Liukkonen , S. An Improved Correlation for Compressed Liquid Densities of Hydrocarbons . Part 2 . Mixtures . Fluid Phase Equilibria 114 ( 1996 ) 21-35 [ 3 ] _ Poling , Bruce E. The Properties of Gases and Liquids . 5th edition . New York : McGraw-Hill Professional , 2000. `` '' '' ''",https://github.com/jjgomera/pychemqt/blob/e048915d151e1688856157184e9fc7376ca3c9b2/lib/mezcla.py#L661,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` This function implements an exact SNP test of Hardy-Weinberg Equilibrium as described in Wigginton , JE , Cutler , DJ , and Abecasis , GR ( 2005 ) A Note on Exact Tests of Hardy-Weinberg Equilibrium . American Journal of Human Genetics . 76 : 887 - 893 . The original version was written by Jan Wigginton . This version was written by Christopher Chang . It contains the following improvements over the original SNPHWE ( ) : - Proper handling of > 64k genotypes . Previously , there was a potential integer overflow . - Detection and efficient handling of floating point overflow and underflow . E.g . instead of summing a tail all the way down , the loop stops once the latest increment underflows the partial sum 's 53-bit precision ; this results in a large speedup when max heterozygote count > 1k . - No malloc ( ) call . It 's only necessary to keep track of a few partial sums . - Support for the mid-p variant of this test . See Graffelman J , Moreno V ( 2013 ) The mid p-value in exact tests for Hardy-Weinberg equilibrium . Note that the HweThresh ( ) function below is a lot more efficient for testing against a p-value inclusion threshold . HweP ( ) should only be used if you need the actual p-value . ''",https://github.com/chrchang/plink-ng/blob/0ad8bbb191a98b8a4549799b174462607da99018/2.0/plink2_stats.cc#L1176,C,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` class ThresholdMaximumConnectedComponentsImageFilter brief Finds the threshold value of an image based on maximizing the number of objects in the image that are larger than a given minimal size . par This method is based on Topological Stable State Thresholding to calculate the threshold set point . This method is particularly effective when there are a large number of objects in a microscopy image . Compiling in Debug mode and enable the debug flag for this filter to print debug information to see how the filter focuses in on a threshold value . Please see the Insight Journal 's MICCAI 2005 workshop for a complete description . References are below . par Parameters The MinimumObjectSizeInPixels parameter is controlled through the class Get SetMinimumObjectSizeInPixels ( ) method . Similar to the standard itk : :BinaryThresholdImageFilter the Get SetInside and Get SetOutside values of the threshold can be set . The GetNumberOfObjects ( ) and GetThresholdValue ( ) methods return the number of objects above the minimum pixel size and the calculated threshold value . par Automatic Thresholding in ITK There are multiple methods to automatically calculate the threshold intensity value of an image . As of version 4.0 , ITK has a Thresholding ( ITKThresholding ) module which contains numerous automatic thresholding methods.implements two of these . Topological Stable State Thresholding works well on images with a large number of objects to be counted . par References : 1 ) Urish KL , August J , Huard J . `` Unsupervised segmentation for myofiber counting in immunoflourescent images '' . Insight Journal . ISC NA-MIC MICCAI Workshop on Open-Source Software ( 2005 ) Dspace handle : https : hdl.handle.net 1926 48 2 ) Pikaz A , Averbuch , A . `` Digital image thresholding based on topological stable-state '' . Pattern Recognition , 29 ( 5 ) : 829-843 , 1996. par Questions : email Ken Urish at ken.urish ( at ) gmail.com Please cc the itk list serve for archival purposes . ingroup ITKConnectedComponents ``",https://github.com/InsightSoftwareConsortium/ITK/blob/a077e98358a87c2b507e5ee9c27939918a9120ea/Modules/Segmentation/ConnectedComponents/include/itkThresholdMaximumConnectedComponentsImageFilter.h#L30,C++,yes,journal,background,n/a,related,computer vision,no transfer,no
"`` Testing cases for numerical optimization problems For both unconstrained problems : MorÔøΩÔøΩ , J.J. , Garbow , B.S . and Hillstrom , K.E. , Testing Unconstrained Optimization Software , ACM Trans . Math . Software 7 ( 1981 ) , 17-41 . And box-constrained problems : Gay , D.M. , A trust-region approach to linearly constrained optimization , pp . 72-105 in : Numerical Analysis ( Griffiths , D.F. , ed . ) , Lecture Notes in Mathematics 1066 , Springer , Berlin 1984. website : http : www.mat.univie.ac.at ~neum glopt bounds.html @ author hongning ``",https://github.com/Linda-sunshine/IR_Base/blob/b415103a6e90a604401534b74cef7ec9b32e5f8b/src/LBFGS/optimzationTest/QuadraticTest.java#L7,Java,yes,journal,background,n/a,related,machine learning,no transfer,no
"`` Cycle breaker implementation that uses a greedy algorithm . Inspired by < ul > < li > Peter Eades , Xuemin Lin , W. F. Smyth , A fast and effective heuristic for the feedback arc set problem . < i > Information Processing Letters < i > 47 ( 6 ) , pp . 319-323 , 1993. < li > < li > Giuseppe di Battista , Peter Eades , Roberto Tamassia , Ioannis G. Tollis , < i > Graph Drawing : Algorithms for the Visualization of Graphs < i > , Prentice Hall , New Jersey , 1999 ( Section 9.4 ) . < li > < ul > < p > This cycle breaker does n't support layer constraints out of the box . If layer constraints should be observed , { @ link org.eclipse.elk.alg.layered.intermediate.EdgeAndLayerConstraintEdgeReverser } and { @ link org.eclipse.elk.alg.layered.intermediate.LayerConstraintProcessor } should be used. < p > < dl > < dt > Precondition : < dt > < dd > none < dd > < dt > Postcondition : < dt > < dd > the graph has no cycles , but possibly new nodes and edges < dd > < dl > @ see org.eclipse.elk.alg.layered.intermediate.EdgeAndLayerConstraintEdgeReverser @ see org.eclipse.elk.alg.layered.intermediate.LayerConstraintProcessor @ author msp @ kieler.design 2012-08-10 chsch grh @ kieler.rating yellow 2012-11-13 review KI-33 by grh , akoc ``",https://github.com/eclipse/elk/blob/150fc359a36181eb59de5b4d0e9728820d67dfe2/plugins/org.eclipse.elk.alg.layered/src/org/eclipse/elk/alg/layered/p1cycles/GreedyCycleBreaker.java#L31,Java,yes,journal,algorithm,class,reference,other,pseudocode to source code,no
"`` class QuadEdge brief Base class for the implementation of a quad-edge data structure as proposed in `` Guibas and Stolfi 1985 '' author Alexandre Gouaillard , Leonardo Florez-Valencia , Eric Boix This implementation was contributed as a paper to the Insight Journal https : hdl.handle.net 1926 306 sa `` Accessing adjacent edges . '' ingroup MeshObjects ingroup ITKQuadEdgeMesh ``",https://github.com/InsightSoftwareConsortium/ITK/blob/a077e98358a87c2b507e5ee9c27939918a9120ea/Modules/Core/QuadEdgeMesh/include/itkQuadEdge.h#L209,C++,yes,journal,algorithm,class,reference,computer vision,description to source code,no
"`` `` '' '' Implement search_light analysis using an arbitrary type of classifier . Parameters -- -- -- -- -- - mask_img : Niimg-like object See http : nilearn.github.io manipulating_images input_output.html boolean image giving location of voxels containing usable signals . process_mask_img : Niimg-like object , optional See http : nilearn.github.io manipulating_images input_output.html boolean image giving voxels on which searchlight should be computed . radius : float , optional radius of the searchlight ball , in millimeters . Defaults to 2. estimator : 'svr ' , 'svc ' , or an estimator object implementing 'fit ' The object to use to fit the data n_jobs : int , optional . Default is -1 . The number of CPUs to use to do the computation . -1 means 'all CPUs ' . scoring : string or callable , optional The scoring strategy to use . See the scikit-learn documentation If callable , takes as arguments the fitted estimator , the test data ( X_test ) and the test target ( y_test ) if y is not None . cv : cross-validation generator , optional A cross-validation generator . If None , a 3-fold cross validation is used or 3-fold stratified cross-validation when y is supplied . verbose : int , optional Verbosity level . Defaut is False Notes -- -- -- The searchlight [ Kriegeskorte 06 ] is a widely used approach for the study of the fine-grained patterns of information in fMRI analysis . Its principle is relatively simple : a small group of neighboring features is extracted from the data , and the prediction function is instantiated on these features only . The resulting prediction accuracy is thus associated with all the features within the group , or only with the feature on the center . This yields a map of local fine-grained information , that can be used for assessing hypothesis on the local spatial layout of the neural code under investigation . Nikolaus Kriegeskorte , Rainer Goebel & Peter Bandettini . Information-based functional brain mapping . Proceedings of the National Academy of Sciences of the United States of America , vol . 103 , no . 10 , pages 3863-3868 , March 2006 `` '' '' ''",https://github.com/nilearn/nilearn/blob/1c5fe0ac030464b2dd0472eaf81b90d3f10bf71a/nilearn/decoding/searchlight.py#L204,Python,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` Purpose : DSCAL scales a vector by a constant . Modified : 30 March 2007 Author : FORTRAN77 original by Jack Dongarra , Cleve Moler , Jim Bunch , Pete Stewart . C version by John Burkardt Reference : Jack Dongarra , Cleve Moler , Jim Bunch , Pete Stewart , LINPACK User 's Guide , SIAM , 1979 . Charles Lawson , Richard Hanson , David Kincaid , Fred Krogh , Basic Linear Algebra Subprograms for Fortran Usage , Algorithm 539 , ACM Transactions on Mathematical Software , Volume 5 , Number 3 , September 1979 , pages 308-323 . Parameters : Input , int N , the number of entries in the vector . Input , double SA , the multiplier . Input output , double X [ ] , the vector to be scaled . Input , int INCX , the increment between successive entries of X. ``",https://github.com/kripken/emscripten/blob/06c58d48f37a126e51b809f4ae17d9297b06a444/tests/linpack2.c#L737,C,yes,book,algorithm,method,reference,other,formulas to source code,no
"`` @ file parallel workstealing.h @ brief Parallelization of embarrassingly parallel execution by means of work-stealing . Work stealing is described in R. D. Blumofe and C. E. Leiserson . Scheduling multithreaded computations by work stealing . Journal of the ACM , 46 ( 5 ) :720‚Äì748 , 1999 . This file is a GNU parallel extension to the Standard C++ Library. ``",https://github.com/DragonFlyBSD/DragonFlyBSD/blob/ce2989fe1212f664d615268edc64a57801fc7404/contrib/gcc-5.0/libstdc++-v3/include/parallel/workstealing.h#L25,C,yes,journal,algorithm,file,reference,networks and os,formulas to source code,no
"`` Revised wakeup rule [ 1 ] : For self-suspending tasks , rather then re-initializing task 's runtime and deadline , the revised wakeup rule adjusts the task 's runtime to avoid the task to overrun its density . Reasoning : a task may overrun the density if : runtime ( deadline - t ) > dl_runtime dl_deadline Therefore , runtime can be adjusted to : runtime = ( dl_runtime dl_deadline ) ( deadline - t ) In such way that runtime will be equal to the maximum density the task can use without breaking any rule . [ 1 ] Luca Abeni , Giuseppe Lipari , and Juri Lelli . 2015 . Constant bandwidth server revisited . SIGBED Rev . 11 , 4 ( January 2015 ) , 19-24. ``",https://github.com/joergroedel/linux-iommu/blob/b12e777a85e31d508f6b194709bbad450064c5e7/kernel/sched/deadline.c#L771,C,yes,journal,algorithm,method,reference,networks and os,formulas to source code,no
"`` defgroup MersenneRandom_c Module MersenneRandom.c ingroup Random_h author Tibbits , M M brief Routine to get a random number based on the Mersenne Twister Algorithm . Description This was implemented as in the paper listed below . We have provided two functions , each which may be called multiple times . One returns a single number and the other returns a vector of length prescribed by the Vector- > length . Below I have listed the abstract from the paper referenced below : < p > A new algorithm called Mersenne Twister ( MT ) is proposed for generating uniform pseudoran-dom numbers . For a particular choice of parameters , the algorithm provides a super astronom-ical period of 2 19937 2 1 and 623-dimensional equidistribution up to 32-bit accuracy , while using a working area of only 624 words . This is a new variant of the previously proposed generators , TGFSR , modified so as to admit a Mersenne-prime period . The characteristic polynomial has many terms . The distribution up to v bits accuracy for 1 v 32 is also shown to be good . An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O ( p 2 ) where p is the degree of the polynomial . < p > They implemented this generator in portable C-code . It passed several stringent statistical tests , including diehard . Its speed is comparable to other modern generators . Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field . Algorithm Please see paper listed below : M. Matsumoto and T. Nishimura , `` Mersenne Twister : A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator '' , ACM Transactions on Modeling and Computer Simulation , Vol . 8 , No . 1 , January 1998 , pp 3 -- 30 . Copyright ( C ) 1997 Makoto Matsumoto and Takuji Nishimura . When you use this , send an email to : matumoto @ math.keio.ac.jp with an appropriate reference to your work . Notes Pulled from distributed source code : A C-program for MT19937 : Real number version ( 1998 4 6 ) genrand ( ) generates one pseudorandom real number ( double ) which is uniformly distributed on [ 0,1 ] -interval , for each call . sgenrand ( seed ) set initial values to the working area of 624 words . Before genrand ( ) , sgenrand ( seed ) must be called once . ( seed is any 32-bit integer except for 0 ) . Integer generator is obtained by modifying two lines . Coded by Takuji Nishimura , considering the suggestions by Topher Cooper and Marc Rieffel in July-Aug. 1997 . Seed value MAY NOT EQUAL ZERO. ``",https://github.com/lscsoft/lalsuite/blob/8cbd1b7187ce3ed9a825d6ed11cc432f3cfde9a5/lal/src/utilities/MersenneRandom.c#L39,C,404,,,,,,,
"`` Class for building and using a PRISM classifier . For more information , see < p > J. Cendrowska ( 1987 ) . < i > PRISM : An algorithm for inducing modular rules < i > . International Journal of Man-Machine Studies . Vol.27 , No.4 , pp.349-370. < p > @ author Ian H. Witten ( ihw @ cs.waikato.ac.nz ) @ version $ Revision : 1.8 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-1-6/weka/classifiers/rules/Prism.java#L25,Java,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` Implementation of a test of tetrad constraints with discrete variables . We are assuming that variables are ordinal or binary . Such tests are a core statistical procedure in algorithm BuildPureClusters and Purify . < p > An `` underlying latent variable '' approach is used to test tetrads indirectly by fitting discrete one-factor and two-factor models . See Bartholomew and Knott ( 1999 ) for details . A two-stage procedure for fitting polychorics correlations ( Olsson , 1979 ) and a chi-square test of tetrad constraints over those correlations is the key for this method . < p > References : < p > Bartholomew , D. and Knott , M. ( 1999 ) . Latent Variable Models and Factor Analysis , 2nd edition . Arnold . < p > Olsson , Ulf ( 1979 ) . `` Maximum likelihood estimation of the polychoric correlation coefficient '' . Psychometrika 44 , 443-460 . < p > Stroud , A. and Secrest D. ( 1966 ) Gaussian Quadrature Formulas . Prentice Hall . @ author Ricardo Silva ``",https://github.com/cmu-phil/tetrad/blob/5f788391a9ebd2fdfe2981ebf716ec199aee4c0c/tetrad-lib/src/main/java/edu/cmu/tetrad/search/DiscreteTetradTest.java#L38,Java,yes,book,algorithm,class,reference,other,paper not available,no
"`` MersenneTwister ( MT19937 ) is one of the strongest uniform pseudo-random number generators known so far ; at the same time it is quick . < p > Produces uniformly distributed { @ code int } 's and { @ code long } 's in the closed intervals { @ code [ Integer.MIN_VALUE , Integer.MAX_VALUE ] } and { @ code [ Long.MIN_VALUE , Long.MAX_VALUE ] } , respectively , as well as { @ code float } 's and { @ code double } 's in the open unit intervals { @ code ( 0.0f,1.0f ) } and { @ code ( 0.0,1.0 ) } , respectively . The seed can be any 32-bit integer except { @ code 0 } . Shawn J. Cokus commented that perhaps the seed should preferably be odd . < h3 > Quality : < h3 > MersenneTwister is designed to pass the k-distribution test . It has an astronomically large period of 2 < sup > 19937 < sup > -1 ( =10 < sup > 6001 < sup > ) and 623-dimensional equidistribution up to 32-bit accuracy . It passes many stringent statistical tests , including the < a href= '' http : stat.fsu.edu ~geo diehard.html '' > diehard < a > test of G. Marsaglia and the load test of P. Hellekalek and S. Wegenkittl . < h3 > Performance : < h3 > Its speed is comparable to other modern generators ( in particular , as fast as { @ code java.util.Random.nextFloat ( ) } ) . 2.5 million calls to { @ code raw ( ) } per second ( Pentium Pro 200 Mhz , JDK 1.2 , NT ) . Be aware , however , that there is a non-negligible amount of overhead required to initialize the data structures used by a MersenneTwister . Code like : < pre > double sum = 0.0 ; for ( int i=0 ; i < 100000 ; ++i ) { RandomElement twister = new MersenneTwister ( new java.util.Date ( ) ) ; sum += twister.raw ( ) ; } < pre > will be wildly inefficient . Consider using < pre > double sum = 0.0 ; RandomElement twister = new MersenneTwister ( new java.util.Date ( ) ) ; for ( int i=0 ; i < 100000 ; ++i ) { sum += twister.raw ( ) ; } < pre > instead . This allows the cost of constructing the MersenneTwister object to be borne only once , rather than once for each iteration in the loop . < h3 > Implementation : < h3 > After M. Matsumoto and T. Nishimura , `` Mersenne Twister : A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator '' , ACM Transactions on Modeling and Computer Simulation , Vol . 8 , No . 1 , January 1998 , pp 3 -- 30 . < p > More info on < A HREF= '' http : www.math.keio.ac.jp ~matumoto eindex.html '' > Masumoto 's homepage < A > . < p > < p > More info on < A HREF= '' http : www.ncsa.uiuc.edu Apps CMP RNG www-rng.html '' > Pseudo-random number generators is on the Web < A > . < p > < p > Yet < A HREF= '' http : nhse.npac.syr.edu random '' > some more info < A > . < p > < p > The correctness of this implementation has been verified against the published output sequence < a href= '' http : www.math.keio.ac.jp ~nisimura random real2 mt19937-2.out '' > mt19937-2.out < a > of the C-implementation < a href= '' http : www.math.keio.ac.jp ~nisimura random real2 mt19937-2.c '' > mt19937-2.c < a > . ( Call { @ code test ( 1000 ) } to print the sequence ) . Note that this implementation is < i > < b > not synchronized < b > < i > . < h3 > Details : < h3 > MersenneTwister is designed with consideration of the flaws of various existing generators in mind . It is an improved version of TT800 , a very successful generator . MersenneTwister is based on linear recurrences modulo 2 . Such generators are very fast , have extremely long periods , and appear quite robust . MersenneTwister produces 32-bit numbers , and every { @ code k } -dimensional vector of such numbers appears the same number of times as { @ code k } successive values over the period length , for each { @ code k & lt ; = 623 } ( except for the zero vector , which appears one time less ) . If one looks at only the first { @ code n & lt ; = 16 } bits of each number , then the property holds for even larger { @ code k } , as shown in the following table ( taken from the publication cited above ) : < div align= '' center '' > < table width= '' 75 % '' border= '' 1 '' cellspacing= '' 0 '' cellpadding= '' 0 '' > < tr > < td width= '' 2 % '' > < div align= '' center '' > n < div > < td > < td width= '' 6 % '' > < div align= '' center '' > 1 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 2 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 3 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 4 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 5 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 6 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 7 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 8 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 9 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 10 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 11 < div > < td > < td width= '' 10 % '' > < div align= '' center '' > 12 .. 16 < div > < td > < td width= '' 10 % '' > < div align= '' center '' > 17 .. 32 < div > < td > < tr > < tr > < td width= '' 2 % '' > < div align= '' center '' > k < div > < td > < td width= '' 6 % '' > < div align= '' center '' > 19937 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 9968 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 6240 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 4984 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 3738 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 3115 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 2493 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 2492 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 1869 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 1869 < div > < td > < td width= '' 5 % '' > < div align= '' center '' > 1248 < div > < td > < td width= '' 10 % '' > < div align= '' center '' > 1246 < div > < td > < td width= '' 10 % '' > < div align= '' center '' > 623 < div > < td > < tr > < table > < div > < p > MersenneTwister generates random numbers in batches of 624 numbers at a time , so the caching and pipelining of modern systems is exploited . The generator is implemented to generate the output by using the fastest arithmetic operations only : 32-bit additions and bit operations ( no division , no multiplication , no mod ) . These operations generate sequences of 32 random bits ( { @ code int } 's ) . { @ code long } 's are formed by concatenating two 32 bit { @ code int } 's . { @ code float } 's are formed by dividing the interval { @ code [ 0.0,1.0 ] } into 2 < sup > 32 < sup > sub intervals , then randomly choosing one subinterval . { @ code double } 's are formed by dividing the interval { @ code [ 0.0,1.0 ] } into 2 < sup > 64 < sup > sub intervals , then randomly choosing one subinterval . < p > @ author wolfgang.hoschek @ cern.ch @ version 1.0 , 09 24 99 @ see java.util.Random ``",https://github.com/emmt/TiPi/blob/812a6ff64b0564bf236fa3f3a99c3c7881ca3b7b/src/mitiv/random/MersenneTwisterGenerator.java#L43,Java,yes,journal,algorithm,class,reference,computer vision,pseudocode to source code,no
"`` TRE05O2AbsModel param [ out ] pxsec cross section ( absorption volume mixing ratio ) of O2 according to TRE05 [ 1 m ] param CCin scaling factor for the O2-continuum [ 1 ] param CLin scaling factor for the O2-line strengths [ 1 ] param CWin scaling factor for the O2-line widths [ 1 ] param COin scaling factor for the O2-line coupling [ 1 ] param model allows user defined input parameter set ( CCin , CLin , CWin , and COin ) < br > or choice of pre-defined parameters of specific models ( see note below ) . param f_grid predefined frequency grid [ Hz ] param abs_p predefined pressure [ Pa ] param abs_t predefined temperature grid [ K ] param abs_h2o H2O volume mixing ratio profile [ 1 ] param vmr O2 volume mixing ratio profile [ 1 ] ote Except for model 'user ' the input parameters CCin , CLin , CWin , and COin are neglected ( model dominates over parameters ) . < br > Allowed models : 'TRE05 ' , 'TRE05Lines ' , 'TRE05Continuum ' , 'TRE05NoCoupling ' , 'TRE05NoCutoff ' , and 'user ' . See the user guide for detailed explanations . remark References : H. J. Liebe and G. A. Hufford and M. G. Cotton , < br > < i > Propagation modeling of moist air and suspended water ice particles at frequencies below 1000 GHz < i > , < br > AGARD 52nd Specialists Meeting of the Electromagnetic Wave Propagation Panel , < br > Palma de Mallorca , Spain , 1993 , May 17-21 M.Yu . Tretyakov , M.A . Koshelev , V.V . Dorovskikh , D.S . Makarov , P.W . Rosenkranz ; 60-GHz oxygen band : precise broadening and central frequencies of fine-structure lines , absolute absorption profile at atmospheric pressure , and revision of mixing coefficients doi:10.1016 j.jms.2004.11.011 remark This is a copy of MPM93O2AbsModel with an exception of having new values from the Tretyakov etal . 2005 paper . author Richard Larsson date 2013-09-20 ``",https://github.com/olemke/arts/blob/a421111a8e5a220a9d422339511ed8855b770d1e/src/continua.cc#L12755,C++,yes,conference,numbers,method,reference,simulation,numbers to hard coded values,no
"`` Stochastic Fairness Queuing algorithm . ======================================= Source : Paul E. McKenney `` Stochastic Fairness Queuing '' , IEEE INFOCOMM'90 Proceedings , San Francisco , 1990 . Paul E. McKenney `` Stochastic Fairness Queuing '' , `` Interworking : Research and Experience '' , v.2 , 1991 , p.113-131 . See also : M. Shreedhar and George Varghese `` Efficient Fair Queuing using Deficit Round Robin '' , Proc . SIGCOMM 95 . This is not the thing that is usually called ( W ) FQ nowadays . It does not use any timestamp mechanism , but instead processes queues in round-robin order . ADVANTAGE : - It is very cheap . Both CPU and memory requirements are minimal . DRAWBACKS : - `` Stochastic '' - > It is not 100 % fair . When hash collisions occur , several flows are considered as one . - `` Round-robin '' - > It introduces larger delays than virtual clock based schemes , and should not be used for isolating interactive traffic from non-interactive . It means , that this scheduler should be used as leaf of CBQ or P3 , which put interactive traffic to higher priority band . We still need true WFQ for top level CSZ , but using WFQ for the best effort traffic is absolutely pointless : SFQ is superior for this purpose . IMPLEMENTATION : This implementation limits : - maximal queue length per flow to 127 packets . - max mtu to 2^18-1 ; - max 65408 flows , - number of hash buckets to 65536 . It is easy to increase these values , but not in flight. ``",https://github.com/imoseyon/leanKernel-shamu/blob/1484c3f3a2053b83b193b00586e38043d58ca198/net/sched/sch_sfq.c#L30,C,yes,conference,algorithm,file,reference,networks and os,formulas to source code,no
"`` double dln1px ( double a ) Double precision LN ( 1+X ) Function Returns ln ( 1+x ) Note that the obvious code of LOG ( 1.0+X ) wo n't work for small X because 1.0+X loses accuracy Arguments X -- > Value for which ln ( 1-x ) is desired . X is DOUBLE PRECISION Method Renames ALNREL from : DiDinato , A. R. and Morris , A. H. Algorithm 708 : Significant Digit Computation of the Incomplete Beta Function Ratios . ACM Trans . Math . Softw . 18 ( 1993 ) , 360-373 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - EVALUATION OF THE FUNCTION LN ( 1 + A ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ``",https://github.com/InsightSoftwareConsortium/ITK/blob/a077e98358a87c2b507e5ee9c27939918a9120ea/Modules/ThirdParty/NIFTI/src/nifti/nifticdf/nifticdf.c#L6590,C++,yes,journal,algorithm,method,reference,computer vision,formulas to source code,no
"`` End of mem3.c Begin file mem5.c 2007 October 14 The author disclaims copyright to this source code . In place of a legal notice , here is a blessing : May you do good and not evil . May you find forgiveness for yourself and forgive others . May you share freely , never taking more than you give . This file contains the C functions that implement a memory allocation subsystem for use by SQLite . This version of the memory allocation subsystem omits all use of malloc ( ) . The application gives SQLite a block of memory before calling sqlite3_initialize ( ) from which allocations are made and returned by the xMalloc ( ) and xRealloc ( ) implementations . Once sqlite3_initialize ( ) has been called , the amount of memory available to SQLite is fixed and can not be changed . This version of the memory allocation subsystem is included in the build only if SQLITE_ENABLE_MEMSYS5 is defined . This memory allocator uses the following algorithm : 1 . All memory allocations sizes are rounded up to a power of 2 . 2 . If two adjacent free blocks are the halves of a larger block , then the two blocks are coalesced into the single larger block . 3 . New memory is allocated from the first available free block . This algorithm is described in : J. M. Robson . `` Bounds for Some Functions Concerning Dynamic Storage Allocation '' . Journal of the Association for Computing Machinery , Volume 21 , Number 8 , July 1974 , pages 491-499 . Let n be the size of the largest allocation divided by the minimum allocation size ( after rounding all sizes up to a power of 2 . ) Let M be the maximum amount of memory ever outstanding at one time . Let N be the total amount of memory available for allocation . Robson proved that this memory allocator will never breakdown due to fragmentation as long as the following constraint holds : N > = M ( 1 + log2 ( n ) 2 ) - n + 1 The sqlite3_status ( ) logic tracks the maximum values of n and M so that an application can , at any time , verify this constraint. ``",https://github.com/sstangl/js-arm64/blob/c01fab6dc6f92a98ccb31e90c9c07a7ba1249e32/db/sqlite3/src/sqlite3.c#L18527,C++,404,,,,,,,
"`` `` '' '' Demo script for Quest routines . By commenting and uncommenting a few lines in this function , you can use this file to implement three QUEST-related procedures for measuring threshold . QuestMode : In the original algorithm of Watson & Pelli ( 1983 ) each trial and the final estimate are at the MODE of the posterior pdf . QuestMean : In the improved algorithm of King-Smith et al . ( 1994 ) . each trial and the final estimate are at the MEAN of the posterior pdf . QuestQuantile & QuestMean : In the ideal algorithm of Pelli ( 1987 ) each trial is at the best QUANTILE , and the final estimate is at the MEAN of the posterior pdf . This was converted from the Psychtoolbox 's QuestDemo function . King-Smith , P. E. , Grigsby , S. S. , Vingrys , A. J. , Benes , S. C. , and Supowit , A . ( 1994 ) Efficient and unbiased modifications of the QUEST threshold method : theory , simulations , experimental evaluation and practical implementation . Vision Res , 34 ( 7 ) , 885-912 . Pelli , D. G. ( 1987 ) The ideal psychometric procedure . Investigative Ophthalmology & Visual Science , 28 ( Suppl ) , 366 . Watson , A . B. and Pelli , D. G. ( 1983 ) QUEST : a Bayesian adaptive psychometric method . Percept Psychophys , 33 ( 2 ) , 113-20. `` '' '' ''",https://github.com/psychopy/psychopy/blob/ec8b367ae3e52072ff7dad0eb290b4bc41ca3655/psychopy/contrib/quest.py#L389,Python,yes,journal,numbers,method,reference,science,numbers to hard coded values,no
"`` multired.py Copyright ( C ) 2015 Vincenzo ( Enzo ) Nicosia < katolaz @ yahoo.it > This program is free software : you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation , either version 3 of the License , or ( at your option ) any later version . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . You should have received a copy of the GNU General Public License long with this program . If not , see < http : www.gnu.org licenses > . This module provides the class multiplex_red , which implements the algorithm for the structural reduction of multi-layer networks based on the Von Neumann entropy and Quantum Jensen-Shannon divergence of graphs . If you use this code please cite the paper : M. De Domenico , V. Nicosia , A. Arenas , V. Latora , `` Structural reducibility of multilayer networks '' Nat . Commun . 6 , 6864 ( 2015 ) doi:10.1038 ncomms7864 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2015 04 23 -- release 0.1 -- 2015 05 11 -- release 0.1.1 -- removed the last full matrices ``",https://github.com/wtgme/ohsn/blob/02261e32390775f3d7786e7d2974642c972f8ae2/ohsn/networkminer/multired.py#L1,Python,yes,journal,algorithm,file,reference,other,formulas to source code,no
"`` Class for boosting a classifier using Freund & amp ; Schapire 's Adaboost M1 method . For more information , see < p > Yoav Freund and Robert E. Schapire ( 1996 ) . < i > Experiments with a new boosting algorithm < i > . Proc International Conference on Machine Learning , pages 148-156 , Morgan Kaufmann , San Francisco. < p > Valid options are : < p > -D < br > Turn on debugging output. < p > -W classname < br > Specify the full class name of a classifier as the basis for boosting ( required ) . < p > -I num < br > Set the number of boost iterations ( default 10 ) . < p > -P num < br > Set the percentage of weight mass used to build classifiers ( default 100 ) . < p > -Q < br > Use resampling instead of reweighting. < p > -S seed < br > Random number seed for resampling ( default 1 ) . < p > Options after -- are passed to the designated classifier. < p > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Len Trigg ( trigg @ cs.waikato.ac.nz ) @ version $ Revision : 1.24.2.3 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/stable-3-4-7/weka/classifiers/meta/AdaBoostM1.java#L31,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` Author : Angus Johnson Version : 6.2.1 Date : 31 October 2014 Website : http : www.angusj.com Copyright : Angus Johnson 2010-2014 License : Use , modification & distribution is subject to Boost Software License Ver 1. http : www.boost.org LICENSE_1_0.txt Attributions : The code in this library is an extension of Bala Vatti 's clipping algorithm : `` A generic solution to polygon clipping '' Communications of the ACM , Vol 35 , Issue 7 ( July 1992 ) pp 56-63. http : portal.acm.org citation.cfm ? id=129906 Computer graphics and geometric modeling : implementation and algorithms By Max K. Agoston Springer ; 1 edition ( January 4 , 2005 ) http : books.google.com books ? q=vatti+clipping+agoston See also : `` Polygon Offsetting by Computing Winding Numbers '' Paper no . DETC2005-85513 pp . 565-575 ASME 2005 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference ( IDETC CIE2005 ) September 24-28 , 2005 , Long Beach , California , USA http : www.me.berkeley.edu ~mcmains pubs DAC05OffsetPolygon.pdf ``",https://github.com/MiKTeX/miktex/blob/c9dae1418fd342eacc140c33a5f5b0837612a0a7/Programs/DviWare/dvisvgm/source/libs/clipper/clipper.hpp#L1,C,yes,journal,background,n/a,related,other,no transfer,no
"`` file brief Header file for GLM Divergence Cleaning Contains function prototypes and global variable declaration for the GLM formulation to control the divergence-free condition of magnetic field . authors A. Mignone ( mignone @ ph.unito.it ) P. Tzeferacos ( petros.tzeferacos @ ph.unito.it ) date July 24 , 2015 b References - `` A Second-order unsplit Godunov scheme for cell-centered MHD : The CTU-GLM scheme '' Mignone & Tzeferacos , JCP ( 2010 ) 229 , 2117 - `` High-order conservative finite difference GLM-MHD scheme for cell-centered MHD '' Mignone , Tzeferacos & Bodo , JCP ( 2010 ) 229 , 5896 ``",https://github.com/Sportsfan77777/vortex/blob/3164ff1b30f14cdaffa06b2fb123836104e134f0/code_pluto/Src_active/MHD/GLM/glm.h#L1,C,yes,journal,background,n/a,related,other,no transfer,no
"`` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - MC PCFIELDSPLIT - Preconditioner created by combining separate preconditioners for individual fields or groups of fields . See the users manual section `` Solving Block Matrices '' for more details . To set options on the solvers for each block append -fieldsplit_ to all the PC options database keys . For example , -fieldsplit_pc_type ilu -fieldsplit_pc_factor_levels 1 To set the options on the solvers separate for each block call PCFieldSplitGetSubKSP ( ) and set the options directly on the resulting KSP object Level : intermediate Options Database Keys : + -pc_fieldsplit_ % d_fields < a , b , .. > - indicates the fields to be used in the % d'th split . -pc_fieldsplit_default - automatically add any fields to additional splits that have not been supplied explicitly by -pc_fieldsplit_ % d_fields . -pc_fieldsplit_block_size < bs > - size of block that defines fields ( i.e . there are bs fields ) . -pc_fieldsplit_type < additive , multiplicative , symmetric_multiplicative , schur > - type of relaxation or factorization splitting . -pc_fieldsplit_schur_precondition < self , selfp , user , a11 , full > - default is a11 . -pc_fieldsplit_detect_saddle_point - automatically finds rows with zero or negative diagonal and uses Schur complement with no preconditioner as the solver- Options prefix for inner solvers when using Schur complement preconditioner are -fieldsplit_0_ and -fieldsplit_1_ for all other solvers they are -fieldsplit_ % d_ for the dth field , use -fieldsplit_ for all fields Notes : Use PCFieldSplitSetFields ( ) to set fields defined by `` strided '' entries and PCFieldSplitSetIS ( ) to define a field by an arbitrary collection of entries . If no fields are set the default is used . The fields are defined by entries strided by bs , beginning at 0 then 1 , etc to bs-1 . The block size can be set with PCFieldSplitSetBlockSize ( ) , if this is not called the block size defaults to the blocksize of the second matrix passed to KSPSetOperators ( ) PCSetOperators ( ) . $ For the Schur complement preconditioner if J = ( A00 A01 ) $ ( A10 A11 ) $ the preconditioner using full factorization is $ ( I -ksp ( A00 ) A01 ) ( inv ( A00 ) 0 ) ( I 0 ) $ ( 0 I ) ( 0 ksp ( S ) ) ( -A10 ksp ( A00 ) I ) where the action of inv ( A00 ) is applied using the KSP solver with prefix -fieldsplit_0_ . S is the Schur complement $ S = A11 - A10 ksp ( A00 ) A01 which is usually dense and not stored explicitly . The action of ksp ( S ) is computed using the KSP solver with prefix -fieldsplit_splitname_ ( where splitname was given in providing the SECOND split or 1 if not give ) . For PCFieldSplitGetKSP ( ) when field number is 0 , it returns the KSP associated with -fieldsplit_0_ while field number 1 gives -fieldsplit_1_ KSP . By default A11 is used to construct a preconditioner for S , use PCFieldSplitSetSchurPre ( ) to turn on or off this option . You can use the preconditioner PCLSC to precondition the Schur complement with -fieldsplit_1_pc_type lsc . When option -fieldsplit_schur_precondition selfp is given , an approximation to S is assembled -- Sp = A11 - A10 inv ( diag ( A00 ) ) A01 , which has type AIJ and can be used with a variety of preconditioners ( e.g. , -fieldsplit_1_pc_type asm ) . The factorization type is set using -pc_fieldsplit_schur_fact_type < diag , lower , upper , full > . The full is shown above , diag gives $ ( inv ( A00 ) 0 ) $ ( 0 -ksp ( S ) ) note that slightly counter intuitively there is a negative in front of the ksp ( S ) so that the preconditioner is positive definite . The lower factorization is the inverse of $ ( A00 0 ) $ ( A10 S ) where the inverses of A00 and S are applied using KSPs . The upper factorization is the inverse of $ ( A00 A01 ) $ ( 0 S ) where again the inverses of A00 and S are applied using KSPs . If only one set of indices ( one IS ) is provided with PCFieldSplitSetIS ( ) then the complement of that IS is used automatically for a second block . The fieldsplit preconditioner can not currently be used with the BAIJ or SBAIJ data formats if the blocksize is larger than 1 . Generally it should be used with the AIJ format . The forms of these preconditioners are closely related if not identical to forms derived as `` Distributive Iterations '' , see , for example , page 294 in `` Principles of Computational Fluid Dynamics '' by Pieter Wesseling . Note that one can also use PCFIELDSPLIT inside a smoother resulting in `` Distributive Smoothers '' . Concepts : physics based preconditioners , block preconditioners There is a nice discussion of block preconditioners in [ El08 ] A taxonomy and comparison of parallel block multi-level preconditioners for the incompressible Navier-Stokes equations Howard Elman , V.E . Howle , John Shadid , Robert Shuttleworth , Ray Tuminaro , Journal of Computational Physics 227 ( 2008 ) 1790 -- 1808 http : chess.cs.umd.edu ~elman papers tax.pdf.seealso : PCCreate ( ) , PCSetType ( ) , PCType ( for list of available types ) , PC , Block_Preconditioners , PCLSC , PCFieldSplitGetSubKSP ( ) , PCFieldSplitSetFields ( ) , PCFieldSplitSetType ( ) , PCFieldSplitSetIS ( ) , PCFieldSplitSetSchurPre ( ) M ``",https://github.com/petsc/petsc/blob/98228bd44f1ac401f769922ccdd902ec91091718/src/ksp/pc/impls/fieldsplit/fieldsplit.c#L2089,C,404,,,,,,,
"`` < -- globalinfo-start -- > Class for boosting a 2-class classifier using the Real Adaboost method. < br > < br > For more information , see < br > < br > J. Friedman , T. Hastie , R. Tibshirani ( 2000 ) . Additive Logistic Regression : a Statistical View of Boosting . Annals of Statistics . 95 ( 2 ) :337-407 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Friedman2000 , author = { J. Friedman and T. Hastie and R. Tibshirani } , journal = { Annals of Statistics } , number = { 2 } , pages = { 337-407 } , title = { Additive Logistic Regression : a Statistical View of Boosting } , volume = { 95 } , year = { 2000 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -P & lt ; num & gt ; Percentage of weight mass to base training on . ( default 100 , reduce to around 90 speed up ) < pre > < pre > -Q Use resampling for boosting . < pre > < pre > -H & lt ; num & gt ; Shrinkage parameter . ( default 1 ) < pre > < pre > -S & lt ; num & gt ; Random number seed . ( default 1 ) < pre > < pre > -I & lt ; num & gt ; Number of iterations . ( default 10 ) < pre > < pre > -W Full name of base classifier . ( default : weka.classifiers.trees.DecisionStump ) < pre > < pre > Options specific to classifier weka.classifiers.trees.DecisionStump : < pre > < -- options-end -- > Options after -- are passed to the designated classifier . < p > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Len Trigg ( trigg @ cs.waikato.ac.nz ) @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-7-8/packages/internal/realAdaBoost/src/main/java/weka/classifiers/meta/RealAdaBoost.java#L46,Java,yes,journal,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` A solver for mixed LCP problems using the Gauss-Seidel iterative method . The problem can contain : - CONSTRAINT = Bilateral constraint ( all atomic , a fixed 3D point=3 atomics independents constraints ) - CONTACT = Unilateral constraint frictionless = > 1 atomic constraint per contact frictional with Coulomb friction ( 1 mu parameter per contact ) = > 3 atomic dependent constraints per contact ( 1 directional + 2 tangentials ) - SUTURING = Sliding constraint for suturing Frictionless suturing constraint = > 2 atomic constraints per sliding point Frictional suturing constraint = > 3 atomic constraints per sliding point ( 2 directional + 1 tangential with friction on it ) = > 1 mu parameter per frictional suturing See e.g . : Duriez , Christian ; Dubois , F. ; Kheddar , A. ; Andriot , C. , `` Realistic haptic rendering of interacting deformable objects in virtual environments , '' < i > IEEE Transactions on Visualization and Computer Graphics , < i > vol.12 , no.1 , pp.36,47 , Jan.-Feb . 2006 . ''",https://github.com/simquest/opensurgsim/blob/15c18ef3c8f040b6e0a2b468c92addf5e44cc791/SurgSim/Math/MlcpGaussSeidelSolver.h#L37,C++,yes,journal,background,n/a,related,simulation,no transfer,no
"`` The choose subtree method proposed by the R -Tree for leaf nodes . < p > N. Beckmann , H.-P. Kriegel , R. Schneider , B. Seeger : < br > The R -tree : an efficient and robust access method for points and rectangles < br > in : Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data , Atlantic City , NJ , May 23-25 , 1990 < p > @ author Erich Schubert @ since 0.5.0 ``",https://github.com/elki-project/elki/blob/3e820f9f6382cb82e40955692337737c24f94b54/elki-index-rtree/src/main/java/de/lmu/ifi/dbs/elki/index/tree/spatial/rstarvariants/strategies/insert/LeastOverlapInsertionStrategy.java#L30,Java,yes,conference,algorithm,class,reference,data science,pseudocode to source code,no
"`` `` '' '' CorpusReader for reviews corpora ( syntax based on Customer Review Corpus ) .- Customer Review Corpus information -Annotated by : Minqing Hu and Bing Liu , 2004 . Department of Computer Sicence University of Illinois at ChicagoContact : Bing Liu , liub @ cs.uic.edu http : www.cs.uic.edu ~liubDistributed with permission.The `` product_reviews_1 '' and `` product_reviews_2 '' datasets respectively containannotated customer reviews of 5 and 9 products from amazon.com.Related papers : - Minqing Hu and Bing Liu . `` Mining and summarizing customer reviews '' . Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ( KDD-04 ) , 2004.- Minqing Hu and Bing Liu . `` Mining Opinion Features in Customer Reviews '' . Proceedings of Nineteeth National Conference on Artificial Intelligence ( AAAI-2004 ) , 2004.- Xiaowen Ding , Bing Liu and Philip S. Yu . `` A Holistic Lexicon-Based Appraoch to Opinion Mining . '' Proceedings of First ACM International Conference on Web Search and Data Mining ( WSDM-2008 ) , Feb 11-12 , 2008 , Stanford University , Stanford , California , USA.Symbols used in the annotated reviews : [ t ] : the title of the review : Each [ t ] tag starts a review . xxxx [ +|-n ] : xxxx is a product feature . [ +n ] : Positive opinion , n is the opinion strength : 3 strongest , and 1 weakest . Note that the strength is quite subjective . You may want ignore it , but only considering + and - [ -n ] : Negative opinion : start of each sentence . Each line is a sentence . [ u ] : feature not appeared in the sentence . [ p ] : feature not appeared in the sentence . Pronoun resolution is needed . [ s ] : suggestion or recommendation . [ cc ] : comparison with a competing product from a different brand . [ cs ] : comparison with a competing product from the same brand.Note : Some of the files ( e.g . `` ipod.txt '' , `` Canon PowerShot SD500.txt '' ) do not provide separation between different reviews . This is due to the fact that the dataset was specifically designed for aspect feature-based sentiment analysis , for which sentence-level annotation is sufficient . For document- level classification and analysis , this peculiarity should be taken into consideration . '' '' '' ''",https://github.com/nltk/nltk/blob/199c30c5cb5dbb46f5931c9d5b926617bc6a588f/nltk/corpus/reader/reviews.py#L8,Python,yes,conference,background,n/a,related,data science,no transfer,no
"`` fft_rader.c : definitions for transforms of prime length using Rader 's algorithm References : [ Rader:1968 ] Charles M. Rader , `` Discrete Fourier Transforms When the Number of Data Samples Is Prime , '' Proceedings of the IEEE , vol . 56 , number 6 , pp . 1107 -- 1108 , June 1968 ``",https://github.com/jgaeddert/liquid-dsp/blob/dd3facf847400716e62127d8b404aa9b6b273482/src/fft/src/fft_rader.c#L23,C,yes,journal,algorithm,file,reference,other,paper not available,no
"`` References CBM training algorithm Conditional Bernoulli Mixtures for Multi-label Classification . Cheng Li , Bingyu Wang , Virgil Pavlu , and Javed Aslam . In Proceedings of the 33rd International Conference on Machine Learning ( ICML ) , 2016 . Deterministic annealing : Katahira , Kentaro , Kazuho Watanabe , and Masato Okada . `` Deterministic annealing variant of variational Bayes method . '' Journal of Physics : Conference Series . Vol . 95 . No . 1 . IOP Publishing , 2008 . Created by Rainicy on 10 23 15. ``",https://github.com/cheng-li/pyramid/blob/639aaee2d02f5087000b7cb85ffc685a60a83abb/core/src/main/java/edu/neu/ccs/pyramid/multilabel_classification/cbm/CBMOptimizer.java#L21,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,yes
"`` Generalized Minimum Residual This solve the unsymmetric linear system Ax = b using restarted GMRES . See : Y. Saad and M. Schulter . GMRES : A generalized minimum residual algorithm for solving nonsysmmetric linear systems , SIAM J. Sci . Statist . Comp . 7 ( 1986 ) , pp , 856-869 ``",https://github.com/hlrs-vis/covise/blob/0ceb4a84903a80eee7126c80a2df65cedc0ab103/src/module/univiz/modules/impl/localized_flow/ext/gmm/gmm_solver_gmres.h#L73,C++,yes,journal,algorithm,method,reference,simulation,pseudocode to source code,no
"`` 80 Purpose : DUNAVANT_SUBRULE_12 returns a compressed Dunavant rule 12 . Licensing : This code is distributed under the GNU LGPL license . Modified : 11 December 2006 Author : John Burkardt Reference : David Dunavant , High Degree Efficient Symmetrical Gaussian Quadrature Rules for the Triangle , International Journal for Numerical Methods in Engineering , Volume 21 , 1985 , pages 1129-1148 . James Lyness , Dennis Jespersen , Moderate Degree Symmetric Quadrature Rules for the Triangle , Journal of the Institute of Mathematics and its Applications , Volume 15 , Number 1 , February 1975 , pages 19-32 . Parameters : Input , int SUBORDER_NUM , the number of suborders of the rule . Output , double SUBORDER_XYZ [ 3 SUBORDER_NUM ] , the barycentric coordinates of the abscissas . Output , double SUBORDER_W [ SUBORDER_NUM ] , the suborder weights. ``",https://github.com/live-clones/dolfin/blob/474ca26489403e8fe8e5006a836bc9c9dc5194a3/dolfin/geometry/SimplexQuadrature.cpp#L2216,C++,yes,journal,algorithm,method,reference,other,paper not available,no
"`` lib ts_bm.c Boyer-Moore text search implementation This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation ; either version 2 of the License , or ( at your option ) any later version . Authors : Pablo Neira Ayuso < pablo @ eurodev.net > ========================================================================== Implements Boyer-Moore string matching algorithm : [ 1 ] A Fast String Searching Algorithm , R.S . Boyer and Moore . Communications of the Association for Computing Machinery , 20 ( 10 ) , 1977 , pp . 762-772. http : www.cs.utexas.edu users moore publications fstrpos.pdf [ 2 ] Handbook of Exact String Matching Algorithms , Thierry Lecroq , 2004 http : www-igm.univ-mlv.fr ~lecroq string string.pdf Note : Since Boyer-Moore ( BM ) performs searches for matchings from right to left , it 's still possible that a matching could be spread over multiple blocks , in that case this algorithm wo n't find any coincidence . If you 're willing to ensure that such thing wo n't ever happen , use the Knuth-Pratt-Morris ( KMP ) implementation instead . In conclusion , choose the proper string search algorithm depending on your setting . Say you 're using the textsearch infrastructure for filtering , NIDS or any similar security focused purpose , then go KMP . Otherwise , if you really care about performance , say you 're classifying packets to apply Quality of Service ( QoS ) policies , and you do n't mind about possible matchings spread over multiple fragments , then go BM. ``",https://github.com/edoko/AirKernel_GN_JB/blob/f2f67fd2cece1276c89271cbcfa8a66b7a7b8bc1/lib/ts_bm.c#L1,C,yes,journal,algorithm,file,reference,networks and os,pseudocode to source code,no
"`` Detect spheres in an image using FFT-based matched bipolar filter . An object of this class can be used to detect spheres of different sizes with only two FFTs applied to the test image and its square . Each detected sphere size does require a different filter kernel and thus three repeated FFTs of the kernel , its mask , and its square . The filter kernel is bipolar , i.e. , +1 inside the sphere and -1 outside the sphere , each within a user-provided margin inside and outside the sphere surface . This makes the filter robust to intensity differences across the images . Because the filter values are either +1 or -1 or 0 , the squared filter is identical to the filter mask . This simplifies the computation and saves FT of the squared filter . see D. Padfield , `` Masked object registration in the Fourier domain , '' IEEE Transactions on Image Processing , vol . 21 , no . 5 , pp . 2706-2718 , 2012. http : dx.doi.org 10.1109 TIP.2011.2181402 http : ieeexplore.ieee.org stamp stamp.jsp ? tp= & arnumber=6111478 & isnumber=4358840 ote This class requires CMTK to be configured with FFTW3 support ( `` CMTK_USE_FFTW '' CMake option ) . todo The current implementation does not take advantage of the real-valued image and filter data , which could be used to reduce the storage requirement of the FT data ( and probably the computational cost of the transform ) by almost 50 % . On the other hand , capitalizing on these savings would either require out-of-place , rather than in-place , transforms , or substantially complicate memory layout of the input data . todo Currently , the FFT calls within the class do not take advantage of the redundancy in the real-to-complex FT applied to the real-valued image and mask . Properly considering this could potentially save about 1 2 of memory and CPU time. ``",https://github.com/jefferis/cmtk/blob/960898ca40f0d55d02e5e397e14fc316b9c072cb/core/libs/Segmentation/cmtkSphereDetectionNormalizedBipolarMatchedFilterFFT.h#L44,C++,yes,journal,algorithm,class,reference,computer vision,paper not available,no
"`` < -- globalinfo-start -- > Class implementing the KDTree search algorithm for nearest neighbour search. < br > The connection to dataset is only a reference . For the tree structure the indexes are stored in an array . < br > Building the tree : < br > If a node has & lt ; maximal-inst-number & gt ; ( option -L ) instances no further splitting is done . Also if the split would leave one side empty , the branch is not split any further even if the instances in the resulting node are more than & lt ; maximal-inst-number & gt ; instances. < br > PLEASE NOTE : The algorithm can not handle missing values , so it is advisable to run ReplaceMissingValues filter if there are any missing values in the dataset. < br > < br > For more information see : < br > < br > Jerome H. Friedman , Jon Luis Bentley , Raphael Ari Finkel ( 1977 ) . An Algorithm for Finding Best Matches in Logarithmic Expected Time . ACM Transactions on Mathematics Software . 3 ( 3 ) :209-226. < br > < br > Andrew Moore ( 1991 ) . A tutorial on kd-trees . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Friedman1977 , author = { Jerome H. Friedman and Jon Luis Bentley and Raphael Ari Finkel } , journal = { ACM Transactions on Mathematics Software } , month = { September } , number = { 3 } , pages = { 209-226 } , title = { An Algorithm for Finding Best Matches in Logarithmic Expected Time } , volume = { 3 } , year = { 1977 } } & 64 ; techreport { Moore1991 , author = { Andrew Moore } , booktitle = { University of Cambridge Computer Laboratory Technical Report No . 209 } , howpublished = { Extract from PhD Thesis } , title = { A tutorial on kd-trees } , year = { 1991 } , HTTP = { Available from http : www.autonlab.org autonweb 14665.html } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -S & lt ; classname and options & gt ; Node splitting method to use . ( default : weka.core.neighboursearch.kdtrees.SlidingMidPointOfWidestSide ) < pre > < pre > -W & lt ; value & gt ; Set minimal width of a box ( default : 1.0E-2 ) . < pre > < pre > -L Maximal number of instances in a leaf ( default : 40 ) . < pre > < pre > -N Normalizing will be done ( Select dimension for split , with normalising to universe ) . < pre > < -- options-end -- > @ author Gabi Schmidberger ( gabi [ at-the-rate ] cs [ dot ] waikato [ dot ] ac [ dot ] nz ) @ author Malcolm Ware ( mfw4 [ at-the-rate ] cs [ dot ] waikato [ dot ] ac [ dot ] nz ) @ author Ashraf M. Kibriya ( amk14 [ at-the-rate ] cs [ dot ] waikato [ dot ] ac [ dot ] nz ) @ version $ Revision : 10141 $ ``",https://github.com/umple/umple/blob/67002649b1a9db2c15b6e9bb5de7b803204e1b14/Umplificator/UmplifiedProjects/weka-umplified-0/src/main/java/weka/core/neighboursearch/KDTree.java#L43,Java,yes,journal,algorithm,class,reference,other,pseudocode to source code,no
"`` This adds J + X to h , where J and X are the matrices defined by Canni and Tomasi , J Comp Chem , 16 ( 12 ) , 1457 , 1995 . The resulting SCF free energy expression is G = 1 2TrP [ h ' + F ' ] + Une + Unn + Vnn -1 2 ( Uee+Uen+Une+Unn ) which in the Canni-Tomasi notation is = 1 2TrP [ h+1 2 ( X+J+Y+G ) ] + Vnn + 1 2Unn which is identical to the Canni-Tomasi energy expression . My Fock matrix is F ' = h + J + X + G while the Canni-Tomasi Fock matrix is F ' = h + 1 2 ( J+Y ) + X + G. However , since J = Y formally , ( assuming no numerical errors and all charge is enclosed , Canni-Tomasi use F ' = h + J + X + G to get better numerical results . If the y_equals_j option is true , the energy expression used here is G = 1 2TrP [ h+1 2 ( X+2J+G ) ] + Vnn + 1 2Unn , however , THIS IS NOT RECOMMENDED . ''",https://github.com/ValeevGroup/mpqc/blob/19ad332791d5966c98420d926364d68aeec859ae/src/lib/chemistry/qc/dft/solvent.cc#L200,C++,yes,journal,background,n/a,related,science,no transfer,no
"`` 80 Purpose : DUNAVANT_SUBRULE_08 returns a compressed Dunavant rule 8 . Licensing : This code is distributed under the GNU LGPL license . Modified : 11 December 2006 Author : John Burkardt Reference : David Dunavant , High Degree Efficient Symmetrical Gaussian Quadrature Rules for the Triangle , International Journal for Numerical Methods in Engineering , Volume 21 , 1985 , pages 1129-1148 . James Lyness , Dennis Jespersen , Moderate Degree Symmetric Quadrature Rules for the Triangle , Journal of the Institute of Mathematics and its Applications , Volume 15 , Number 1 , February 1975 , pages 19-32 . Parameters : Input , int SUBORDER_NUM , the number of suborders of the rule . Output , double SUBORDER_XYZ [ 3 SUBORDER_NUM ] , the barycentric coordinates of the abscissas . Output , double SUBORDER_W [ SUBORDER_NUM ] , the suborder weights. ``",https://github.com/live-clones/dolfin/blob/474ca26489403e8fe8e5006a836bc9c9dc5194a3/dolfin/geometry/SimplexQuadrature.cpp#L1904,C++,yes,journal,algorithm,method,reference,other,paper not available,no
"`` The algorithm is very close to that in `` Implementing the complex arcsine and arccosine functions using exception handling '' by T. E. Hull , Thomas F. Fairgrieve , and Ping Tak Peter Tang , published in ACM Transactions on Mathematical Software , Volume 23 Issue 3 , 1997 , Pages 299-335 , http : dl.acm.org citation.cfm ? id=275324 . See catrig.c for complete comments . XXX comments were removed automatically , and even short ones on the right of statements were removed ( all of them ) , contrary to normal style . Only a few comments on the right of declarations remain. ``",https://github.com/freebsd/freebsd/blob/27b7f1989483d2014a93a8316f4dd211e6b25563/lib/msun/src/catrigl.c#L28,C,yes,journal,background,n/a,related,networks and os,no transfer,no
"`` File : $ HeadURL $ Purpose : Define an API for the MCS lock : a fair queue-based lock . Reference : John M. Mellor-Crummey and Michael L. Scott . 1991 . Algorithms for scalable synchronization on shared-memory multiprocessors . ACM Transactions on Computing Systems 9 , 1 ( February 1991 ) , 21-65. http : doi.acm.org 10.1145 103727.103729 ``",https://github.com/HPCToolkit/hpctoolkit/blob/a8071928103d9a54c2506bbad3bafe51c1cf0e63/src/lib/prof-lean/mcs-lock.h#L45,C++,yes,journal,background,n/a,related,other,no transfer,no
"`` These weights for calculating Gaussian Quadrature points are taken from the paper `` Integration Points for Triangles and Tetrahedra Obtained from the Gaussian Quadrature Points for a Line '' by K. S. Sunder and R. A. Cookson , Computers and Structures , Vol 21 , No . 5 , 1985. ``",https://github.com/lammps/lammps/blob/94c6d2d5460f4ccb6f60ba58fe205243b923451e/lib/atc/FE_Quadrature.h#L608,C++,yes,journal,number,method,reference,simulation,paper not available,no
"`` r '' '' '' Compute the Weil pairing of self and ` Q ` using Miller 's algorithm . INPUT : - `` Q `` -- a point on self.curve ( ) . - `` n `` -- an integer ` n ` such that ` nP = nQ = ( 0:1:0 ) ` where ` P ` = self . OUTPUT : An ` n ` 'th root of unity in the base field self.curve ( ) .base_field ( ) EXAMPLES : : sage : F. < a > =GF ( 2^5 ) sage : E=EllipticCurve ( F , [ 0,0,1,1,1 ] ) sage : P = E ( a^4 + 1 , a^3 ) sage : Fx. < b > =GF ( 2^ ( 4 5 ) ) sage : Ex=EllipticCurve ( Fx , [ 0,0,1,1,1 ] ) sage : phi=Hom ( F , Fx ) ( F.gen ( ) .minpoly ( ) .roots ( Fx ) [ 0 ] [ 0 ] ) sage : Px=Ex ( phi ( P.xy ( ) [ 0 ] ) , phi ( P.xy ( ) [ 1 ] ) ) sage : O = Ex ( 0 ) sage : Qx = Ex ( b^19 + b^18 + b^16 + b^12 + b^10 + b^9 + b^8 + b^5 + b^3 + 1 , b^18 + b^13 + b^10 + b^8 + b^5 + b^4 + b^3 + b ) sage : Px.weil_pairing ( Qx,41 ) == b^19 + b^15 + b^9 + b^8 + b^6 + b^4 + b^3 + b^2 + 1 True sage : Px.weil_pairing ( 17 Px,41 ) == Fx ( 1 ) True sage : Px.weil_pairing ( O,41 ) == Fx ( 1 ) True An error is raised if either point is not n-torsion : : sage : Px.weil_pairing ( O,40 ) Traceback ( most recent call last ) : ... ValueError : points must both be n-torsion A larger example ( see : trac : ` 4964 ` ) : : sage : P , Q = EllipticCurve ( GF ( 19^4 , ' a ' ) , [ -1,0 ] ) .gens ( ) sage : P.order ( ) , Q.order ( ) ( 360 , 360 ) sage : z = P.weil_pairing ( Q,360 ) sage : z.multiplicative_order ( ) 360 An example over a number field : : sage : P , Q = EllipticCurve ( '11a1 ' ) .change_ring ( CyclotomicField ( 5 ) ) .torsion_subgroup ( ) .gens ( ) long time ( 10s ) sage : P , Q = ( P.element ( ) , Q.element ( ) ) long time sage : ( P.order ( ) , Q.order ( ) ) long time ( 5 , 5 ) sage : P.weil_pairing ( Q,5 ) long time zeta5^2 sage : Q.weil_pairing ( P,5 ) long time zeta5^3 ALGORITHM : Implemented using Proposition 8 in [ Mil04 ] _ . The value 1 is returned for linearly dependent input points . This condition is caught via a DivisionByZeroError , since the use of a discrete logarithm test for linear dependence , is much too slow for large ` n ` . REFERENCES : .. [ Mil04 ] Victor S. Miller , `` The Weil pairing , and its efficient calculation '' , J . Cryptol. , 17 ( 4 ) :235-261 , 2004 AUTHOR : - David Hansen ( 2009-01-25 ) `` '' '' ''",https://github.com/sagemath/sage/blob/854f9764d14236110b8d7f7b35a7d52017e044f8/src/sage/schemes/elliptic_curves/ell_point.py#L1537,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` < -- globalinfo-start -- > Converts all nominal attributes into binary numeric attributes . An attribute with k values is transformed into k binary attributes if the class is nominal ( using the one-attribute-per-value approach ) . Binary attributes are left binary , if option '-A ' is not given.If the class is numeric , k - 1 new binary attributes are generated in the manner described in `` Classification and Regression Trees '' by Breiman et al . ( i.e . taking the average class value associated with each attribute value into account ) < br > < br > For more information , see : < br > < br > L. Breiman , J.H . Friedman , R.A. Olshen , C.J . Stone ( 1984 ) . Classification and Regression Trees . Wadsworth Inc. < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; book { Breiman1984 , author = { L. Breiman and J.H . Friedman and R.A. Olshen and C.J . Stone } , publisher = { Wadsworth Inc } , title = { Classification and Regression Trees } , year = { 1984 } , ISBN = { 0412048418 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -N Sets if binary attributes are to be coded as nominal ones . < pre > < pre > -A For each nominal value a new attribute is created , not only if there are more than 2 values . < pre > < -- options-end -- > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ version $ Revision : 10215 $ ``",https://github.com/umple/umple/blob/67002649b1a9db2c15b6e9bb5de7b803204e1b14/Umplificator/UmplifiedProjects/weka-umplified-0/src/main/java/weka/filters/supervised/attribute/NominalToBinary.java#L47,Java,yes,book,algorithm,class,reference,other,paper not available,no
"`` < -- globalinfo-start -- > This class performs Bias-Variance decomposion on any classifier using the sub-sampled cross-validation procedure as specified in ( 1 ) . < br > The Kohavi and Wolpert definition of bias and variance is specified in ( 2 ) . < br > The Webb definition of bias and variance is specified in ( 3 ) . < br > < br > Geoffrey I. Webb , Paul Conilione ( 2002 ) . Estimating bias and variance from data . School of Computer Science and Software Engineering , Victoria , Australia. < br > < br > Ron Kohavi , David H. Wolpert : Bias Plus Variance Decomposition for Zero-One Loss Functions . In : Machine Learning : Proceedings of the Thirteenth International Conference , 275-283 , 1996. < br > < br > Geoffrey I. Webb ( 2000 ) . MultiBoosting : A Technique for Combining Boosting and Wagging . Machine Learning . 40 ( 2 ) :159-196 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; misc { Webb2002 , address = { School of Computer Science and Software Engineering , Victoria , Australia } , author = { Geoffrey I. Webb and Paul Conilione } , institution = { Monash University } , title = { Estimating bias and variance from data } , year = { 2002 } , PDF = { http : www.csse.monash.edu.au ~webb Files WebbConilione04.pdf } } & 64 ; inproceedings { Kohavi1996 , author = { Ron Kohavi and David H. Wolpert } , booktitle = { Machine Learning : Proceedings of the Thirteenth International Conference } , editor = { Lorenza Saitta } , pages = { 275-283 } , publisher = { Morgan Kaufmann } , title = { Bias Plus Variance Decomposition for Zero-One Loss Functions } , year = { 1996 } , PS = { http : robotics.stanford.edu ~ronnyk biasVar.ps } } & 64 ; article { Webb2000 , author = { Geoffrey I. Webb } , journal = { Machine Learning } , number = { 2 } , pages = { 159-196 } , title = { MultiBoosting : A Technique for Combining Boosting and Wagging } , volume = { 40 } , year = { 2000 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -c & lt ; class index & gt ; The index of the class attribute . ( default last ) < pre > < pre > -D Turn on debugging output. < pre > < pre > -l & lt ; num & gt ; The number of times each instance is classified . ( default 10 ) < pre > < pre > -p & lt ; proportion of objects in common & gt ; The average proportion of instances common between any two training sets < pre > < pre > -s & lt ; seed & gt ; The random number seed used. < pre > < pre > -t & lt ; name of arff file & gt ; The name of the arff file used for the decomposition. < pre > < pre > -T & lt ; number of instances in training set & gt ; The number of instances in the training set. < pre > < pre > -W & lt ; classifier class name & gt ; Full class name of the learner used in the decomposition . eg : weka.classifiers.bayes.NaiveBayes < pre > < pre > Options specific to learner weka.classifiers.rules.ZeroR : < pre > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < -- options-end -- > Options after -- are passed to the designated sub-learner . < p > @ author Paul Conilione ( paulc4321 @ yahoo.com.au ) @ version $ Revision : 10141 $ ``",https://github.com/umple/umple/blob/67002649b1a9db2c15b6e9bb5de7b803204e1b14/Umplificator/UmplifiedProjects/weka-umplified-0/src/main/java/weka/classifiers/BVDecomposeSegCVSub.java#L63,Java,yes,other(draft),algorithm,class,reference,other,pseudocode to source code,no
"`` This function seeks the parameter vector p that best describes the measurements vector x under box constraints . More precisely , given a vector function func : R^m -- > R^n with n > =m , it finds p s.t . func ( p ) ~= x , i.e . the squared second order ( i.e . L2 ) norm of e=x-func ( p ) is minimized under the constraints lb [ i ] < =p [ i ] < =ub [ i ] . If no lower bound constraint applies for p [ i ] , use -DBL_MAX -FLT_MAX for lb [ i ] ; If no upper bound constraint applies for p [ i ] , use DBL_MAX FLT_MAX for ub [ i ] . This function requires an analytic Jacobian . In case the latter is unavailable , use LEVMAR_BC_DIF ( ) bellow Returns the number of iterations ( > =0 ) if successful , LM_ERROR if failed For details , see C. Kanzow , N. Yamashita and M. Fukushima : `` Levenberg-Marquardt methods for constrained nonlinear equations with strong local convergence properties '' , Journal of Computational and Applied Mathematics 172 , 2004 , pp . 375-397 . Also , see K. Madsen , H.B . Nielsen and O. Tingleff 's lecture notes on unconstrained Levenberg-Marquardt at http : www.imm.dtu.dk pubdb views edoc_download.php 3215 pdf imm3215.pdf ``",https://github.com/hlrs-vis/covise/blob/0ceb4a84903a80eee7126c80a2df65cedc0ab103/src/OpenCOVER/plugins/ukoeln/TouchInteraction/levmar/lmbc_core.c#L282,C++,yes,journal,algorithm,method,reference,simulation,formulas to source code,no
"`` Generic binary BCH encoding decoding library SPDX-License-Identifier : GPL-2.0 Copyright ¬© 2011 Parrot S.A . Author : Ivan Djelic < ivan.djelic @ parrot.com > Description : This library provides runtime configurable encoding decoding of binary Bose-Chaudhuri-Hocquenghem ( BCH ) codes . Call init_bch to get a pointer to a newly allocated bch_control structure for the given m ( Galois field order ) , t ( error correction capability ) and ( optional ) primitive polynomial parameters . Call encode_bch to compute and store ecc parity bytes to a given buffer . Call decode_bch to detect and locate errors in received data . On systems supporting hw BCH features , intermediate results may be provided to decode_bch in order to skip certain steps . See decode_bch ( ) documentation for details . Option CONFIG_BCH_CONST_PARAMS can be used to force fixed values of parameters m and t ; thus allowing extra compiler optimizations and providing better ( up to 2x ) encoding performance . Using this option makes sense when ( m , t ) are fixed and known in advance , e.g . when using BCH error correction on a particular NAND flash device . Algorithmic details : Encoding is performed by processing 32 input bits in parallel , using 4 remainder lookup tables . The final stage of decoding involves the following internal steps : a . Syndrome computation b . Error locator polynomial computation using Berlekamp-Massey algorithm c. Error locator root finding ( by far the most expensive step ) In this implementation , step c is not performed using the usual Chien search . Instead , an alternative approach described in [ 1 ] is used . It consists in factoring the error locator polynomial using the Berlekamp Trace algorithm ( BTA ) down to a certain degree ( 4 ) , after which ad hoc low-degree polynomial solving techniques [ 2 ] are used . The resulting algorithm , called BTZ , yields much better performance than Chien search for usual ( m , t ) values ( typically m > = 13 , t < 32 , see [ 1 ] ) . [ 1 ] B. Biswas , V. Herbert . Efficient root finding of polynomials over fields of characteristic 2 , in : Western European Workshop on Research in Cryptology - WEWoRC 2009 , Graz , Austria , LNCS , Springer , July 2009 , to appear . [ 2 ] [ Zin96 ] V.A . Zinoviev . On the solution of equations of degree 10 over finite fields GF ( 2^q ) . In Rapport de recherche INRIA no 2829 , 1996. ``",https://github.com/axxia/axxia_u-boot/blob/8c84287a0f225e29b688bda848e49a555c68a442/lib/bch.c#L1,C,yes,conference,algorithm,file,reference,other,pseudocode to source code,no
"`` We are merging cluster A and cluster B to make a new cluster R. Cluster Q is one of remaining cluster to which we update distance . Lance , G. N. and Williams , W. T.. `` A general theory of classificatory sorting strategies 1 . Hierarchical systems . '' The Computer Journal 9 , no . 4 ( 1967 ) :373-380 . @ param ma size of cluster A @ param mb size of cluster B @ param mq size of cluster Q @ return Lance-Williams coefficient beta ``",https://github.com/deric/clueminer/blob/3f7b27ea5a0d70237106832e56ef496d1e3ed40c/modules/clustering-api/src/main/java/org/clueminer/clustering/api/ClusterLinkage.java#L134,Java,yes,journal,background,n/a,related,data science,no transfer,no
"`` < -- globalinfo-start -- > This Bayes Network learning algorithm uses tabu search for finding a well scoring Bayes network structure . Tabu search is hill climbing till an optimum is reached . The following step is the least worst possible step . The last X steps are kept in a list and none of the steps in this so called tabu list is considered in taking the next step . The best network found in this traversal is returned. < br > < br > For more information see : < br > < br > R.R . Bouckaert ( 1995 ) . Bayesian Belief Networks : from Construction to Inference . Utrecht , Netherlands . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; phdthesis { Bouckaert1995 , address = { Utrecht , Netherlands } , author = { R.R . Bouckaert } , institution = { University of Utrecht } , title = { Bayesian Belief Networks : from Construction to Inference } , year = { 1995 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -L & lt ; integer & gt ; Tabu list length < pre > < pre > -U & lt ; integer & gt ; Number of runs < pre > < pre > -P & lt ; nr of parents & gt ; Maximum number of parents < pre > < pre > -R Use arc reversal operation . ( default false ) < pre > < pre > -P & lt ; nr of parents & gt ; Maximum number of parents < pre > < pre > -R Use arc reversal operation . ( default false ) < pre > < pre > -N Initial structure is empty ( instead of Naive Bayes ) < pre > < pre > -mbc Applies a Markov Blanket correction to the network structure , after a network structure is learned . This ensures that all nodes in the network are part of the Markov blanket of the classifier node. < pre > < pre > -S [ BAYES|MDL|ENTROPY|AIC|CROSS_CLASSIC|CROSS_BAYES ] Score type ( BAYES , BDeu , MDL , ENTROPY and AIC ) < pre > < -- options-end -- > @ author Remco Bouckaert ( rrb @ xm.co.nz ) @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-7-9/weka/src/main/java/weka/classifiers/bayes/net/search/local/TabuSearch.java#L37,Java,yes,book,algorithm,class,reference,machine learning,paper not available,no
"`` `` '' '' Calculate gas compressibility factor using the correlation of Brill- Beggs ( 1974 ) Parameters -- -- -- -- -- -- Tr : float Reduced temperature [ - ] Pr : float Reduced pressure [ - ] Returns -- -- -- - Z : float Gas compressibility factor [ - ] Raises -- -- -- NotImplementedError : If input is n't in limit 1.15 ‚â§ Tr ‚â§ 2.4 0.2 ‚â§ Pr ‚â§ 15 References -- -- -- -- -- .. [ 34 ] Brill , J .P . and Beggs , H .D . Two-Phase Flow in Pipes . University of Tulsa , INTERCOMP Course , The Hague , 1974 .. [ 35 ] Kumar , N. Compressibility factors for natural and sour reservoir gases by correlations and cubic equations of state . Thesis of master of science in Petroleum Engineering , 2004 , Texas Tech University. `` '' '' ''",https://github.com/jjgomera/pychemqt/blob/e048915d151e1688856157184e9fc7376ca3c9b2/lib/crude.py#L369,Python,yes,book,number,method,reference,science,paper not available,no
"`` write down the rupture force in pN based on '' '' '' Hatch , K. , Danilowicz , C. , Coljee , V. , and Prentiss , M. ( 2008 ) . Demonstration that the shear force required to separate short double-stranded DNA does not increase significantly with sequence length for sequences longer than 25 base pairs . Phys . Rev . E 78 , 011920. `` '' '' ''",https://github.com/prheenan/Research/blob/9d015b554cef582372359ceaf6ef8e48b8926c9f/Perkins/Projects/WetLab/Demos/Cantilever_Drag/Main_Drag.py#L83,Python,yes,journal,background,n/a,related,other,no transfer,no
"`` < -- globalinfo-start -- > Class implementing an Apriori-type algorithm . Iteratively reduces the minimum support until it finds the required number of rules with the given minimum confidence. < br > The algorithm has an option to mine class association rules . It is adapted as explained in the second reference. < br > < br > For more information see : < br > < br > R. Agrawal , R. Srikant : Fast Algorithms for Mining Association Rules in Large Databases . In : 20th International Conference on Very Large Data Bases , 478-499 , 1994. < br > < br > Bing Liu , Wynne Hsu , Yiming Ma : Integrating Classification and Association Rule Mining . In : Fourth International Conference on Knowledge Discovery and Data Mining , 80-86 , 1998 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Agrawal1994 , author = { R. Agrawal and R. Srikant } , booktitle = { 20th International Conference on Very Large Data Bases } , pages = { 478-499 } , publisher = { Morgan Kaufmann , Los Altos , CA } , title = { Fast Algorithms for Mining Association Rules in Large Databases } , year = { 1994 } } & 64 ; inproceedings { Liu1998 , author = { Bing Liu and Wynne Hsu and Yiming Ma } , booktitle = { Fourth International Conference on Knowledge Discovery and Data Mining } , pages = { 80-86 } , publisher = { AAAI Press } , title = { Integrating Classification and Association Rule Mining } , year = { 1998 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -N & lt ; required number of rules output & gt ; The required number of rules . ( default = 10 ) < pre > < pre > -T & lt ; 0=confidence | 1=lift | 2=leverage | 3=Conviction & gt ; The metric type by which to rank rules . ( default = confidence ) < pre > < pre > -C & lt ; minimum metric score of a rule & gt ; The minimum confidence of a rule . ( default = 0.9 ) < pre > < pre > -D & lt ; delta for minimum support & gt ; The delta by which the minimum support is decreased in each iteration . ( default = 0.05 ) < pre > < pre > -U & lt ; upper bound for minimum support & gt ; Upper bound for minimum support . ( default = 1.0 ) < pre > < pre > -M & lt ; lower bound for minimum support & gt ; The lower bound for the minimum support . ( default = 0.1 ) < pre > < pre > -S & lt ; significance level & gt ; If used , rules are tested for significance at the given level . Slower . ( default = no significance testing ) < pre > < pre > -I If set the itemsets found are also output . ( default = no ) < pre > < pre > -R Remove columns that contain all missing values ( default = no ) < pre > < pre > -V Report progress iteratively . ( default = no ) < pre > < pre > -A If set class association rules are mined . ( default = no ) < pre > < pre > -Z Treat zero ( i.e . first value of nominal attributes ) as missing < pre > < pre > -B & lt ; toString delimiters & gt ; If used , two characters to use as rule delimiters in the result of toString : the first to delimit fields , the second to delimit items within fields . ( default = traditional toString result ) < pre > < pre > -c & lt ; the class index & gt ; The class index . ( default = last ) < pre > < -- options-end -- > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Mark Hall ( mhall @ cs.waikato.ac.nz ) @ author Stefan Mutter ( mutter @ cs.waikato.ac.nz ) @ version $ Revision : 10203 $ ``",https://github.com/umple/umple/blob/67002649b1a9db2c15b6e9bb5de7b803204e1b14/Umplificator/UmplifiedProjects/weka-umplified-0/src/main/java/weka/associations/Apriori.java#L48,Java,yes,book,number,method,reference,science,paper not available,no
"`` class MoreauJeanDirectProjectionOSI brief One Step time Integrator for First Order Dynamical Systems for mechanical Systems ( LagrangianDS and NewtonEulerDS ) with Direct Projection Algorithm author SICONOS Development Team - copyright INRIA version 3.4.0. date ( Creation ) May 02 , 2012 This class reimplement a special activation of constraints in the MoreauJeanOSI for the Direct Projection Algorithm References : V. Acary . Projected event-capturing time-stepping schemes for nonsmooth mechanical systems with unilateral contact and coulomb ‚Äô s friction . Computer Methods in Applied Mechanics and Engineering , 256:224 ‚Äì 250 , 2013 . ISSN 0045-7825 . URL http : www.sciencedirect.com science article pii S0045782512003829. ``",https://github.com/siconos/siconos/blob/bbb742d4a4409e2e6b612e9bff5adf9de0f4c990/kernel/src/simulationTools/MoreauJeanDirectProjectionOSI.hpp#L32,C++,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` Compute curvature at each point of a surface mesh This curvature computation is based on a robust estimation of the 3D curvature tensor field as implemented in Matlab by Gabriel Peyre : http : uk.mathworks.com matlabcentral fileexchange 5355-toolbox-graph content toolbox_graph compute_curvature.m The algorithm is detailed in David Cohen-Steiner and Jean-Marie Morvan . Restricted Delaunay triangulations and normal cycle . In Proc . 19th Annual ACM Symposium on Computational Geometry , pages 237-246 , 2003. and also in Pierre Alliez , David Cohen-Steiner , Olivier Devillers , Bruno Le¬évy , and Mathieu Desbrun . Anisotropic Polygonal Remeshing . ACM Transactions on Graphics , 2003 . Note : SIGGRAPH '2003 Conference Proceedings Alternatively , the vtkCurvatures filter can be used to compute mean curvature , Gauss curvature , and minimum and maximum curvature . To enable the use of this VTK filter , call VtkCurvaturesOn ( ) . This option is ignored when the curvature tensor or principle directions are requested . If multiple curvature types supported by vtkCurvatures are requested , the minimum and maximum curvatures are computed by this filter from the mean and Gauss curvature values obtained by vtkCurvatures , to avoid the duplicate computation of these curvature values when using the vtkCurvatures filter . This computation is identical to vtkCurvatures : :GetMinimumCurvature and vtkCurvatures : :GetMaximumCurvature. ``",https://github.com/BioMedIA/MIRTK/blob/b77743e1526ef57b14fa46b3e43d1481f0c03545/Modules/PointSet/include/mirtk/SurfaceCurvature.h#L32,C++,yes,conference,background,n/a,related,computer vision,no transfer,no
"`` - - Mode : python ; tab-width : 4 ; indent-tabs-mode : nil ; coding : utf-8 - - vim : tabstop=4 expandtab shiftwidth=4 softtabstop=4 MDAnalysis -- - https : www.mdanalysis.org Copyright ( c ) 2006-2017 The MDAnalysis Development Team and contributors ( see the file AUTHORS for the full list of names ) Released under the GNU Public Licence , v2 or any higher version Please cite your use of MDAnalysis in published work : R. J. Gowers , M. Linke , J. Barnoud , T. J. E. Reddy , M. N. Melo , S. L. Seyler , D. L. Dotson , J. Domanski , S. Buchoux , I. M. Kenney , and O. Beckstein . MDAnalysis : A Python package for the rapid analysis of molecular dynamics simulations . In S. Benthall and S. Rostrup editors , Proceedings of the 15th Python in Science Conference , pages 102-109 , Austin , TX , 2016 . SciPy . N. Michaud-Agrawal , E. J. Denning , T. B. Woolf , and O. Beckstein . MDAnalysis : A Toolkit for the Analysis of Molecular Dynamics Simulations . J. Comput . Chem . 32 ( 2011 ) , 2319 -- 2327 , doi:10.1002 jcc.21787 `` '' '' Null output -- - : mod : ` MDAnalysis.coordinates.null ` ==================================================The : class : ` NullWriter ` provides a Writer instance that behaves likeany other writer but effectively ignores its input and does not writeany output files . This is similar to writing to `` dev null `` .This class exists to allow developers writing generic code and tests.Classes -- -- -- -.. autoclass : : NullWriter : members : '' '' '' ''",https://github.com/MDAnalysis/mdanalysis/blob/bdb1844d17d1be6340452a25e95ed39d614d9edf/package/MDAnalysis/coordinates/null.py#L1,Python,yes,conference,background,n/a,related,simulation,no transfer,yes
"`` < -- globalinfo-start -- > Runs Partial Least Square Regression over the given instances and computes the resulting beta matrix for prediction. < br > By default it replaces missing values and centers the data. < br > < br > For more information see : < br > < br > Tormod Naes , Tomas Isaksson , Tom Fearn , Tony Davies ( 2002 ) . A User Friendly Guide to Multivariate Calibration and Classification . NIR Publications. < br > < br > StatSoft , Inc.. Partial Least Squares ( PLS ) . < br > < br > Bent Jorgensen , Yuri Goegebeur . Module 7 : Partial least squares regression I. < br > < br > S. de Jong ( 1993 ) . SIMPLS : an alternative approach to partial least squares regression . Chemometrics and Intelligent Laboratory Systems . 18:251-263 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; book { Naes2002 , author = { Tormod Naes and Tomas Isaksson and Tom Fearn and Tony Davies } , publisher = { NIR Publications } , title = { A User Friendly Guide to Multivariate Calibration and Classification } , year = { 2002 } , ISBN = { 0-9528666-2-5 } } & 64 ; misc { missing_id , author = { StatSoft , Inc. } , booktitle = { Electronic Textbook StatSoft } , title = { Partial Least Squares ( PLS ) } , HTTP = { http : www.statsoft.com textbook stpls.html } } & 64 ; misc { missing_id , author = { Bent Jorgensen and Yuri Goegebeur } , booktitle = { ST02 : Multivariate Data Analysis and Chemometrics } , title = { Module 7 : Partial least squares regression I } , HTTP = { http : statmaster.sdu.dk courses ST02 module07 } } & 64 ; article { Jong1993 , author = { S. de Jong } , journal = { Chemometrics and Intelligent Laboratory Systems } , pages = { 251-263 } , title = { SIMPLS : an alternative approach to partial least squares regression } , volume = { 18 } , year = { 1993 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D Turns on output of debugging information. < pre > < pre > -C & lt ; num & gt ; The number of components to compute . ( default : 20 ) < pre > < pre > -U Updates the class attribute as well . ( default : off ) < pre > < pre > -M Turns replacing of missing values on . ( default : off ) < pre > < pre > -A & lt ; SIMPLS|PLS1 & gt ; The algorithm to use . ( default : PLS1 ) < pre > < pre > -P & lt ; none|center|standardize & gt ; The type of preprocessing that is applied to the data . ( default : center ) < pre > < -- options-end -- > @ author FracPete ( fracpete at waikato dot ac dot nz ) @ version $ Revision : 1.3.2.1 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_scripting_upgrade-waikato/weka/filters/supervised/attribute/PLSFilter.java#L52,Java,yes,book,algorithm,class,reference,machine learning,paper not available,yes
"`` Compute Euclidean distance function to a specified value . param value Reference value . param metric Type of metric . Can be < tt > { 0=Chebyshev | 1=Manhattan | 2=Euclidean | 3=Squared-euclidean } < tt > . ote The distance transform implementation has been submitted by A. Meijster , and implements the article ' W.H . Hesselink , A. Meijster , J.B.T.M . Roerdink , `` A general algorithm for computing distance transforms in linear time . `` , In : Mathematical Morphology and its Applications to Image and Signal Processing , J. Goutsias , L. Vincent , and D.S . Bloomberg ( eds . ) , Kluwer , 2000 , pp . 331-340 . ' The submitted code has then been modified to fit CImg coding style and constraints. ``",https://github.com/gideros/gideros/blob/5e1c3e114d1b97a509722806566742299b8a97bd/plugins/gmedia/source/iOS/Plugins/Media/CImg.h#L25797,C,yes,book,algorithm,method,reference,other,pseudocode to source code,no
"`` Author : Mohamed S. Ebeida ( msebeid @ sandia.gov ) Description : This class is based on the method presented in the following article M. S. Ebeida , S. A. Mitchell , A. Patney , A . A. Davidson , and J. D. Owens , `` A simple algorithm for maximal Poisson-disk sampling in high dimensions '' , Computer Graphics Forum ( Eurographics 2012 ) , 31 ( 2 ) , May 2012. input : a set of conforming simplices defining the boundaries of a ( non-convex ) domain + a ditribution radius ( greater than the smallest feature size ) output : a maximal Poisson-disk sample The Random Number Generator is provided by George Marsaglia available at http : www.velocityreviews.com forums t720512-re-rngs-a-double-kiss.html Last modified : 11 21 2012 ``",https://github.com/trilinos/Trilinos/blob/d1a98e6faf63065bee9155d63ad18248d3d32ad8/packages/meshinggenie/src/standalone/mps/MeshingGenie_mps_nd.h#L43,C++,yes,journal,background,n/a,related,science,no transfer,no
"`` Implement addition , subtraction , multiplication and division based on : `` Software for Doubled-Precision Floating-Point Computations '' , by Seppo Linnainmaa , ACM TOMS vol 7 no 3 , September 1981 , pages 272-283 . ''",https://github.com/root-project/root/blob/8921a1c18564d4a606d11a1173f42c7df8501dfd/interpreter/llvm/src/lib/Support/APFloat.cpp#L3902,C++,yes,journal,algorithm,method,reference,data science,pseudocode to source code,no
"`` Constructs a new , empty treemap layout . Layouts are not typically constructed directly ; instead , they are added to an existing panel via { @ link pv.Mark add } . @ class Implements a space-filling rectangular layout , with the hierarchy represented via containment . Treemaps represent nodes as boxes , with child nodes placed within parent boxes . The size of each box is proportional to the size of the node in the tree . This particular algorithm is taken from Bruls , D.M. , C. Huizing , and J.J. van Wijk , < a href= '' http : www.win.tue.nl ~vanwijk stm.pdf '' > '' Squarified Treemaps '' < a > in < i > Data Visualization 2000 , Proceedings of the Joint Eurographics and IEEE TCVG Sumposium on Visualization < i > , 2000 , pp . 33-42 . < p > The meaning of the exported mark prototypes changes slightly in the space-filling implementation : < ul > < li > < tt > node < tt > - for rendering nodes ; typically a { @ link pv.Bar } . The node data is populated with < tt > dx < tt > and < tt > dy < tt > attributes , in addition to the standard < tt > x < tt > and < tt > y < tt > position attributes . < p > < li > < tt > leaf < tt > - for rendering leaf nodes only , with no fill or stroke style by default ; typically a { @ link pv.Panel } or another layout < p > < li > < tt > link < tt > - unsupported ; undefined . Links are encoded implicitly in the arrangement of the space-filling nodes . < p > < li > < tt > label < tt > - for rendering node labels ; typically a { @ link pv.Label } . < ul > For more details on how to use this layout , see { @ link pv.Layout.Hierarchy } . @ extends pv.Layout.Hierarchy ``",https://github.com/transmart/transmartApp/blob/58d42be64be656d2b9efc99941c4c286526915a2/web-app/js/datasetExplorer/protovis/protovis.js#L11499,JavaScript,yes,conference,background,n/a,related,science,no transfer,no
"`` % Quasi-Newton direction We will calculate a quasi-Newton direction for the related minimization problem & nbsp ; & nbsp ; f $ f ( x ) = frac { 1 } { 2 } |F ( x ) |^2 . f $ We compute the direction & nbsp ; & nbsp ; f $ d_k = -H_k g_k . f $ Here f $ H_k f $ is a limited-memory BFGS ( L-BFGS ) approximation to the < em > inverse < em > Hessian of f $ f f $ at f $ x_k f $ , and f $ g_k = J_k^T F_k f $ represents the gradient of f $ f f $ at f $ x_k f $ . The f $ H_k f $ for L-BFGS is only stored implicitly using information from the f $ m f $ most recent changes in f $ x f $ and changes in f $ g f $ . Specifically , we store pairs f $ ( s_i , y_i ) f $ for f $ i=0 , dots , m-1 f $ . Here f $ ( s_0 , y_0 ) f $ denotes the < em > oldest < em > information , i.e. , & nbsp ; & nbsp ; f $ s_0 = x_ { k+1-m } - x_ { k-m } quad mbox { and } quad y_0 = g_ { k+1-m } - g_ { k-m } . f $ Likewise , f $ ( s_m , y_m ) f $ denotes the < em > newest < em > information , i.e. , & nbsp ; & nbsp ; f $ s_ { m-1 } = x_ { k+1 } - x_ { k } quad mbox { and } quad y_ { m-1 } = g_ { k+1 } - g_ { k } . f $ The two-loop recursion formula for computing f $ d = -H_k g_k f $ is then given as follows . < ol > < li > f $ d ; leftarrow ; -g_k f $ < li > For f $ i=m-1 , dots,0 f $ & nbsp ; & nbsp ; f $ alpha_i = rho_i s_i^T d quad mbox { ( store $ alpha_i $ ) } f $ & nbsp ; & nbsp ; f $ d ; leftarrow ; d - alpha_i y_i f $ < li > f $ d ; leftarrow ; displaystyle frac { s_ { m-1 } ^T y_ { m-1 } } { y_ { m-1 } ^T y_ { m-1 } } d f $ < li > For f $ i=0 , dots , m-1 f $ & nbsp ; & nbsp ; f $ beta = rho_i y_i^T d f $ & nbsp ; & nbsp ; f $ d ; leftarrow ; d + ( alpha_i - beta ) s_i f $ < ol > < b > References < b > < ul > < li > R. H. Byrd , J. Nocedal , and R. B. Schnabel , Representations of quasi-Newton matrices and their use in limited memory methods , < em > Math . Prog. < em > 63 ( 1994 ) : 129-156 . < li > J. Nocedal , Updating quasi-Newton matrices with limited storage , < em > Mathematics of Computation < em > 35 ( 1980 ) : 773-782 . < ul > < b > Parameters < b > To use this direction , specify that the `` Method '' is `` QuasiNewton '' in the `` Direction '' sublist of the parameters that are passed to the solver ( see NOX : :Direction : :Manager for more information on choosing the search direction ) . The following options are valid in the `` Quasi-Newton '' sublist of the `` Direction '' sublist of the parameters that are passed to the nonlinear solver : - `` Memory '' - Number of saved updates , defaults to 5. ``",https://github.com/trilinos/Trilinos/blob/d1a98e6faf63065bee9155d63ad18248d3d32ad8/packages/nox/src/NOX_Direction_QuasiNewton.H#L71,C++,yes,journal,background,n/a,related,science,no transfer,no
"`` Value from : Y. Jiang and Y. Deng , Strong pseudoprimes to the first eight prime bases , Math . Comp . 83 ( 290 ) :2915-2924 , 2014. ``",https://github.com/trueos/trueos/blob/19e1c316cb3f0dd4f5778723979e56a09a9d3da7/usr.bin/primes/spsp.c#L173,C,yes,journal,number,method,reference,networks and os,numbers to hard coded values,no
"`` Triangle triangle intersection test routine , by Tomas Moller , 1997 . See article `` A Fast Triangle-Triangle Intersection Test '' , Journal of Graphics Tools , 2 ( 2 ) , 1997 updated : 2001-06-20 ( added line of intersection ) int tri_tri_intersect ( float V0 [ 3 ] , float V1 [ 3 ] , float V2 [ 3 ] , float U0 [ 3 ] , float U1 [ 3 ] , float U2 [ 3 ] ) parameters : vertices of triangle 1 : V0 , V1 , V2 vertices of triangle 2 : U0 , U1 , U2 result : returns 1 if the triangles intersect , otherwise 0 Here is a version withouts divisions ( a little faster ) int NoDivTriTriIsect ( float V0 [ 3 ] , float V1 [ 3 ] , float V2 [ 3 ] , float U0 [ 3 ] , float U1 [ 3 ] , float U2 [ 3 ] ) ; This version computes the line of intersection as well ( if they are not coplanar ) : int tri_tri_intersect_with_isectline ( float V0 [ 3 ] , float V1 [ 3 ] , float V2 [ 3 ] , float U0 [ 3 ] , float U1 [ 3 ] , float U2 [ 3 ] , int coplanar , float isectpt1 [ 3 ] , float isectpt2 [ 3 ] ) ; coplanar returns whether the tris are coplanar isectpt1 , isectpt2 are the endpoints of the line of intersection ``",https://github.com/FreeCAD/FreeCAD/blob/c4fc02cbcfff975712e977dc08f859fba71ba0ad/src/Mod/Mesh/App/Core/tritritest.h#L1,C++,yes,journal,algorithm,file,reference,simulation,formulas to source code,no
"`` `` '' '' Get the forward committors of the reaction sources - > sinks . Parameters -- -- -- -- -- sources : array_like , int The set of unfolded reactant states . sinks : array_like , int The set of folded product states . tprob : np.ndarray Transition matrix Returns -- -- -- - forward_committors : np.ndarray The forward committors for the reaction sources - > sinks References -- -- -- -- -- .. [ 1 ] Weinan , E. and Vanden-Eijnden , E. Towards a theory of transition paths . J. Stat . Phys . 123 , 503-523 ( 2006 ) . .. [ 2 ] Metzner , P. , Schutte , C. & Vanden-Eijnden , E. Transition path theory for Markov jump processes . Multiscale Model . Simul . 7 , 1192-1219 ( 2009 ) . .. [ 3 ] Berezhkovskii , A. , Hummer , G. & Szabo , A. Reactive flux and folding pathways in network models of coarse-grained protein dynamics . J. Chem . Phys . 130 , 205102 ( 2009 ) . .. [ 4 ] Noe , Frank , et al . `` Constructing the equilibrium ensemble of folding pathways from short off-equilibrium simulations . '' PNAS 106.45 ( 2009 ) : 19011-19016. `` '' '' ''",https://github.com/msmbuilder/msmbuilder/blob/dea86dca81b1ca9c666b410eef2ee72303b5960e/msmbuilder/tpt/committor.py#L219,Python,yes,journal,background,n/a,related,simulation,no transfer,no
"`` brief Detects 2D AGAST corner points . Based on the original work and paper reference by par Elmar Mair , Gregory D. Hager , Darius Burschka , Michael Suppa , and Gerhard Hirzinger . Adaptive and generic corner detection based on the accelerated segment test . In Proceedings of the European Conference on Computer Vision ( ECCV'10 ) , September 2010 . Code example : code pcl : :PointCloud < pcl : :PointXYZRGBA > cloud ; pcl : :AgastKeypoint2D < pcl : :PointXYZRGBA > agast ; agast.setThreshold ( 30 ) ; agast.setInputCloud ( cloud ) ; PointCloud < pcl : :PointUV > keypoints ; agast.compute ( keypoints ) ; endcode ote The AGAST keypoint type used is 7_12s . author Stefan Holzer , Radu B. Rusu ingroup keypoints ``",https://github.com/PointCloudLibrary/pcl/blob/d5759fdb8a5cc751821a29770fb0ce9b8f34faeb/keypoints/include/pcl/keypoints/agast_2d.h#L725,C++,yes,conference,background,n/a,related,computer vision,no transfer,no
"`` u '' '' '' [ 169 ] Sillesen , A. ; Ratajczak , E. ; Pagsberg , P. Chem . Phys . Lett . 1993 , 201 , 171.Data derived from fitting to a complex mechanism . Excitation : radiolysis , analysis : IR absroption . Pressure 0.10 barH + C2H5 ( + M ) -- > C2H6 ( + M ) ( Rxn . 2b ) NHP Verified by Greg Magoon ; I changed the DA uncertainty from ( times divide ) 1.1 to ( + - ) 1E13 , as this is what the abstract reports ( also , Table 3 mentions uncertainties in the range of 10 % -20 % ) '' '' '' ''",https://github.com/ReactionMechanismGenerator/RMG-database/blob/33db9f3bc9fe4cd6f364e36bdc7d30a1620f911f/input/kinetics/families/R_Recombination/rules.py#L134,Python,yes,journal,science,method,reference,science,scientific finding to hardcoded rule,no
"`` jidctint.c Copyright ( C ) 1991-1998 , Thomas G. Lane . Modification developed 2002-2009 by Guido Vollbeding . This file is part of the Independent JPEG Group 's software . For conditions of distribution and use , see the accompanying README file . This file contains a slow-but-accurate integer implementation of the inverse DCT ( Discrete Cosine Transform ) . In the IJG code , this routine must also perform dequantization of the input coefficients . A 2-D IDCT can be done by 1-D IDCT on each column followed by 1-D IDCT on each row ( or vice versa , but it 's more convenient to emit a row at a time ) . Direct algorithms are also available , but they are much more complex and seem not to be any faster when reduced to code . This implementation is based on an algorithm described in C. Loeffler , A. Ligtenberg and G. Moschytz , `` Practical Fast 1-D DCT Algorithms with 11 Multiplications '' , Proc . Int ' l . Conf . on Acoustics , Speech , and Signal Processing 1989 ( ICASSP '89 ) , pp . 988-991 . The primary algorithm described there uses 11 multiplies and 29 adds . We use their alternate method with 12 multiplies and 32 adds . The advantage of this method is that no data path contains more than one multiplication ; this allows a very simple and accurate implementation in scaled fixed-point arithmetic , with a minimal number of shifts . We also provide IDCT routines with various output sample block sizes for direct resolution reduction or enlargement and for direct resolving the common 2x1 and 1x2 subsampling cases without additional resampling : NxN ( N=1 ... 16 ) , 2NxN , and Nx2N ( N=1 ... 8 ) pixels for one 8x8 input DCT block . For N < 8 we simply take the corresponding low-frequency coefficients of the 8x8 input DCT block and apply an NxN point IDCT on the sub-block to yield the downscaled outputs . This can be seen as direct low-pass downsampling from the DCT domain point of view rather than the usual spatial domain point of view , yielding significant computational savings and results at least as good as common bilinear ( averaging ) spatial downsampling . For N > 8 we apply a partial NxN IDCT on the 8 input coefficients as lower frequencies and higher frequencies assumed to be zero . It turns out that the computational effort is similar to the 8x8 IDCT regarding the output size . Furthermore , the scaling and descaling is the same for all IDCT sizes . CAUTION : We rely on the FIX ( ) macro except for the N=1,2,4,8 cases since there would be too many additional constants to pre-calculate. ``",https://github.com/kunkun39/CH_BOX/blob/7311a61c33f31d9c125ec21ae0155d9d58f88615/chbox/TVhelper/android-pdfview/jni/mupdf/jpeg/jidctint.c#L1,C,yes,conference,algorithm,file,reference,other,formulas to source code,no
"`` This is a maximally equidistributed combined Tausworthe generator based on code from GNU Scientific Library 1.5 ( 30 Jun 2004 ) x_n = ( s1_n ^ s2_n ^ s3_n ) s1_ { n+1 } = ( ( ( s1_n & 4294967294 ) < < 12 ) ^ ( ( ( s1_n < < 13 ) ^ s1_n ) > > 19 ) ) s2_ { n+1 } = ( ( ( s2_n & 4294967288 ) < < 4 ) ^ ( ( ( s2_n < < 2 ) ^ s2_n ) > > 25 ) ) s3_ { n+1 } = ( ( ( s3_n & 4294967280 ) < < 17 ) ^ ( ( ( s3_n < < 3 ) ^ s3_n ) > > 11 ) ) The period of this generator is about 2^88 . From : P. L'Ecuyer , `` Maximally Equidistributed Combined Tausworthe Generators '' , Mathematics of Computation , 65 , 213 ( 1996 ) , 203 -- 213 . This is available on the net from L'Ecuyer 's home page , http : www.iro.umontreal.ca ~lecuyer myftp papers tausme.ps ftp : ftp.iro.umontreal.ca pub simulation lecuyer papers tausme.ps There is an erratum in the paper `` Tables of Maximally Equidistributed Combined LFSR Generators '' , Mathematics of Computation , 68 , 225 ( 1999 ) , 261 -- 269 : http : www.iro.umontreal.ca ~lecuyer myftp papers tausme2.ps ... the k_j most significant bits of z_j must be non- zero , for each j . ( Note : this restriction also applies to the computer code given in [ 4 ] , but was mistakenly not mentioned in that paper . ) This affects the seeding procedure by imposing the requirement s1 > 1 , s2 > 7 , s3 > 15. ``",https://github.com/DirtyUnicorns/android_kernel_lge_bullhead/blob/c0865f991e21ff902433dae45892f7b839e46b7a/lib/random32.c#L1,C,yes,journal,background,n/a,related,networks and os,no transfer,no
"`` Purpose -- -- -- - SGEHRD2 reduces a REAL general matrix A to upper Hessenberg form H by an orthogonal similarity transformation : Q ' A Q = H . Arguments -- -- -- -- - @ param [ in ] n INTEGER The order of the matrix A. N > = 0 . @ param [ in ] ilo INTEGER @ param [ in ] ihi INTEGER It is assumed that A is already upper triangular in rows and columns 1 : ILO-1 and IHI+1 : N. ILO and IHI are normally set by a previous call to SGEBAL ; otherwise they should be set to 1 and N respectively . See Further Details . 1 < = ILO < = IHI < = N , if N > 0 ; ILO=1 and IHI=0 , if N=0 . @ param [ in , out ] A REAL array , dimension ( LDA , N ) On entry , the N-by-N general matrix to be reduced . On exit , the upper triangle and the first subdiagonal of A are overwritten with the upper Hessenberg matrix H , and the elements below the first subdiagonal , with the array TAU , represent the orthogonal matrix Q as a product of elementary reflectors . See Further Details . @ param [ in ] lda INTEGER The leading dimension of the array A. LDA > = max ( 1 , N ) . @ param [ out ] tau REAL array , dimension ( N-1 ) The scalar factors of the elementary reflectors ( see Further Details ) . Elements 1 : ILO-1 and IHI : N-1 of TAU are set to zero . @ param [ out ] work ( workspace ) REAL array , dimension ( LWORK ) On exit , if INFO = 0 , WORK [ 0 ] returns the optimal LWORK . @ param [ in ] lwork INTEGER The length of the array WORK . LWORK > = max ( 1 , N ) . For optimum performance LWORK > = N NB , where NB is the optimal blocksize . If LWORK = -1 , then a workspace query is assumed ; the routine only calculates the optimal size of the WORK array , returns this value as the first entry of the WORK array , and no error message related to LWORK is issued by XERBLA . @ param [ out ] info INTEGER - = 0 : successful exit - < 0 : if INFO = -i , the i-th argument had an illegal value . Further Details -- -- -- -- -- -- -- - The matrix Q is represented as a product of ( ihi-ilo ) elementary reflectors Q = H ( ilo ) H ( ilo+1 ) . . . H ( ihi-1 ) . Each H ( i ) has the form H ( i ) = I - tau v v ' where tau is a real scalar , and v is a real vector with v ( 1 : i ) = 0 , v ( i+1 ) = 1 and v ( ihi+1 : n ) = 0 ; v ( i+2 : ihi ) is stored on exit in A ( i+2 : ihi , i ) , and tau in TAU ( i ) . The contents of A are illustrated by the following example , with n = 7 , ilo = 2 and ihi = 6 : @ verbatim on entry , on exit , ( a a a a a a a ) ( a a h h h h a ) ( a a a a a a ) ( a h h h h a ) ( a a a a a a ) ( h h h h h h ) ( a a a a a a ) ( v2 h h h h h ) ( a a a a a a ) ( v2 v3 h h h h ) ( a a a a a a ) ( v2 v3 v4 h h h ) ( a ) ( a ) @ endverbatim where a denotes an element of the original matrix A , h denotes a modified element of the upper Hessenberg matrix H , and vi denotes an element of the vector defining H ( i ) . This implementation follows the hybrid algorithm and notations described in S. Tomov and J. Dongarra , `` Accelerating the reduction to upper Hessenberg form through hybrid GPU-based computing , '' University of Tennessee Computer Science Technical Report , UT-CS-09-642 ( also LAPACK Working Note 219 ) , May 24 , 2009 . @ ingroup magma_sgeev_comp ``",https://github.com/cjy7117/FT-MAGMA/blob/f084356fd94ecdce75926a4651f8fd95390280ec/old_source_code/magma-1.6.2-v8/src/sgehrd2.cpp#L15,C++,yes,report,algorithm,file,reference,other,pseudocode to source code,no
"`` @ brief Class CStochasticSOSVM solves SOSVM using stochastic subgradient descent on the SVM primal problem [ 1 ] , which is equivalent to SGD or Pegasos [ 2 ] . This class is inspired by the matlab SGD implementation in [ 3 ] . [ 1 ] N. Ratliff , J . A. Bagnell , and M. Zinkevich . ( online ) subgradient methods for structured prediction . AISTATS , 2007 . [ 2 ] S. Shalev-Shwartz , Y . Singer , N. Srebro . Pegasos : Primal Estimated sub-GrAdient SOlver for SVM . ICML 2007 . [ 3 ] S. Lacoste-Julien , M. Jaggi , M. Schmidt and P. Pletscher . Block-Coordinate Frank-Wolfe Optimization for Structural SVMs . ICML 2013. ``",https://github.com/shogun-toolbox/shogun/blob/13f06e471f0af119136ee35d1da83835942ec9a9/src/shogun/structure/StochasticSOSVM.h#L18,C++,yes,conference,background,n/a,related,machine learning,no transfer,no
"`` This file implements a generic value propagation engine based on the same propagation used by the SSA-CCP algorithm [ 1 ] . Propagation is performed by simulating the execution of every statement that produces the value being propagated . Simulation proceeds as follows : 1- Initially , all edges of the CFG are marked not executable and the CFG worklist is seeded with all the statements in the entry basic block ( block 0 ) . 2- Every statement S is simulated with a call to the call-back function SSA_PROP_VISIT_STMT . This evaluation may produce 3 results : SSA_PROP_NOT_INTERESTING : Statement S produces nothing of interest and does not affect any of the work lists . The statement may be simulated again if any of its input operands change in future iterations of the simulator . SSA_PROP_VARYING : The value produced by S can not be determined at compile time . Further simulation of S is not required . If S is a conditional jump , all the outgoing edges for the block are considered executable and added to the work list . SSA_PROP_INTERESTING : S produces a value that can be computed at compile time . Its result can be propagated into the statements that feed from S. Furthermore , if S is a conditional jump , only the edge known to be taken is added to the work list . Edges that are known not to execute are never simulated . 3- PHI nodes are simulated with a call to SSA_PROP_VISIT_PHI . The return value from SSA_PROP_VISIT_PHI has the same semantics as described in 2 . 4- Three work lists are kept . Statements are only added to these lists if they produce one of SSA_PROP_INTERESTING or SSA_PROP_VARYING . CFG_BLOCKS contains the list of blocks to be simulated . Blocks are added to this list if their incoming edges are found executable . SSA_EDGE_WORKLIST contains the list of statements that we need to revisit . 5- Simulation terminates when all three work lists are drained . Before calling ssa_propagate , it is important to clear prop_simulate_again_p for all the statements in the program that should be simulated . This initialization allows an implementation to specify which statements should never be simulated . It is also important to compute def-use information before calling ssa_propagate . References : [ 1 ] Constant propagation with conditional branches , Wegman and Zadeck , ACM TOPLAS 13 ( 2 ) :181-210 . [ 2 ] Building an Optimizing Compiler , Robert Morgan , Butterworth-Heinemann , 1998 , Section 8.9 . [ 3 ] Advanced Compiler Design and Implementation , Steven Muchnick , Morgan Kaufmann , 1997 , Section 12.6 ``",https://github.com/DragonFlyBSD/DragonFlyBSD/blob/ce2989fe1212f664d615268edc64a57801fc7404/contrib/gcc-8.0/gcc/tree-ssa-propagate.c#L42,C,yes,journal,background,n/a,related,networks and os,no transfer,no
"`` `` '' '' Pattern tracking with a dynamic Bayesian network ( DBN ) approximated by a Hidden Markov Model ( HMM ) . Parameters -- -- -- -- -- pattern_files : list List of files with the patterns ( including the fitted GMMs and information about the number of beats ) . min_bpm : list , optional Minimum tempi used for pattern tracking [ bpm ] . max_bpm : list , optional Maximum tempi used for pattern tracking [ bpm ] . num_tempi : int or list , optional Number of tempi to model ; if set , limit the number of tempi and use a log spacings , otherwise a linear spacings . transition_lambda : float or list , optional Lambdas for the exponential tempo change distributions ( higher values prefer constant tempi from one beat to the next one ) . fps : float , optional Frames per second . Notes -- -- - ` min_bpm ` , ` max_bpm ` , ` num_tempo_states ` , and ` transition_lambda ` must contain as many items as rhythmic patterns are modeled ( i.e . length of ` pattern_files ` ) . If a single value is given for ` num_tempo_states ` and ` transition_lambda ` , this value is used for all rhythmic patterns . Instead of the originally proposed state space and transition model for the DBN [ 1 ] _ , the more efficient version proposed in [ 2 ] _ is used . References -- -- -- -- -- .. [ 1 ] Florian Krebs , Sebastian B√∂ck and Gerhard Widmer , `` Rhythmic Pattern Modeling for Beat and Downbeat Tracking in Musical Audio '' , Proceedings of the 15th International Society for Music Information Retrieval Conference ( ISMIR ) , 2013 . .. [ 2 ] Florian Krebs , Sebastian B√∂ck and Gerhard Widmer , `` An Efficient State Space Model for Joint Tempo and Meter Tracking '' , Proceedings of the 16th International Society for Music Information Retrieval Conference ( ISMIR ) , 2015 . Examples -- -- -- -- Create a PatternTrackingProcessor from the given pattern files . These pattern files include fitted GMMs for the observation model of the HMM . The returned array represents the positions of the beats and their position inside the bar . The position is given in seconds , thus the expected sampling rate is needed . The position inside the bar follows the natural counting and starts at 1 . > > > from madmom.models import PATTERNS_BALLROOM > > > proc = PatternTrackingProcessor ( PATTERNS_BALLROOM , fps=50 ) > > > proc doctest : +ELLIPSIS < madmom.features.downbeats.PatternTrackingProcessor object at 0x ... > Call this PatternTrackingProcessor with a multi-band spectrogram to obtain the beat and downbeat positions . The parameters of the spectrogram have to correspond to those used to fit the GMMs . > > > from madmom.audio.spectrogram import LogarithmicSpectrogramProcessor , SpectrogramDifferenceProcessor , MultiBandSpectrogramProcessor > > > from madmom.processors import SequentialProcessor > > > log = LogarithmicSpectrogramProcessor ( ) > > > diff = SpectrogramDifferenceProcessor ( positive_diffs=True ) > > > mb = MultiBandSpectrogramProcessor ( crossover_frequencies= [ 270 ] ) > > > pre_proc = SequentialProcessor ( [ log , diff , mb ] ) > > > act = pre_proc ( 'tests data audio sample.wav ' ) > > > proc ( act ) doctest : +ELLIPSIS array ( [ [ 0.82 , 4 . ] , [ 1.78 , 1 . ] , ... , [ 3.7 , 3 . ] , [ 4.66 , 4 . ] ] ) `` `` '' ''",https://github.com/CPJKU/madmom/blob/f163865521c0c126a7719704aec8b90e6af30e55/madmom/features/downbeats.py#L445,Python,yes,conference,background,n/a,related,data science,no transfer,yes
"`` < -- globalinfo-start -- > The class that builds a BallTree middle out. < br > < br > For more information see also : < br > < br > Andrew W. Moore : The Anchors Hierarchy : Using the Triangle Inequality to Survive High Dimensional Data . In : UAI '00 : Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence , San Francisco , CA , USA , 397-405 , 2000. < br > < br > Ashraf Masood Kibriya ( 2007 ) . Fast Algorithms for Nearest Neighbour Search . Hamilton , New Zealand . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Moore2000 , address = { San Francisco , CA , USA } , author = { Andrew W. Moore } , booktitle = { UAI '00 : Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence } , pages = { 397-405 } , publisher = { Morgan Kaufmann Publishers Inc. } , title = { The Anchors Hierarchy : Using the Triangle Inequality to Survive High Dimensional Data } , year = { 2000 } } & 64 ; mastersthesis { Kibriya2007 , address = { Hamilton , New Zealand } , author = { Ashraf Masood Kibriya } , school = { Department of Computer Science , School of Computing and Mathematical Sciences , University of Waikato } , title = { Fast Algorithms for Nearest Neighbour Search } , year = { 2007 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -S & lt ; num & gt ; The seed for the random number generator used in selecting random anchor . ( default : 1 ) < pre > < pre > -R Use randomly chosen initial anchors. < pre > < -- options-end -- > @ author Ashraf M. Kibriya ( amk14 [ at-the-rate ] cs [ dot ] waikato [ dot ] ac [ dot ] nz ) @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/trunk/weka/src/main/java/weka/core/neighboursearch/balltrees/MiddleOutConstructor.java#L44,Java,yes,conference,algorithm,class,reference,machine learning,formulas to source code,no
"`` r '' '' '' Asymptotics of Multivariate Generating SeriesLet ` F ( x ) = sum_ { u in NN^d } F_ { u } x^ u ` be a multivariate power serieswith complex coefficients that converges in a neighborhood of the origin.Assume that ` F = G H ` for some functions ` G ` and ` H ` holomorphic in aneighborhood of the origin . Assume also that ` H ` is a polynomial.This computes asymptotics for the coefficients ` F_ { r alpha } ` as ` r to infty ` with ` r alpha in NN^d ` for ` alpha ` in a permissible subset of ` d ` -tuples ofpositive reals . More specifically , it computes arbitrary terms of theasymptotic expansion for ` F_ { r alpha } ` when the asymptotics are controlled bya strictly minimal multiple point of the algebraic variety ` H = 0 ` .The algorithms and formulas implemented here come from [ RaWi2008a ] _and [ RaWi2012 ] _ . For a general reference take a look in the book [ PeWi2013 ] ... [ AiYu1983 ] I.A . Aizenberg and A.P . Yuzhakov . Integral representations and residues in multidimensional complex analysis . Translations of Mathematical Monographs , 58 . American Mathematical Society , Providence , RI . ( 1983 ) . x+283 pp . ISBN : 0-8218-4511-X ... [ Raic2012 ] Alexander Raichev . Leinartas 's partial fraction decomposition . : arxiv : ` 1206.4740 ` ... [ RaWi2008a ] Alexander Raichev and Mark C. Wilson . Asymptotics of coefficients of multivariate generating functions : improvements for smooth points , Electronic Journal of Combinatorics , Vol . 15 ( 2008 ) . R89 : arxiv : ` 0803.2914 ` ... [ RaWi2012 ] Alexander Raichev and Mark C. Wilson . Asymptotics of coefficients of multivariate generating functions : improvements for smooth points . Online Journal of Analytic Combinatorics . Issue 6 , ( 2011 ) . : arxiv : ` 1009.5715 ` ... [ PeWi2013 ] Robin Pemantle and Mark C. Wilson . Analytic Combinatorics in Several Variables . Cambridge University Press , 2013.Introductory Examples===================== : : sage : from sage.rings.asymptotic.asymptotics_multivariate_generating_functions import FractionWithFactoredDenominatorRingA univariate smooth point example : : sage : R. < x > = PolynomialRing ( QQ ) sage : FFPD = FractionWithFactoredDenominatorRing ( R , SR ) sage : H = ( x - 1 2 ) ^3 sage : Hfac = H.factor ( ) sage : G = -1 ( x + 3 ) Hfac.unit ( ) sage : F = FFPD ( G , Hfac ) sage : F ( -1 ( x + 3 ) , [ ( x - 1 2 , 3 ) ] ) sage : alpha = [ 1 ] sage : decomp = F.asymptotic_decomposition ( alpha ) sage : decomp ( 0 , [ ] ) + ( -1 2 r^2 ( x^2 ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) + 6 x ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) + 9 ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) ) - 1 2 r ( 5 x^2 ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) + 24 x ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) + 27 ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) ) - 3 x^2 ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) - 9 x ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) - 9 ( x^5 + 9 x^4 + 27 x^3 + 27 x^2 ) , [ ( x - 1 2 , 1 ) ] ) sage : F1 = decomp [ 1 ] sage : p = { x : 1 2 } sage : asy = F1.asymptotics ( p , alpha , 3 ) sage : asy ( 8 343 ( 49 r^2 + 161 r + 114 ) 2^r , 2 , 8 7 r^2 + 184 49 r + 912 343 ) sage : F.relative_error ( asy [ 0 ] , alpha , [ 1 , 2 , 4 , 8 , 16 ] , asy [ 1 ] ) [ ( ( 1 , ) , 7.555555556 , [ 7.556851312 ] , [ -0.0001714971672 ] ) , ( ( 2 , ) , 14.74074074 , [ 14.74052478 ] , [ 0.00001465051901 ] ) , ( ( 4 , ) , 35.96502058 , [ 35.96501458 ] , [ 1.667911934e-7 ] ) , ( ( 8 , ) , 105.8425656 , [ 105.8425656 ] , [ 4.399565380e-11 ] ) , ( ( 16 , ) , 355.3119534 , [ 355.3119534 ] , [ 0.0000000000 ] ) ] Another smooth point example ( Example 5.4 of [ RaWi2008a ] _ ) : : sage : R. < x , y > = PolynomialRing ( QQ ) sage : FFPD = FractionWithFactoredDenominatorRing ( R ) sage : q = 1 2 sage : qq = q.denominator ( ) sage : H = 1 - q x + q x y - x^2 y sage : Hfac = H.factor ( ) sage : G = ( 1 - q x ) Hfac.unit ( ) sage : F = FFPD ( G , Hfac ) sage : alpha = list ( qq vector ( [ 2 , 1 - q ] ) ) sage : alpha [ 4 , 1 ] sage : I = F.smooth_critical_ideal ( alpha ) sage : I Ideal ( y^2 - 2 y + 1 , x + 1 4 y - 5 4 ) of Multivariate Polynomial Ring in x , y over Rational Field sage : s = solve ( [ SR ( z ) for z in I.gens ( ) ] , ... . : [ SR ( z ) for z in R.gens ( ) ] , solution_dict=true ) sage : s == [ { SR ( x ) : 1 , SR ( y ) : 1 } ] True sage : p = s [ 0 ] sage : asy = F.asymptotics ( p , alpha , 1 , verbose=True ) Creating auxiliary functions ... Computing derivatives of auxiliary functions ... Computing derivatives of more auxiliary functions ... Computing second order differential operator actions ... sage : asy ( 1 24 2^ ( 2 3 ) ( sqrt ( 3 ) + 4 ( sqrt ( 3 ) + I ) + I ) gamma ( 1 3 ) ( pi r^ ( 1 3 ) ) , 1 , 1 24 2^ ( 2 3 ) ( sqrt ( 3 ) + 4 ( sqrt ( 3 ) + I ) + I ) gamma ( 1 3 ) ( pi r^ ( 1 3 ) ) ) sage : r = SR ( ' r ' ) sage : tuple ( ( a r^ ( 1 3 ) ) .full_simplify ( ) r^ ( 1 3 ) for a in asy ) make nicer coefficients ( 1 12 sqrt ( 3 ) 2^ ( 2 3 ) gamma ( 1 3 ) ( pi r^ ( 1 3 ) ) , 1 , 1 12 sqrt ( 3 ) 2^ ( 2 3 ) gamma ( 1 3 ) ( pi r^ ( 1 3 ) ) ) sage : F.relative_error ( asy [ 0 ] , alpha , [ 1 , 2 , 4 , 8 , 16 ] , asy [ 1 ] ) [ ( ( 4 , 1 ) , 0.1875000000 , [ 0.1953794675 ... ] , [ -0.042023826 ... ] ) , ( ( 8 , 2 ) , 0.1523437500 , [ 0.1550727862 ... ] , [ -0.017913673 ... ] ) , ( ( 16 , 4 ) , 0.1221771240 , [ 0.1230813519 ... ] , [ -0.0074009592 ... ] ) , ( ( 32 , 8 ) , 0.09739671811 , [ 0.09768973377 ... ] , [ -0.0030084757 ... ] ) , ( ( 64 , 16 ) , 0.07744253816 , [ 0.07753639308 ... ] , [ -0.0012119297 ... ] ) ] A multiple point example ( Example 6.5 of [ RaWi2012 ] _ ) : : sage : R. < x , y > = PolynomialRing ( QQ ) sage : FFPD = FractionWithFactoredDenominatorRing ( R , SR ) sage : H = ( 1 - 2 x - y ) 2 ( 1 - x - 2 y ) 2 sage : Hfac = H.factor ( ) sage : G = 1 Hfac.unit ( ) sage : F = FFPD ( G , Hfac ) sage : F ( 1 , [ ( x + 2 y - 1 , 2 ) , ( 2 x + y - 1 , 2 ) ] ) sage : I = F.singular_ideal ( ) sage : I Ideal ( x - 1 3 , y - 1 3 ) of Multivariate Polynomial Ring in x , y over Rational Field sage : p = { x : 1 3 , y : 1 3 } sage : F.is_convenient_multiple_point ( p ) ( True , 'convenient in variables [ x , y ] ' ) sage : alpha = ( var ( ' a ' ) , var ( ' b ' ) ) sage : decomp = F.asymptotic_decomposition ( alpha ) ; decomp ( 0 , [ ] ) + ( -1 9 r^2 ( 2 a^2 x^2 + 2 b^2 y^2 - 5 a b ( x y ) ) - 1 9 r ( 6 a x^2 + 6 b y^2 - 5 a ( x y ) - 5 b ( x y ) ) - 4 9 x^2 - 4 9 y^2 + 5 9 ( x y ) , [ ( x + 2 y - 1 , 1 ) , ( 2 x + y - 1 , 1 ) ] ) sage : F1 = decomp [ 1 ] sage : F1.asymptotics ( p , alpha , 2 ) ( -3 ( ( 2 a^2 - 5 a b + 2 b^2 ) r^2 + ( a + b ) r + 3 ) ( ( 1 3 ) ^ ( -a ) ( 1 3 ) ^ ( -b ) ) ^r , ( 1 3 ) ^ ( -a ) ( 1 3 ) ^ ( -b ) , -3 ( 2 a^2 - 5 a b + 2 b^2 ) r^2 - 3 ( a + b ) r - 9 ) sage : alpha = [ 4 , 3 ] sage : decomp = F.asymptotic_decomposition ( alpha ) sage : F1 = decomp [ 1 ] sage : asy = F1.asymptotics ( p , alpha , 2 ) sage : asy ( 3 ( 10 r^2 - 7 r - 3 ) 2187^r , 2187 , 30 r^2 - 21 r - 9 ) sage : F.relative_error ( asy [ 0 ] , alpha , [ 1 , 2 , 4 , 8 ] , asy [ 1 ] ) [ ( ( 4 , 3 ) , 30.72702332 , [ 0.0000000000 ] , [ 1.000000000 ] ) , ( ( 8 , 6 ) , 111.9315678 , [ 69.00000000 ] , [ 0.3835519207 ] ) , ( ( 16 , 12 ) , 442.7813138 , [ 387.0000000 ] , [ 0.1259793763 ] ) , ( ( 32 , 24 ) , 1799.879232 , [ 1743.000000 ] , [ 0.03160169385 ] ) ] TESTS : : sage : R. < x , y > = PolynomialRing ( QQ ) sage : FFPD = FractionWithFactoredDenominatorRing ( R ) sage : H = ( 1 - 2 x - y ) ( 1 - x - 2 y ) sage : G = 1 sage : Hfac = H.factor ( ) sage : G = G Hfac.unit ( ) sage : F = FFPD ( G , Hfac ) ; F ( 1 , [ ( x + 2 y - 1 , 1 ) , ( 2 x + y - 1 , 1 ) ] ) sage : p = { x : 1 , y : 1 } sage : alpha = [ 1 , 1 ] sage : F.asymptotics ( p , alpha , 1 ) ( 1 3 , 1 , 1 3 ) : : sage : R. < x , y , t > = PolynomialRing ( QQ ) sage : FFPD = FractionWithFactoredDenominatorRing ( R ) sage : H = ( 1 - y ) ( 1 + x^2 ) ( 1 - t ( 1 + x^2 + x y^2 ) ) sage : G = ( 1 + x ) ( 1 + x^2 - x y^2 ) sage : Hfac = H.factor ( ) sage : G = G Hfac.unit ( ) sage : F = FFPD ( G , Hfac ) ; F ( -x^2 y^2 + x^3 - x y^2 + x^2 + x + 1 , [ ( y - 1 , 1 ) , ( x^2 + 1 , 1 ) , ( x y^2 t + x^2 t + t - 1 , 1 ) ] ) sage : p = { x : 1 , y : 1 , t : 1 3 } sage : alpha = [ 1 , 1 , 1 ] sage : F.asymptotics_multiple ( p , alpha , 1 , var ( ' r ' ) ) not tested - see 19989Various=======AUTHORS : - Alexander Raichev ( 2008 ) - Daniel Krenn ( 2014 , 2016 ) Classes and Methods=================== '' '' '' ''",https://github.com/sagemath/sage/blob/854f9764d14236110b8d7f7b35a7d52017e044f8/src/sage/rings/asymptotic/asymptotics_multivariate_generating_functions.py#L1,Python,yes,book,background,n/a,related,science,no transfer,no
"`` Purpose : TMAT_ROT_AXIS applies an axis rotation to the geometric transformation matrix . Modified : 19 April 1999 Author : John Burkardt Reference : Foley , van Dam , Feiner , Hughes , Computer Graphics , Principles and Practice , Addison Wesley , Second Edition , 1990 . Parameters : Input , float A [ 4 ] [ 4 ] , the current geometric transformation matrix . Output , float B [ 4 ] [ 4 ] , the modified geometric transformation matrix . A and B may share the same memory . Input , float ANGLE , the angle , in degrees , of the rotation . Input , character AXIS , is ' X ' , ' Y ' or ' Z ' , specifying the coordinate axis about which the rotation occurs. ``",https://github.com/multigcs/multigcs/blob/5eeceb892a70b01ac151468e86d395326efec39e/utils/ivcon.c#L12386,C,yes,book,algorithm,method,reference,other,paper not available,no
"`` Conditional constant propagation ( CCP ) is based on the SSA propagation engine ( tree-ssa-propagate.c ) . Constant assignments of the form VAR = CST are propagated from the assignments into uses of VAR , which in turn may generate new constants . The simulation uses a four level lattice to keep track of constant values associated with SSA names . Given an SSA name V_i , it may take one of the following values : UNINITIALIZED - > This is the default starting value . V_i has not been processed yet . UNDEFINED - > V_i is a local variable whose definition has not been processed yet . Therefore we do n't yet know if its value is a constant or not . CONSTANT - > V_i has been found to hold a constant value C. VARYING - > V_i can not take a constant value , or if it does , it is not possible to determine it at compile time . The core of SSA-CCP is in ccp_visit_stmt and ccp_visit_phi_node : 1- In ccp_visit_stmt , we are interested in assignments whose RHS evaluates into a constant and conditional jumps whose predicate evaluates into a boolean true or false . When an assignment of the form V_i = CONST is found , V_i 's lattice value is set to CONSTANT and CONST is associated with it . This causes the propagation engine to add all the SSA edges coming out the assignment into the worklists , so that statements that use V_i can be visited . If the statement is a conditional with a constant predicate , we mark the outgoing edges as executable or not executable depending on the predicate 's value . This is then used when visiting PHI nodes to know when a PHI argument can be ignored . 2- In ccp_visit_phi_node , if all the PHI arguments evaluate to the same constant C , then the LHS of the PHI is set to C. This evaluation is known as the `` meet operation '' . Since one of the goals of this evaluation is to optimistically return constant values as often as possible , it uses two main short cuts : - If an argument is flowing in through a non-executable edge , it is ignored . This is useful in cases like this : if ( PRED ) a_9 = 3 ; else a_10 = 100 ; a_11 = PHI ( a_9 , a_10 ) If PRED is known to always evaluate to false , then we can assume that a_11 will always take its value from a_10 , meaning that instead of consider it VARYING ( a_9 and a_10 have different values ) , we can consider it CONSTANT 100 . - If an argument has an UNDEFINED value , then it does not affect the outcome of the meet operation . If a variable V_i has an UNDEFINED value , it means that either its defining statement has n't been visited yet or V_i has no defining statement , in which case the original symbol ' V ' is being used uninitialized . Since ' V ' is a local variable , the compiler may assume any initial value for it . After propagation , every variable V_i that ends up with a lattice value of CONSTANT will have the associated constant value in the array CONST_VAL [ i ] .VALUE . That is fed into substitute_and_fold for final substitution and folding . Constant propagation in stores and loads ( STORE-CCP ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- While CCP has all the logic to propagate constants in GIMPLE registers , it is missing the ability to associate constants with stores and loads ( i.e. , pointer dereferences , structures and global aliased variables ) . We do n't keep loads and stores in SSA , but we do build a factored use-def web for them ( in the virtual operands ) . For instance , consider the following code fragment : struct A a ; const int B = 42 ; void foo ( int i ) { if ( i > 10 ) a.a = 42 ; else { a.b = 21 ; a.a = a.b + 21 ; } if ( a.a = B ) never_executed ( ) ; } We should be able to deduce that the predicate ' a.a = B ' is always false . To achieve this , we associate constant values to the SSA names in the V_MAY_DEF and V_MUST_DEF operands for each store . Additionally , since we also glob partial loads stores with the base symbol , we also keep track of the memory reference where the constant value was stored ( in the MEM_REF field of PROP_VALUE_T ) . For instance , a_5 = V_MAY_DEF < a_4 > a.a = 2 ; VUSE < a_5 > x_3 = a.b ; In the example above , CCP will associate value ' 2 ' with 'a_5 ' , but it would be wrong to replace the load from ' a.b ' with ' 2 ' , because ' 2 ' had been stored into a.a. To support STORE-CCP , it is necessary to add a new value to the constant propagation lattice . When evaluating a load for a memory reference we can no longer assume a value of UNDEFINED if we have n't seen a preceding store to the same memory location . Consider , for instance global variables : int A ; foo ( int i ) { if ( i_3 > 10 ) A_4 = 3 ; A_5 = PHI ( A_4 , A_2 ) ; VUSE < A_5 > A.0_6 = A ; return A.0_6 ; } The value of A_2 can not be assumed to be UNDEFINED , as it may have been defined outside of foo . If we were to assume it UNDEFINED , we would erroneously optimize the above into 'return 3 ; ' . Therefore , when doing STORE-CCP , we introduce a fifth lattice value ( UNKNOWN_VAL ) , which overrides any other value when computing the meet operation in PHI nodes . Though STORE-CCP is not too expensive , it does have to do more work than regular CCP , so it is only enabled at -O2 . Both regular CCP and STORE-CCP use the exact same algorithm . The only distinction is that when doing STORE-CCP , the boolean variable DO_STORE_CCP is set to true . This affects the evaluation of statements and PHI nodes . References : Constant propagation with conditional branches , Wegman and Zadeck , ACM TOPLAS 13 ( 2 ) :181-210 . Building an Optimizing Compiler , Robert Morgan , Butterworth-Heinemann , 1998 , Section 8.9 . Advanced Compiler Design and Implementation , Steven Muchnick , Morgan Kaufmann , 1997 , Section 12.6 ``",https://github.com/HardenedBSD/hardenedBSD-playground/blob/8e66564ae0c65b1af287b10eab55bed6f771c8e9/contrib/gcc/tree-ssa-ccp.c#L24,C,yes,conference,algorithm,file,reference,networks and os,description to source code,no
"`` r '' '' '' Return the communicability centrality for each node of G Communicability centrality , also called subgraph centrality , of a node ` n ` is the sum of closed walks of all lengths starting and ending at node ` n ` . Parameters -- -- -- -- -- G : graph Returns -- -- -- - nodes : dictionary Dictionary of nodes with communicability centrality as the value . Raises -- -- -- NetworkXError If the graph is not undirected and simple . See Also -- -- -- -- communicability : Communicability between all pairs of nodes in G. communicability_centrality : Communicability centrality for each node of G. Notes -- -- - This version of the algorithm exponentiates the adjacency matrix . The communicability centrality of a node ` u ` in G can be found using the matrix exponential of the adjacency matrix of G [ 1 ] _ [ 2 ] _ , .. math : : SC ( u ) = ( e^A ) _ { uu } . References -- -- -- -- -- .. [ 1 ] Ernesto Estrada , Juan A. Rodriguez-Velazquez , `` Subgraph centrality in complex networks '' , Physical Review E 71 , 056103 ( 2005 ) . http : arxiv.org abs cond-mat 0504730 .. [ 2 ] Ernesto Estrada , Naomichi Hatano , `` Communicability in complex networks '' , Phys . Rev . E 77 , 036111 ( 2008 ) . http : arxiv.org abs 0707.0756 Examples -- -- -- -- > > > G = nx.Graph ( [ ( 0,1 ) , ( 1,2 ) , ( 1,5 ) , ( 5,4 ) , ( 2,4 ) , ( 2,3 ) , ( 4,3 ) , ( 3,6 ) ] ) > > > sc = nx.communicability_centrality_exp ( G ) `` '' '' ''",https://github.com/dkratzert/DSR/blob/efbb09e1a37268f98b9044ebed880a150c2fa0c8/networkx/algorithms/centrality/communicability_alg.py#L26,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` === -- -- -- RegAllocPBQP.cpp -- -- PBQP Register Allocator -- -- -- - - C++ - -=== The LLVM Compiler Infrastructure This file is distributed under the University of Illinois Open Source License . See LICENSE.TXT for details . === -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- === This file contains a Partitioned Boolean Quadratic Programming ( PBQP ) based register allocator for LLVM . This allocator works by constructing a PBQP problem representing the register allocation problem under consideration , solving this using a PBQP solver , and mapping the solution back to a register assignment . If any variables are selected for spilling then spill code is inserted and the process repeated . The PBQP solver ( pbqp.c ) provided for this allocator uses a heuristic tuned for register allocation . For more information on PBQP for register allocation , see the following papers : ( 1 ) Hames , L. and Scholz , B . 2006 . Nearly optimal register allocation with PBQP . In Proceedings of the 7th Joint Modular Languages Conference ( JMLC'06 ) . LNCS , vol . 4228 . Springer , New York , NY , USA . 346-361 . ( 2 ) Scholz , B. , Eckstein , E. 2002 . Register allocation for irregular architectures . In Proceedings of the Joint Conference on Languages , Compilers and Tools for Embedded Systems ( LCTES'02 ) , ACM Press , New York , NY , USA , 139-148 . === -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- === ``",https://github.com/pfsense/FreeBSD-src/blob/a595f803281ea3b25079c9d04a3f5b9a3f0b8a52/contrib/llvm/lib/CodeGen/RegAllocPBQP.cpp#L1,C,yes,conference,algorithm,file,reference,networks and os,formulas to source code,no
"`` Algorithm 1 , page 12 , Nakos , G. C. , Turner , P. R. , Williams , R. M. ( 1997 ) . Fraction-free algorithms for linear and polynomial equations . ACM SIGSAM Bulletin , 31 ( 3 ) , 11‚Äì19 . doi:10.1145 271130.271133 . ''",https://github.com/symengine/symengine/blob/b3a2159b7e6950726157626069c30712427d55b2/symengine/dense_matrix.cpp#L560,C++,yes,journal,algorithm,method,reference,science,pseudocode to source code,no
"`` Class for boosting a classifier using Freund & Schapire 's Adaboost M1 method . For more information , see < p > Yoav Freund and Robert E. Schapire ( 1996 ) . < i > Experiments with a new boosting algorithm < i > . Proc International Conference on Machine Learning , pages 148-156 , Morgan Kaufmann , San Francisco. < p > Valid options are : < p > -D < br > Turn on debugging output. < p > -W classname < br > Specify the full class name of a classifier as the basis for boosting ( required ) . < p > -I num < br > Set the number of boost iterations ( default 10 ) . < p > -P num < br > Set the percentage of weight mass used to build classifiers ( default 100 ) . < p > -Q < br > Use resampling instead of reweighting. < p > -S seed < br > Random number seed for resampling ( default 1 ) . < p > Options after -- are passed to the designated classifier. < p > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Len Trigg ( trigg @ cs.waikato.ac.nz ) @ version $ Revision : 1.17 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/preRemovalDistributionClassifier/weka/classifiers/meta/AdaBoostM1.java#L34,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` < -- globalinfo-start -- > Classifier for building 'logistic model trees ' , which are classification trees with logistic regression functions at the leaves . The algorithm can deal with binary and multi-class target variables , numeric and nominal attributes and missing values. < br > < br > For more information see : < br > < br > Niels Landwehr , Mark Hall , Eibe Frank ( 2005 ) . Logistic Model Trees. < br > < br > Marc Sumner , Eibe Frank , Mark Hall : Speeding up Logistic Model Tree Induction . In : 9th European Conference on Principles and Practice of Knowledge Discovery in Databases , 675-683 , 2005 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Landwehr2005 , author = { Niels Landwehr and Mark Hall and Eibe Frank } , booktitle = { Machine Learning } , number = { 1-2 } , pages = { 161-205 } , title = { Logistic Model Trees } , volume = { 95 } , year = { 2005 } } & 64 ; inproceedings { Sumner2005 , author = { Marc Sumner and Eibe Frank and Mark Hall } , booktitle = { 9th European Conference on Principles and Practice of Knowledge Discovery in Databases } , pages = { 675-683 } , publisher = { Springer } , title = { Speeding up Logistic Model Tree Induction } , year = { 2005 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -B Binary splits ( convert nominal attributes to binary ones ) < pre > < pre > -R Split on residuals instead of class values < pre > < pre > -C Use cross-validation for boosting at all nodes ( i.e. , disable heuristic ) < pre > < pre > -P Use error on probabilities instead of misclassification error for stopping criterion of LogitBoost. < pre > < pre > -I & lt ; numIterations & gt ; Set fixed number of iterations for LogitBoost ( instead of using cross-validation ) < pre > < pre > -M & lt ; numInstances & gt ; Set minimum number of instances at which a node can be split ( default 15 ) < pre > < pre > -W & lt ; beta & gt ; Set beta for weight trimming for LogitBoost . Set to 0 ( default ) for no weight trimming. < pre > < pre > -A The AIC is used to choose the best iteration. < pre > < -- options-end -- > @ author Niels Landwehr @ author Marc Sumner @ version $ Revision : 1.10 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_waikato/weka/classifiers/trees/LMT.java#L50,Java,yes,book,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` @ see J. S. Hicks and R. F. Wheeling ( 1959 ) . An efficient method for generating uniformly distributed points on the surface of an n-dimensional sphere . Communications of the ACM , 2 ( 4 ) , pp . 17-19 . In summary , it works as followed : 1 . Uniformly draw an n -dimensional direction vector ( by normalising a normal distribution vector ) . 2 . Multiply it with a length , uniformly drawn from [ minimal_distance , maximal_distance ] . 3 . Translate its origin by adding parameter . ``",https://github.com/SebastianNiemann/Mantella/blob/abc1342a67e126fb569010e14890c2fb2b014628/include/mantella0_bits/helper/random_neighbour.hpp#L27,C++,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` Class for boosting a classifier using Freund & Schapire 's Adaboost M1 method . For more information , see < p > Yoav Freund and Robert E. Schapire ( 1996 ) . < i > Experiments with a new boosting algorithm < i > . Proc International Conference on Machine Learning , pages 148-156 , Morgan Kaufmann , San Francisco. < p > Valid options are : < p > -D < br > Turn on debugging output. < p > -W classname < br > Specify the full class name of a classifier as the basis for boosting ( required ) . < p > -I num < br > Set the number of boost iterations ( default 10 ) . < p > -P num < br > Set the percentage of weight mass used to build classifiers ( default 100 ) . < p > -Q < br > Use resampling instead of reweighting. < p > -S seed < br > Random number seed for resampling ( default 1 ) . < p > Options after -- are passed to the designated classifier. < p > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Len Trigg ( trigg @ cs.waikato.ac.nz ) @ version $ Revision : 1.25 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-5-0/weka/classifiers/meta/AdaBoostM1.java#L31,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` r '' '' '' Returns the iterated ( ` f ` - ) palindromic closure of self . INPUT : - `` f `` - involution ( default : None ) on the alphabet of self . It must be callable on letters as well as words ( e.g . WordMorphism ) . - `` algorithm `` - string ( default : `` 'recursive ' `` ) specifying which algorithm to be used when computing the iterated palindromic closure . It must be one of the two following values : - `` 'definition ' `` - computed using the definition - `` 'recursive ' `` - computation based on an efficient formula that recursively computes the iterated right palindromic closure without having to recompute the longest ` f ` -palindromic suffix at each iteration [ 2 ] . OUTPUT : word -- the iterated ( ` f ` - ) palindromic closure of self EXAMPLES : : sage : Word ( '123 ' ) .iterated_right_palindromic_closure ( ) word : 1213121 : : sage : w = Word ( 'abc ' ) sage : w.iterated_right_palindromic_closure ( ) word : abacaba : : sage : w = Word ( 'aaa ' ) sage : w.iterated_right_palindromic_closure ( ) word : aaa : : sage : w = Word ( 'abbab ' ) sage : w.iterated_right_palindromic_closure ( ) word : ababaabababaababa A right ` f ` -palindromic closure : : sage : f = WordMorphism ( ' a- > b , b- > a ' ) sage : w = Word ( 'abbab ' ) sage : w.iterated_right_palindromic_closure ( f=f ) word : abbaabbaababbaabbaabbaababbaabbaab An infinite word : : sage : t = words.ThueMorseWord ( 'ab ' ) sage : t.iterated_right_palindromic_closure ( ) word : ababaabababaababaabababaababaabababaabab ... There are two implementations computing the iterated right ` f ` -palindromic closure , the latter being much more efficient : : sage : w = Word ( 'abaab ' ) sage : u = w.iterated_right_palindromic_closure ( algorithm='definition ' ) sage : v = w.iterated_right_palindromic_closure ( algorithm='recursive ' ) sage : u word : abaabaababaabaaba sage : u == v True sage : w = words.RandomWord ( 8 ) sage : u = w.iterated_right_palindromic_closure ( algorithm='definition ' ) sage : v = w.iterated_right_palindromic_closure ( algorithm='recursive ' ) sage : u == v True TESTS : The empty word : : sage : w = Word ( ) sage : w.iterated_right_palindromic_closure ( ) word : The length- ` 1 ` word : : sage : Word ( ' 1 ' ) .iterated_right_palindromic_closure ( ) word : 1 If the word is finite , so is the result : : sage : w = Word ( [ 0,1 ] 7 ) sage : c = w.iterated_right_palindromic_closure ( ) sage : type ( c ) < class 'sage.combinat.words.word.FiniteWord_iter_with_caching ' > REFERENCES : - [ 1 ] A. de Luca , A . De Luca , Pseudopalindrome closure operators in free monoids , Theoret . Comput . Sci . 362 ( 2006 ) 282 -- 300 . - [ 2 ] J. Justin , Episturmian morphisms and a Galois theorem on continued fractions , RAIRO Theoret . Informatics Appl . 39 ( 2005 ) 207-215. `` '' '' ''",https://github.com/sagemath/sage/blob/854f9764d14236110b8d7f7b35a7d52017e044f8/src/sage/combinat/words/abstract_word.py#L1006,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` This technique splits the 8PSK constellation into 2 squashed QPSK constellations , one when I is larger than Q and one where Q is larger than I . The error is then calculated proportionally to these squashed constellations by the const K = sqrt ( 2 ) -1 . The signal magnitude must be > 1 or K will incorrectly bias the error value . Ref : Z. Huang , Z. Yi , M. Zhang , K. Wang , `` 8PSK demodulation for new generation DVB-S2 '' , IEEE Proc . Int . Conf . Communications , Circuits and Systems , Vol . 2 , pp . 1447 - 1450 , 2004. ``",https://github.com/gnuradio/gnuradio/blob/82d0a6bc2d326dfe002cecd2e6c7341cb4a1d021/gr-digital/lib/costas_loop_cc_impl.cc#L96,C++,yes,conference,algorithm,method,reference,other,paper not available,no
"`` TYPE-I Lorentzian , Becker , P. J . & Coppens , P. ( 1974 ) . Acta Cryst . A30 , 129 ; ''",https://github.com/mantidproject/mantid/blob/d54220f0fe7088dfcb96ae6a47684e4df98961e4/Framework/Crystal/src/TOFExtinction.cpp#L223,C++,yes,journal,algorithm,method,reference,science,paper not available,no
"`` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Imported src nmath chebyshev.c from R. Mathlib : A C Library of Special Functions Copyright ( C ) 1998 Ross Ihaka This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation ; either version 2 of the License , or ( at your option ) any later version . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . You should have received a copy of the GNU General Public License along with this program ; if not , write to the Free Software Foundation , Inc. , 51 Franklin St , Fifth Floor , Boston , MA 02110-1301 USA . SYNOPSIS int chebyshev_init ( double dos , int nos , double eta ) double chebyshev_eval ( double x , double a , int n ) DESCRIPTION `` chebyshev_init '' determines the number of terms for the double precision orthogonal series `` dos '' needed to insure the error is no larger than `` eta '' . Ordinarily eta will be chosen to be one-tenth machine precision . `` chebyshev_eval '' evaluates the n-term Chebyshev series `` a '' at `` x '' . NOTES These routines are translations into C of Fortran routines by W. Fullerton of Los Alamos Scientific Laboratory . Based on the Fortran routine dcsevl by W. Fullerton . Adapted from R. Broucke , Algorithm 446 , CACM. , 16 , 254 ( 1973 ) . ``",https://github.com/GNOME/gnumeric/blob/dce0ef0eabd634044ed504a75fe5ec02edae66f4/src/sf-gamma.c#L195,C,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` A first-order , semi-explicit Euler integrator . State is updated in the following manner : < pre > v ( t‚ÇÄ+h ) = v ( t‚ÇÄ ) + dv dt ( t‚ÇÄ ) h dq dt = N v ( t‚ÇÄ+h ) q ( t‚ÇÄ+h ) = q ( t‚ÇÄ ) + dq dt h < pre > where ` v ` are the generalized velocity variables and ` q ` are generalized coordinates . ` h ` is the integration step size , and ` N ` is a matrix ( dependent upon ` q ( t‚ÇÄ ) ` ) that maps velocities to time derivatives of generalized coordinates . For rigid body systems in 2D , for example , ` N ` will generally be an identity matrix . For a single rigid body in 3D , ` N ` and its pseudo-inverse ( ` N ` is generally non-square but always left invertible ) are frequently used to transform between time derivatives of Euler parameters ( unit quaternions ) and angular velocities ( and vice versa ) , [ Nikravesh 1988 ] . Note that these equations imply that the velocity variables are updated first and that these new velocities are then used to update the generalized coordinates ( compare to ExplicitEulerIntegrator , where the generalized coordinates are updated using the previous velocity variables ) . When a mechanical system is Hamiltonian ( informally meaning that the system is not subject to velocity-dependent forces ) , the semi-explicit Euler integrator is a symplectic ( energy conserving ) integrator . Symplectic integrators advertise energetically consistent behavior with large step sizes compared to non-symplectic integrators . Multi-body systems are not Hamiltonian , even in the absence of externally applied velocity-dependent forces , due to the presence of both Coriolis and gyroscopic forces . This integrator thus does not generally conserve energy for such systems . < h4 > Association between time stepping and the semi-explicit Euler integrator : < h4 > Though many time stepping approaches use the formulations above , these equations do not represent a `` time stepping scheme '' . The semi-explicit Euler integration equations can be applied from one point in state space to another , assuming smoothness in between , just like any other integrator using the following process : ( 1 ) a simulator integrates to discontinuities , ( 2 ) the state of the ODE DAE is re-initialized , and ( 3 ) integration continues . In contrast , time stepping schemes enforce all constraints at a single time in the integration process : though a billiard break may consist of tens of collisions occurring sequentially over a millisecond of time , a time stepping method will treat all of these collisions as occurring simultaneously . - [ Nikravesh 1988 ] P. Nikravesh . Computer-Aided Analysis of Mechanical Systems . Prentice Hall . New Jersey , 1988 . - [ Stewart 2000 ] D. Stewart . Rigid-body Dynamics with Friction and Impact . SIAM Review , 42:1 , 2000. ``",https://github.com/RobotLocomotion/drake/blob/5686d08b5e08f864540991863f1a1f8ca0be73d0/systems/analysis/semi_explicit_euler_integrator.h#L10,C++,yes,book,algorithm,class,reference,other,paper not available,no
"`` Behaves the same as PairedTTester , only it uses the corrected resampled t-test statistic. < p > For more information see : < p > < -- technical-plaintext-start -- > Claude Nadeau , Yoshua Bengio ( 2001 ) . Inference for the Generalization Error . Machine Learning.. < -- technical-plaintext-end -- > < p > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Nadeau2001 , author = { Claude Nadeau and Yoshua Bengio } , journal = { Machine Learning } , title = { Inference for the Generalization Error } , year = { 2001 } , PDF = { http : www.iro.umontreal.ca ~lisa bib pub_subject comparative pointeurs nadeau_MLJ1597.pdf } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D & lt ; index , index2-index4 , ... & gt ; Specify list of columns that specify a unique dataset . First and last are valid indexes . ( default none ) < pre > < pre > -R & lt ; index & gt ; Set the index of the column containing the run number < pre > < pre > -F & lt ; index & gt ; Set the index of the column containing the fold number < pre > < pre > -G & lt ; index1 , index2-index4 , ... & gt ; Specify list of columns that specify a unique 'result generator ' ( eg : classifier name and options ) . First and last are valid indexes . ( default none ) < pre > < pre > -S & lt ; significance level & gt ; Set the significance level for comparisons ( default 0.05 ) < pre > < pre > -V Show standard deviations < pre > < pre > -L Produce table comparisons in Latex table format < pre > < pre > -csv Produce table comparisons in CSV table format < pre > < pre > -html Produce table comparisons in HTML table format < pre > < pre > -significance Produce table comparisons with only the significance values < pre > < pre > -gnuplot Produce table comparisons output suitable for GNUPlot < pre > < -- options-end -- > @ author Richard Kirkby ( rkirkby @ cs.waikato.ac.nz ) @ version $ Revision : 1.13 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/weka-stable-3.6.8/src/main/java/weka/experiment/PairedCorrectedTTester.java#L42,Java,yes,journal,background,n/a,related,machine learning,no transfer,no
"`` Here is the basic algorithm : First , some design criteria I used : - I think a false hit is more serious than a false miss - A false hit for an RPC that has Op ( s ) that order via seqid must be avoided at all cost - A valid hit will probably happen a long time after the original reply and the TCP socket that the original request was received on will no longer be active ( The long time delay implies to me that LRU is not appropriate . ) - The mechanism will satisfy the requirements of ordering Ops with seqid s in them as well as minimizing the risk of redoing retried non-idempotent Ops . Because it is biased towards avoiding false hits , multiple entries with the same xid are to be expected , especially for the case of the entry in the cache being related to a seqid sequenced Op . The basic algorithm I 'm about to code up : - Null RPCs bypass the cache and are just done For TCP - key on < xid , NFS version > ( as noted above , there can be several entries with the same key ) When a request arrives : For all that match key - if RPC = OR request_size = - not a match with this one - if NFSv4 and received on same TCP socket OR received on a TCP connection created before the entry was cached - not a match with this one ( V2,3 clients might retry on same TCP socket ) - calculate checksum on first N bytes of NFS XDR - if checksum = - not a match for this one If any of the remaining ones that match has a seqid_refcnt > 0 - not a match ( go do RPC , using new cache entry ) If one match left - a hit ( reply from cache ) else - miss ( go do RPC , using new cache entry ) During processing of NFSv4 request : - set a flag when a non-idempotent Op is processed - when an Op that uses a seqid ( Open , ... ) is processed - if same seqid as referenced entry in cache - free new cache entry - reply from referenced cache entry else if next seqid in order - free referenced cache entry - increment seqid_refcnt on new cache entry - set pointer from Openowner Lockowner to new cache entry ( aka reference it ) else if first seqid in sequence - increment seqid_refcnt on new cache entry - set pointer from Openowner Lockowner to new cache entry ( aka reference it ) At end of RPC processing : - if seqid_refcnt > 0 OR flagged non-idempotent on new cache entry - save reply in cache entry - calculate checksum on first N bytes of NFS XDR request - note op and length of XDR request ( in bytes ) - timestamp it else - free new cache entry - Send reply ( noting info for socket activity check , below ) For cache entries saved above : - if saved since seqid_refcnt was > 0 - free when seqid_refcnt decrements to 0 ( when next one in sequence is processed above , or when Openowner Lockowner is discarded ) else { non-idempotent Op ( s ) } - free when - some further activity observed on same socket ( I 'm not yet sure how I 'm going to do this . Maybe look at the TCP connection to see if the send_tcp_sequence is well past sent reply OR K additional RPCs replied on same socket OR ? ) OR - when very old ( hours , days , weeks ? ) For UDP ( v2 , 3 only ) , pretty much the old way : - key on < xid , NFS version , RPC , Client host ip > ( at most one entry for each key ) When a Request arrives : - if a match with entry via key - if RPC marked In_progress - discard request ( do n't send reply ) else - reply from cache - timestamp cache entry else - add entry to cache , marked In_progress - do RPC - when RPC done - if RPC non-idempotent - mark entry Done ( not In_progress ) - save reply - timestamp cache entry else - free cache entry - send reply Later , entries with saved replies are free 'd a short time ( few minutes ) after reply sent ( timestamp ) . Reference : Chet Juszczak , `` Improving the Performance and Correctness of an NFS Server '' , in Proc . Winter 1989 USENIX Conference , pages 53-63 . San Diego , February 1989. for the UDP case . nfsrc_floodlevel is set to the allowable upper limit for saved replies for TCP . For V3 , a reply wo n't be saved when the flood level is hit . For V4 , the non-idempotent Op will return NFSERR_RESOURCE in that case . This level should be set high enough that this almost never happens. ``",https://github.com/HardenedBSD/hardenedBSD-playground/blob/8e66564ae0c65b1af287b10eab55bed6f771c8e9/sys/fs/nfsserver/nfs_nfsdcache.c#L39,C,yes,conference,algorithm,file,reference,networks and os,paper not available,no
"`` < -- globalinfo-start -- > Locally weighted learning . Uses an instance-based algorithm to assign instance weights which are then used by a specified WeightedInstancesHandler. < br > Can do classification ( e.g . using naive Bayes ) or regression ( e.g . using linear regression ) . < br > < br > For more info , see < br > < br > Eibe Frank , Mark Hall , Bernhard Pfahringer : Locally Weighted Naive Bayes . In : 19th Conference in Uncertainty in Artificial Intelligence , 249-256 , 2003. < br > < br > C. Atkeson , A. Moore , S. Schaal ( 1996 ) . Locally weighted learning . AI Review.. < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Frank2003 , author = { Eibe Frank and Mark Hall and Bernhard Pfahringer } , booktitle = { 19th Conference in Uncertainty in Artificial Intelligence } , pages = { 249-256 } , publisher = { Morgan Kaufmann } , title = { Locally Weighted Naive Bayes } , year = { 2003 } } & 64 ; article { Atkeson1996 , author = { C. Atkeson and A. Moore and S. Schaal } , journal = { AI Review } , title = { Locally weighted learning } , year = { 1996 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -A The nearest neighbour search algorithm to use ( default : LinearNN ) . < pre > < pre > -K & lt ; number of neighbours & gt ; Set the number of neighbours used to set the kernel bandwidth . ( default all ) < pre > < pre > -U & lt ; number of weighting method & gt ; Set the weighting kernel shape to use . 0=Linear , 1=Epanechnikov , 2=Tricube , 3=Inverse , 4=Gaussian . ( default 0 = Linear ) < pre > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < pre > -W Full name of base classifier . ( default : weka.classifiers.trees.DecisionStump ) < pre > < pre > Options specific to classifier weka.classifiers.trees.DecisionStump : < pre > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < -- options-end -- > @ author Len Trigg ( trigg @ cs.waikato.ac.nz ) @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Ashraf M. Kibriya ( amk14 @ waikato.ac.nz ) @ version $ Revision : 1.20 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_Instances_IO_changes/weka/classifiers/lazy/LWL.java#L45,Java,yes,conference,algorithm,class,reference,machine learning,formulas to source code,no
"`` `` '' '' New uphill tangent formulation The method was described in `` Improved tangent estimate in the nudged elastic band method for finding minimum energy paths and saddle points '' Graeme Henkelman and Hannes Jonsson J. Chem . Phys 113 ( 22 ) , 9978 ( 2000 ) Parameters -- -- -- -- -- central : float central image energy left : float left image energy right : float right image energy gleft : np.array gradient to left image ( x_0 - x_left ) gright : np.array gradient to right image ( x_0 - x_right ) `` '' '' ''",https://github.com/pele-python/pele/blob/d89d7c0041358050da918e213ed30ec5552bd822/pele/transition_states/_NEB.py#L287,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` < -- globalinfo-start -- > Class implementing the predictive apriori algorithm to mine association rules. < br > It searches with an increasing support threshold for the best ' n ' rules concerning a support-based corrected confidence value. < br > < br > For more information see : < br > < br > Tobias Scheffer : Finding Association Rules That Trade Support Optimally against Confidence . In : 5th European Conference on Principles of Data Mining and Knowledge Discovery , 424-435 , 2001. < br > < br > The implementation follows the paper expect for adding a rule to the output of the ' n ' best rules . A rule is added if : < br > the expected predictive accuracy of this rule is among the ' n ' best and it is not subsumed by a rule with at least the same expected predictive accuracy ( out of an unpublished manuscript from T. Scheffer ) . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Scheffer2001 , author = { Tobias Scheffer } , booktitle = { 5th European Conference on Principles of Data Mining and Knowledge Discovery } , pages = { 424-435 } , publisher = { Springer } , title = { Finding Association Rules That Trade Support Optimally against Confidence } , year = { 2001 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -N & lt ; required number of rules output & gt ; The required number of rules . ( default = 100 ) < pre > < pre > -A If set class association rules are mined . ( default = no ) < pre > < pre > -c & lt ; the class index & gt ; The class index . ( default = last ) < pre > < -- options-end -- > @ author Stefan Mutter ( mutter @ cs.waikato.ac.nz ) @ version $ Revision : 1.12 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_revisionhandler/weka/associations/PredictiveApriori.java#L42,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` < -- globalinfo-start -- > This meta classifier creates a number of disjoint , stratified folds out of the data and feeds each chunk of data to a copy of the supplied base classifier . Predictions are made via majority vote , since all the generated base classifiers are put into the Vote meta classifier . < br > Useful for base classifiers that are quadratic or worse in time behavior , regarding number of instances in the training data . < br > < br > For more information , see : < br > Ting , K. M. , Witten , I. H. : Stacking Bagged and Dagged Models . In : Fourteenth international Conference on Machine Learning , San Francisco , CA , 367-375 , 1997 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Ting1997 , address = { San Francisco , CA } , author = { Ting , K. M. and Witten , I. H. } , booktitle = { Fourteenth international Conference on Machine Learning } , editor = { D. H. Fisher } , pages = { 367-375 } , publisher = { Morgan Kaufmann Publishers } , title = { Stacking Bagged and Dagged Models } , year = { 1997 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -F & lt ; folds & gt ; The number of folds for splitting the training set into smaller chunks for the base classifier . ( default 10 ) < pre > < pre > -verbose Whether to print some more information during building the classifier . ( default is off ) < pre > < pre > -S & lt ; num & gt ; Random number seed . ( default 1 ) < pre > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < pre > -W Full name of base classifier . ( default : weka.classifiers.trees.DecisionStump ) < pre > < -- options-end -- > Options after -- are passed to the designated classifier . < p > @ author Bernhard Pfahringer ( bernhard at cs dot waikato dot ac dot nz ) @ author FracPete ( fracpete at waikato dot ac dot nz ) @ version $ Revision $ @ see Vote ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-7-12/packages/internal/dagging/src/main/java/weka/classifiers/meta/Dagging.java#L40,Java,yes,conference,background,n/a,related,machine learning,no transfer,no
"`` addtogroup layout @ { file brief A layout algorithm to resolve all node overlaps . An implementation of a layout algorithm resolving the problem of node overlaps . It was first published as : Tim Dwyer , Kim Marriot , Peter J. Stuckey , `` Fast Node Overlap Removal '' in Graph Drawing , Vol . 3843 ( 2006 ) , pp . 153-164 . < b > NOTES < b > The algorithm works on general graphs . It finds and applies the minimum scale factor to ensure that no two pairs of nodes overlap in the graph . Let n be the number of nodes , the algorithm complexity is in O ( n ) . If two nodes start out at the same postion the algorithm can not succeed . It will return failure . < b > HISTORY < b > - 2006 Version 1.0 : first version by Daniel Archambault , Department of Computer Science , University of British Columbia < b > LICENCE < b > This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation ; either version 2 of the License , or ( at your option ) any later version. ``",https://github.com/anlambert/tulip/blob/a3ecb5669be1a8b79004b1b12e42e9f3b76861de/plugins/layout/FastOverlapRemoval/FastOverlapRemoval.h#L5,C++,yes,book,background,n/a,related,other,no transfer,no
"`` `` '' '' Returns MacKinnon 's approximate p-value for test stat . Parameters -- -- -- -- -- stat : float `` T-value '' from an Augmented Dickey-Fuller or DFGLS regression . regression : { ' c ' , 'nc ' , 'ct ' , 'ctt ' } This is the method of regression that was used . Following MacKinnon 's notation , this can be `` c '' for constant , `` nc '' for no constant , `` ct '' for constant and trend , and `` ctt '' for constant , trend , and trend-squared . num_unit_roots : int The number of series believed to be I ( 1 ) . For ( Augmented ) Dickey- Fuller N = 1. dist_type : { 'ADF-t ' , 'ADF-z ' , 'DFGLS ' } The test type to use when computing p-values . Options include 'ADF-t ' - ADF t-stat based bootstrap 'ADF-z ' - ADF z bootstrap 'DFGLS ' - GLS detrended Dickey Fuller Returns -- -- -- - p-value : float The p-value for the ADF statistic estimated using MacKinnon 1994 . References -- -- -- -- -- MacKinnon , J.G . 1994 `` Approximate Asymptotic Distribution Functions for Unit-Root and Cointegration Tests . '' Journal of Business & Economics Statistics , 12.2 , 167-76 . Notes -- -- - Most values are from MacKinnon ( 1994 ) . Values for DFGLS test statistics and the 'nc ' version of the ADF z test statistic were computed following the methodology of MacKinnon ( 1994 ) . `` '' '' ''",https://github.com/bashtage/arch/blob/03cd0ddfdd74b66a1197168df2f965d96823dc6f/arch/unitroot/unitroot.py#L1151,Python,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` Reference M. Matsumoto and T. Nishimura , `` Mersenne Twister : A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator '' , ACM Transactions on Modeling and Computer Simulation , Vol . 8 , No . 1 , January 1998 , pp 3-30 . ''",https://github.com/Bforartists/Bforartists/blob/24b8b8138275f988005a8e99f16a94b5b6d1793e/intern/smoke/intern/MERSENNETWISTER.h#L16,C,yes,journal,algorithm,file,reference,computer vision,pseudocode to source code,no
"`` Class for manipulating chi-square mixture distributions . < p > For more information see : < p > < -- technical-plaintext-start -- > Wang , Y ( 2000 ) . A new approach to fitting linear models in high dimensional spaces . Hamilton , New Zealand. < br > < br > Wang , Y. , Witten , I. H. : Modeling for optimal probability prediction . In : Proceedings of the Nineteenth International Conference in Machine Learning , Sydney , Australia , 650-657 , 2002 . < -- technical-plaintext-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; phdthesis { Wang2000 , address = { Hamilton , New Zealand } , author = { Wang , Y } , school = { Department of Computer Science , University of Waikato } , title = { A new approach to fitting linear models in high dimensional spaces } , year = { 2000 } } & 64 ; inproceedings { Wang2002 , address = { Sydney , Australia } , author = { Wang , Y. and Witten , I. H. } , booktitle = { Proceedings of the Nineteenth International Conference in Machine Learning } , pages = { 650-657 } , title = { Modeling for optimal probability prediction } , year = { 2002 } } < pre > < p > < -- technical-bibtex-end -- > @ author Yong Wang ( yongwang @ cs.waikato.ac.nz ) @ version $ Revision : 1.4 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_classdiscovery_cleanup/weka/classifiers/functions/pace/ChisqMixture.java#L29,Java,yes,other (thesis),algorithm,class,reference,machine learning,formulas to source code,no
"`` Generates a basis matrix of size projectedVectorSize x vectorSize . Multiplying a a vector by this matrix results in the projected vector . The rows of a matrix are sample from a distribution where : - +1 has probability 1 2 , - -1 has probability 1 2 See Achlioptas , D. ( 2003 ) . Database-friendly random projections : Johnson-Lindenstrauss with binary coins . Journal of Computer and System Sciences , 66 ( 4 ) , 671‚Äì687 . doi:10.1016 S0022-0000 ( 03 ) 00025-4 @ param projectedVectorSize final projected size of a vector ( number of projection vectors ) @ param vectorSize initial vector size @ return a projection matrix ``",https://github.com/apache/mahout/blob/7dff35bc3c61c3e0b95e8e59406742be15203a69/mr/src/main/java/org/apache/mahout/math/random/RandomProjector.java#L54,Java,yes,journal,algorithm,method,reference,machine learning,formulas to source code,no
"`` Reference : James Demmel and Yozo Hida , Fast and accurate floating-point summation with application to computational geometry , Numerical Algorithms , volume 37 , number 1-4 , pages 101 -- 112 , 2004. ``",https://github.com/DragonFlyBSD/DragonFlyBSD/blob/ce2989fe1212f664d615268edc64a57801fc7404/contrib/mpfr/src/sum.c#L23,C,yes,journal,algorithm,file,reference,networks and os,pseudocode to source code,no
"`` `` '' '' Minimize a function using the downhill simplex algorithm . This algorithm only uses function values , not derivatives or second derivatives . Parameters -- -- -- -- -- func : callable func ( x , args ) The objective function to be minimized . x0 : ndarray Initial guess . args : tuple , optional Extra arguments passed to func , i.e . `` f ( x , args ) `` . xtol : float , optional Absolute error in xopt between iterations that is acceptable for convergence . ftol : number , optional Absolute error in func ( xopt ) between iterations that is acceptable for convergence . maxiter : int , optional Maximum number of iterations to perform . maxfun : number , optional Maximum number of function evaluations to make . full_output : bool , optional Set to True if fopt and warnflag outputs are desired . disp : bool , optional Set to True to print convergence messages . retall : bool , optional Set to True to return list of solutions at each iteration . callback : callable , optional Called after each iteration , as callback ( xk ) , where xk is the current parameter vector . initial_simplex : array_like of shape ( N + 1 , N ) , optional Initial simplex . If given , overrides ` x0 ` . `` initial_simplex [ j , : ] `` should contain the coordinates of the j-th vertex of the `` N+1 `` vertices in the simplex , where `` N `` is the dimension . Returns -- -- -- - xopt : ndarray Parameter that minimizes function . fopt : float Value of function at minimum : `` fopt = func ( xopt ) `` . iter : int Number of iterations performed . funcalls : int Number of function calls made . warnflag : int 1 : Maximum number of function evaluations made . 2 : Maximum number of iterations reached . allvecs : list Solution at each iteration . See also -- -- -- -- minimize : Interface to minimization algorithms for multivariate functions . See the 'Nelder-Mead ' ` method ` in particular . Notes -- -- - Uses a Nelder-Mead simplex algorithm to find the minimum of function of one or more variables . This algorithm has a long history of successful use in applications . But it will usually be slower than an algorithm that uses first or second derivative information . In practice it can have poor performance in high-dimensional problems and is not robust to minimizing complicated functions . Additionally , there currently is no complete theory describing when the algorithm will successfully converge to the minimum , or how fast it will if it does . Both the ftol and xtol criteria must be met for convergence . References -- -- -- -- -- .. [ 1 ] Nelder , J.A . and Mead , R. ( 1965 ) , `` A simplex method for function minimization '' , The Computer Journal , 7 , pp . 308-313 .. [ 2 ] Wright , M.H . ( 1996 ) , `` Direct Search Methods : Once Scorned , Now Respectable '' , in Numerical Analysis 1995 , Proceedings of the 1995 Dundee Biennial Conference in Numerical Analysis , D.F . Griffiths and G.A . Watson ( Eds . ) , Addison Wesley Longman , Harlow , UK , pp . 191-208. `` '' '' ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/scipy/optimize/optimize.py#L299,Python,yes,journal,background,n/a,related,other,no transfer,no
"`` r '' '' '' Small primes of degree oneIterator for finding several primes of absolute degree one of a number field of small prime norm. -- -- -- Algorithm : Let ` P ` denote the product of some set of prime numbers . ( In practice , weuse the product of the first 10000 primes , because Pari computes this many bydefault . ) Let ` K ` be a number field and let ` f ( x ) ` be a polynomial defining ` K ` over therational field . Let ` alpha ` be a root of ` f ` in ` K ` .We know that ` [ O_K : ZZ [ alpha ] ] ^2 = | Delta ( f ( x ) ) Delta ( O_K ) | ` , where ` Delta ` denotes the discriminant ( see , for example , Proposition 4.4.4 , p165 of [ C ] _ ) . Therefore , after discarding primes dividing ` Delta ( f ( x ) ) ` ( thisincludes all ramified primes ) , any integer ` n ` such that ` gcd ( f ( n ) , P ) > 0 ` yields a prime ` p | P ` such that ` f ( x ) ` has a root modulo ` p ` . By thecondition on discriminants , this root is a single root . As is well known ( see , for example Theorem 4.8.13 , p199 of [ C ] _ ) , the ideal generated by ` ( p , alpha -n ) ` is prime and of degree one ... [ C ] H. Cohen . A Course in Computational Algebraic Number Theory . Springer-Verlag , 1993 ... warning : : It is possible that there are no primes of ` K ` of absolute degree one of small prime norm , and it is possible that this algorithm will not find any primes of small norm. -- -- -- To do : There are situations when this will fail . There are questions of findingprimes of relative degree one . There are questions of finding primes of exactdegree larger than one . In short , if you can contribute , please do -- -- -- -- EXAMPLES : : sage : x = ZZ [ ' x ' ] .gen ( ) sage : F. < a > = NumberField ( x^2 - 2 ) sage : Ps = F.primes_of_degree_one_list ( 3 ) sage : Ps random [ Fractional ideal ( 2 a + 1 ) , Fractional ideal ( -3 a + 1 ) , Fractional ideal ( -a + 5 ) ] sage : [ P.norm ( ) for P in Ps ] random [ 7 , 17 , 23 ] sage : all ( ZZ ( P.norm ( ) ) .is_prime ( ) for P in Ps ) True sage : all ( P.residue_class_degree ( ) == 1 for P in Ps ) TrueThe next two examples are for relative number fields . : : sage : L. < b > = F.extension ( x^3 - a ) sage : Ps = L.primes_of_degree_one_list ( 3 ) sage : Ps random [ Fractional ideal ( 17 , b - 5 ) , Fractional ideal ( 23 , b - 4 ) , Fractional ideal ( 31 , b - 2 ) ] sage : [ P.absolute_norm ( ) for P in Ps ] random [ 17 , 23 , 31 ] sage : all ( ZZ ( P.absolute_norm ( ) ) .is_prime ( ) for P in Ps ) True sage : all ( P.residue_class_degree ( ) == 1 for P in Ps ) True sage : M. < c > = NumberField ( x^2 - x b^2 + b ) sage : Ps = M.primes_of_degree_one_list ( 3 ) sage : Ps random [ Fractional ideal ( 17 , c - 2 ) , Fractional ideal ( c - 1 ) , Fractional ideal ( 41 , c + 15 ) ] sage : [ P.absolute_norm ( ) for P in Ps ] random [ 17 , 31 , 41 ] sage : all ( ZZ ( P.absolute_norm ( ) ) .is_prime ( ) for P in Ps ) True sage : all ( P.residue_class_degree ( ) == 1 for P in Ps ) TrueAUTHORS : - Nick Alexander ( 2008 ) - David Loeffler ( 2009 ) : fixed a bug with relative fields- Maarten Derickx ( 2017 ) : fixed a bug with number fields not generated by an integral element '' '' '' ''",https://github.com/sagemath/sage/blob/854f9764d14236110b8d7f7b35a7d52017e044f8/src/sage/rings/number_field/small_primes_of_degree_one.py#L1,Python,yes,book,background,n/a,related,science,no transfer,no
"`` `` '' '' Contributed code for the stochastic PDHG . [ CERS2017 ] A. Chambolle , M. J. Ehrhardt , P. Richtarik and C.-B . Schoenlieb , Stochastic Primal-Dual Hybrid Gradient Algorithm with Arbitrary Samplingand Imaging Applications . ArXiv : http : arxiv.org abs 1706.04957 ( 2017 ) . [ E+2017 ] M. J. Ehrhardt , P. J. Markiewicz , P. Richtarik , J. Schott , A . Chambolle and C.-B . Schoenlieb , Faster PET reconstruction with astochastic primal-dual hybrid gradient method . Wavelets and Sparsity XVII,58 ( 2017 ) http : doi.org 10.1117 12.2272946 . '' '' '' ''",https://github.com/odlgroup/odl/blob/094b5542a45ec2bc369c176ca254b320b66f3689/odl/contrib/solvers/spdhg/__init__.py#L9,Python,yes,journal,background,n/a,related,science,no transfer,no
"`` This class implements the WELL512a pseudo-random number generator from Fran & ccedil ; ois Panneton , Pierre L'Ecuyer and Makoto Matsumoto . < p > This generator is described in a paper by Fran & ccedil ; ois Panneton , Pierre L'Ecuyer and Makoto Matsumoto < a href= '' http : www.iro.umontreal.ca ~lecuyer myftp papers wellrng.pdf '' > Improved Long-Period Generators Based on Linear Recurrences Modulo 2 < a > ACM Transactions on Mathematical Software , 32 , 1 ( 2006 ) . The errata for the paper are in < a href= '' http : www.iro.umontreal.ca ~lecuyer myftp papers wellrng-errata.txt '' > wellrng-errata.txt < a > . < p > @ see < a href= '' http : www.iro.umontreal.ca ~panneton WELLRNG.html '' > WELL Random number generator < a > @ since 2.2 ``",https://github.com/happyjack27/autoredistrict/blob/2ed723ff9189753c72da2f380f56322d0df46a20/src/org/apache/commons/math3/random/Well512a.java#L20,Java,yes,journal,algorithm,class,reference,other,pseudocode to source code,no
"`` Differential Evolution configuration object The algorithm and strategy names are taken from here : Price , K. , Storn , R. , 1997 . Differential Evolution - A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces . Journal of Global Optimization , Kluwer Academic Publishers , 1997 , Vol . 11 , pp . 341 - 359 . There are seven basic strategies for creating mutant population currently implemented . Three basic crossover types are also available . Future development : 1 ) base element type to be extracted 2 ) L differences to be used instead of fixed number 3 ) various weights distributions for the differences ( dither etc . ) 4 ) printFullInfo parameter usage to track the algorithm warning This was reported to fail tests on Mac OS X 10.8.4. ``",https://github.com/lballabio/QuantLib/blob/2c2dd00a3ecd71ae7c7607e025bba159d8687637/ql/math/optimization/differentialevolution.hpp#L34,C++,yes,journal,algorithm,class,reference,other,no transfer,no
"`` `` '' '' Active contour model . Active contours by fitting snakes to features of images . Supports single and multichannel 2D images . Snakes can be periodic ( for segmentation ) or have fixed and or free ends . The output snake has the same length as the input boundary . As the number of points is constant , make sure that the initial snake has enough points to capture the details of the final contour . Parameters -- -- -- -- -- image : ( N , M ) or ( N , M , 3 ) ndarray Input image . snake : ( N , 2 ) ndarray Initialisation coordinates of snake . For periodic snakes , it should not include duplicate endpoints . alpha : float , optional Snake length shape parameter . Higher values makes snake contract faster . beta : float , optional Snake smoothness shape parameter . Higher values makes snake smoother . w_line : float , optional Controls attraction to brightness . Use negative values to attract to dark regions . w_edge : float , optional Controls attraction to edges . Use negative values to repel snake from edges . gamma : float , optional Explicit time stepping parameter . bc : { 'periodic ' , 'free ' , 'fixed ' } , optional Boundary conditions for worm . 'periodic ' attaches the two ends of the snake , 'fixed ' holds the end-points in place , and'free ' allows free movement of the ends . 'fixed ' and 'free ' can be combined by parsing 'fixed-free ' , 'free-fixed ' . Parsing 'fixed-fixed ' or 'free-free ' yields same behaviour as 'fixed ' and 'free ' , respectively . max_px_move : float , optional Maximum pixel distance to move per iteration . max_iterations : int , optional Maximum iterations to optimize snake shape . convergence : float , optional Convergence criteria . Returns -- -- -- - snake : ( N , 2 ) ndarray Optimised snake , same shape as input parameter . References -- -- -- -- -- .. [ 1 ] Kass , M. ; Witkin , A. ; Terzopoulos , D. `` Snakes : Active contour models '' . International Journal of Computer Vision 1 ( 4 ) : 321 ( 1988 ) . Examples -- -- -- -- > > > from skimage.draw import circle_perimeter > > > from skimage.filters import gaussian Create and smooth image : > > > img = np.zeros ( ( 100 , 100 ) ) > > > rr , cc = circle_perimeter ( 35 , 45 , 25 ) > > > img [ rr , cc ] = 1 > > > img = gaussian ( img , 2 ) Initiliaze spline : > > > s = np.linspace ( 0 , 2 np.pi,100 ) > > > init = 50 np.array ( [ np.cos ( s ) , np.sin ( s ) ] ) .T+50 Fit spline to image : > > > snake = active_contour ( img , init , w_edge=0 , w_line=1 ) doctest : +SKIP > > > dist = np.sqrt ( ( 45-snake [ : , 0 ] ) 2 + ( 35-snake [ : , 1 ] ) 2 ) doctest : +SKIP > > > int ( np.mean ( dist ) ) doctest : +SKIP 25 `` '' '' ''",https://github.com/scikit-image/scikit-image/blob/51f598aaedc73ef180913c670d2c20a8032aaf1e/skimage/segmentation/active_contour_model.py#L12,Python,yes,journal,algorithm,method,reference,computer vision,formulas to source code,no
"`` An implementation of the Vegas congestion control algorithm for FreeBSD , based on L. S. Brakmo and L. L. Peterson , `` TCP Vegas : end to end congestion avoidance on a global internet '' , IEEE J. Sel . Areas Commun. , vol . 13 , no . 8 , pp . 1465-1480 , Oct. 1995 . The original Vegas duplicate ack policy has not been implemented , since clock ticks are not as coarse as they were ( i.e . 500ms ) when Vegas was designed . Also , packets are timed once per RTT as in the original paper . Originally released as part of the NewTCP research project at Swinburne University of Technology 's Centre for Advanced Internet Architectures , Melbourne , Australia , which was made possible in part by a grant from the Cisco University Research Program Fund at Community Foundation Silicon Valley . More details are available at : http : caia.swin.edu.au urp newtcp ``",https://github.com/freenas/os/blob/0e9eb48d96a1edb197c779609b5a889f5e8306cf/sys/netinet/cc/cc_vegas.c#L39,C,yes,journal,algorithm,file,reference,networks and os,description to source code,no
"`` Class implementing an Apriori-type algorithm . Iteratively reduces the minimum support until it finds the required number of rules with the given minimum confidence . < p > Reference : R. Agrawal , R. Srikant ( 1994 ) . < i > Fast algorithms for mining association rules in large databases < i > . Proc International Conference on Very Large Databases , pp . 478-499 . Santiage , Chile : Morgan Kaufmann , Los Altos , CA . < p > Valid options are : < p > -N required number of rules < br > The required number of rules ( default : 10 ) . < p > -T type of metric by which to sort rules < br > 0 = confidence | 1 = lift | 2 = leverage | 3 = Conviction . < p > -C minimum confidence of a rule < br > The minimum confidence of a rule ( default : 0.9 ) . < p > -D delta for minimum support < br > The delta by which the minimum support is decreased in each iteration ( default : 0.05 ) . < p > -U upper bound for minimum support < br > The upper bound for minimum support . Do n't explicitly look for rules with more than this level of support . < p > -M lower bound for minimum support < br > The lower bound for the minimum support ( default = 0.1 ) . < p > -S significance level < br > If used , rules are tested for significance at the given level . Slower ( default = no significance testing ) . < p > -R < br > If set then columns that contain all missing values are removed from the data . -I < br > If set the itemsets found are also output ( default = no ) . < p > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Mark Hall ( mhall @ cs.waikato.ac.nz ) @ version $ Revision : 1.16 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/stable-3-4-2/weka/associations/Apriori.java#L31,Java,yes,book,algorithm,class,reference,machine learning,paper not available,no
"`` `` '' '' Perform Normalized Graph cut on the Region Adjacency Graph . Given an image 's labels and its similarity RAG , recursively perform a 2-way normalized cut on it . All nodes belonging to a subgraph that can not be cut further are assigned a unique label in the output . Parameters -- -- -- -- -- labels : ndarray The array of labels . rag : RAG The region adjacency graph . thresh : float The threshold . A subgraph wo n't be further subdivided if the value of the N-cut exceeds ` thresh ` . num_cuts : int The number or N-cuts to perform before determining the optimal one . in_place : bool If set , modifies ` rag ` in place . For each node ` n ` the function will set a new attribute `` rag.node [ n ] [ 'ncut label ' ] `` . max_edge : float , optional The maximum possible value of an edge in the RAG . This corresponds to an edge between identical regions . This is used to put self edges in the RAG . Returns -- -- -- - out : ndarray The new labeled array . Examples -- -- -- -- > > > from skimage import data , segmentation > > > from skimage.future import graph > > > img = data.astronaut ( ) > > > labels = segmentation.slic ( img ) > > > rag = graph.rag_mean_color ( img , labels , mode='similarity ' ) > > > new_labels = graph.cut_normalized ( labels , rag ) References -- -- -- -- -- .. [ 1 ] Shi , J. ; Malik , J. , `` Normalized cuts and image segmentation '' , Pattern Analysis and Machine Intelligence , IEEE Transactions on , vol . 22 , no . 8 , pp . 888-905 , August 2000. `` '' '' ''",https://github.com/scikit-image/scikit-image/blob/51f598aaedc73ef180913c670d2c20a8032aaf1e/skimage/future/graph/graph_cut.py#L78,Python,yes,journal,background,n/a,related,computer vision,no transfer,no
"`` This class implements a bidirectional A algorithm . It is interesting to note that a bidirectional dijkstra is far more efficient than a single direction one . The same does not hold for a bidirectional A as the heuristic can not be as tight . < p > See http : research.microsoft.com apps pubs default.aspx ? id=64511 http : i11www.iti.uni-karlsruhe.de _media teaching sommer2012 routenplanung vorlesung4.pdf http : research.microsoft.com pubs 64504 goldberg-sofsem07.pdf http : www.cs.princeton.edu courses archive spr06 cos423 Handouts EPP % 20shortest % 20path % 20algorithms.pdf < p > and < p > 1 . Ikeda , T. , Hsu , M.-Y. , Imai , H. , Nishimura , S. , Shimoura , H. , Hashimoto , T. , Tenmoku , K. , and Mitoh , K. ( 1994 ) . A fast algorithm for finding better routes by ai search techniques . In VNIS , pages 291‚Äì296 . < p > 2 . Whangbo , T. K. ( 2007 ) . Efficient modified bidirectional a algorithm for optimal route- finding . In IEA AIE , volume 4570 , pages 344‚Äì353 . Springer . < p > or could we even use this three phase approach ? www.lix.polytechnique.fr ~giacomon papers bidirtimedep.pdf < p > @ author Peter Karich @ author jansoe ``",https://github.com/graphhopper/graphhopper/blob/a2fef827a7dd6498e467ee523fabaebc3148c2ca/core/src/main/java/com/graphhopper/routing/AStarBidirection.java#L33,Java,yes,conference,algorithm,class,reference,other,paper not available,no
"`` The triangle overlap code in this file is based on the triangle triangle intersection test routine by by Tomas Moller , 1997 . See article `` A Fast Triangle-Triangle Intersection Test '' , Journal of Graphics Tools , 2 ( 2 ) , 1997 In contrast to Moller 's general triangle-triangle intersection routine our code only tests for area overlap of coplanar triangles . In particular it will treat triangles which share an edge or vertex as non-overlapping . Only triangles with a bona-fide area overlap will be flagged . The code returns 1 if the provided triangles have an area overlap and 0 otherwise. ``",https://github.com/mcellteam/mcell/blob/7b7b128d79545c923eb8d83c19f18bdc599d6c9b/src/triangle_overlap.c#L24,C,yes,journal,algorithm,file,reference,simulation,formulas to source code,no
"`` Implementation of a inverse velocity kinematics algorithm based on the weighted pseudo inverse with damped least-square to calculate the velocity transformation from Cartesian to joint space of a general KDL : :Chain . It uses a svd-calculation based on householders rotations . J = M_q Vb pinv_dls ( Db ) Ub ' M_x where B = Mx J Mq and B = Ub Db Vb ' is the SVD decomposition of B Mq and Mx represent , respectively , the joint-space and task-space weighting matrices . Please refer to the documentation of setWeightJS ( const Eigen : :MatrixXd & Mq ) and setWeightTS ( const Eigen : :MatrixXd & Mx ) for details on the effects of these matrices . For more details on Weighted Pseudo Inverse , see : 1 ) [ Ben Israel 03 ] A. Ben Israel & T.N.E . Greville . Generalized Inverses : Theory and Applications , second edition . Springer , 2003 . ISBN 0-387-00293-6 . 2 ) [ Doty 93 ] K. L. Doty , C. Melchiorri & C. Boniveto . A theory of generalized inverses applied to Robotics . The International Journal of Robotics Research , vol . 12 , no . 1 , pages 1-19 , february 1993 . @ ingroup KinematicFamily ``",https://github.com/FreeCAD/FreeCAD/blob/c4fc02cbcfff975712e977dc08f859fba71ba0ad/src/Mod/Robot/App/kdl_cp/chainiksolvervel_wdls.hpp#L31,C++,yes,book,background,n/a,related,simulation,no transfer,no
"`` `` '' '' Return ( z , p , k ) for analog prototype of an Nth-order Bessel filter . Parameters -- -- -- -- -- N : int The order of the filter . norm : { 'phase ' , 'delay ' , 'mag ' } , optional Frequency normalization : `` phase `` The filter is normalized such that the phase response reaches its midpoint at an angular ( e.g . rad s ) cutoff frequency of 1 . This happens for both low-pass and high-pass filters , so this is the `` phase-matched '' case . [ 6 ] _ The magnitude response asymptotes are the same as a Butterworth filter of the same order with a cutoff of ` Wn ` . This is the default , and matches MATLAB 's implementation . `` delay `` The filter is normalized such that the group delay in the passband is 1 ( e.g . 1 second ) . This is the `` natural '' type obtained by solving Bessel polynomials `` mag `` The filter is normalized such that the gain magnitude is -3 dB at angular frequency 1 . This is called `` frequency normalization '' by Bond . [ 1 ] _ .. versionadded : : 0.18.0 Returns -- -- -- - z : ndarray Zeros of the transfer function . Is always an empty array . p : ndarray Poles of the transfer function . k : scalar Gain of the transfer function . For phase-normalized , this is always 1 . See Also -- -- -- -- bessel : Filter design function using this prototype Notes -- -- - To find the pole locations , approximate starting points are generated [ 2 ] _ for the zeros of the ordinary Bessel polynomial [ 3 ] _ , then the Aberth-Ehrlich method [ 4 ] _ [ 5 ] _ is used on the Kv ( x ) Bessel function to calculate more accurate zeros , and these locations are then inverted about the unit circle . References -- -- -- -- -- .. [ 1 ] C.R . Bond , `` Bessel Filter Constants '' , http : www.crbond.com papers bsf.pdf .. [ 2 ] Campos and Calderon , `` Approximate closed-form formulas for the zeros of the Bessel Polynomials '' , : arXiv : ` 1105.0957 ` . .. [ 3 ] Thomson , W.E. , `` Delay Networks having Maximally Flat Frequency Characteristics '' , Proceedings of the Institution of Electrical Engineers , Part III , November 1949 , Vol . 96 , No . 44 , pp . 487-490 . .. [ 4 ] Aberth , `` Iteration Methods for Finding all Zeros of a Polynomial Simultaneously '' , Mathematics of Computation , Vol . 27 , No . 122 , April 1973 .. [ 5 ] Ehrlich , `` A modified Newton method for polynomials '' , Communications of the ACM , Vol . 10 , Issue 2 , pp . 107-108 , Feb. 1967 , : DOI : ` 10.1145 363067.363115 ` .. [ 6 ] Miller and Bohn , `` A Bessel Filter Crossover , and Its Relation to Others '' , RaneNote 147 , 1998 , http : www.rane.com note147.html `` '' '' ''",https://github.com/scipy/scipy/blob/dfb0cc2e5946e81a7ccefbe101a72b0f96317a6e/scipy/signal/filter_design.py#L3894,Python,yes,other(report),background,n/a,related,science,no transfer,no
"`` Behaves the same as PairedTTester , only it uses the corrected resampled t-test statistic . < p > For more information see : < p > < -- technical-plaintext-start -- > Claude Nadeau , Yoshua Bengio ( 2001 ) . Inference for the Generalization Error . Machine Learning.. < -- technical-plaintext-end -- > < p > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Nadeau2001 , author = { Claude Nadeau and Yoshua Bengio } , journal = { Machine Learning } , title = { Inference for the Generalization Error } , year = { 2001 } , PDF = { http : www.iro.umontreal.ca ~lisa bib pub_subject comparative pointeurs nadeau_MLJ1597.pdf } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D & lt ; index , index2-index4 , ... & gt ; Specify list of columns that specify a unique dataset . First and last are valid indexes . ( default none ) < pre > < pre > -R & lt ; index & gt ; Set the index of the column containing the run number < pre > < pre > -F & lt ; index & gt ; Set the index of the column containing the fold number < pre > < pre > -G & lt ; index1 , index2-index4 , ... & gt ; Specify list of columns that specify a unique 'result generator ' ( eg : classifier name and options ) . First and last are valid indexes . ( default none ) < pre > < pre > -S & lt ; significance level & gt ; Set the significance level for comparisons ( default 0.05 ) < pre > < pre > -V Show standard deviations < pre > < pre > -L Produce table comparisons in Latex table format < pre > < pre > -csv Produce table comparisons in CSV table format < pre > < pre > -html Produce table comparisons in HTML table format < pre > < pre > -significance Produce table comparisons with only the significance values < pre > < pre > -gnuplot Produce table comparisons output suitable for GNUPlot < pre > < -- options-end -- > @ author Richard Kirkby ( rkirkby @ cs.waikato.ac.nz ) @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/waikato-3-7-7-1/src/main/java/weka/experiment/PairedCorrectedTTester.java#L41,Java,yes,journal,background,n/a,related,machine learning,no transfer,no
"`` < -- globalinfo-start -- > Class for building and using a multinomial Naive Bayes classifier . For more information see , < br > < br > Andrew Mccallum , Kamal Nigam : A Comparison of Event Models for Naive Bayes Text Classification . In : AAAI-98 Workshop on 'Learning for Text Categorization ' , 1998. < br > < br > The core equation for this classifier : < br > < br > P [ Ci|D ] = ( P [ D|Ci ] x P [ Ci ] ) P [ D ] ( Bayes rule ) < br > < br > where Ci is class i and D is a document . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Mccallum1998 , author = { Andrew Mccallum and Kamal Nigam } , booktitle = { AAAI-98 Workshop on 'Learning for Text Categorization ' } , title = { A Comparison of Event Models for Naive Bayes Text Classification } , year = { 1998 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < -- options-end -- > @ author Andrew Golightly ( acg4 @ cs.waikato.ac.nz ) @ author Bernhard Pfahringer ( bernhard @ cs.waikato.ac.nz ) @ version $ Revision : 1.13 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_fixes_for_FilteredClassifier/weka/classifiers/bayes/NaiveBayesMultinomial.java#L36,Java,yes,conference,algorithm,class,reference,machine learning,formulas to source code,no
"`` `` '' '' Compute all mixed Nash equilibria of a 2-player ( non-degenerate ) normalform game by support enumeration.References -- -- -- -- -- B. von Stengel , `` Equilibrium Computation for Two-Player Games inStrategic and Extensive Form , '' Chapter 3 , N. Nisan , T. Roughgarden , E.Tardos , and V. Vazirani eds. , Algorithmic Game Theory , 2007 . '' '' '' ''",https://github.com/QuantEcon/QuantEcon.py/blob/35f41b118b26c12e104d412ff33a252e903c0bfd/quantecon/game_theory/support_enumeration.py#L1,Python,yes,book,algorithm,file,reference,other,paper not available,no
"`` < -- globalinfo-start -- > Implements John Platt 's sequential minimal optimization algorithm for training a support vector classifier. < br > < br > This implementation globally replaces all missing values and transforms nominal attributes into binary ones . It also normalizes all attributes by default . ( In that case the coefficients in the output are based on the normalized data , not the original data -- - this is important for interpreting the classifier . ) < br > < br > Multi-class problems are solved using pairwise classification ( 1-vs-1 and if logistic models are built pairwise coupling according to Hastie and Tibshirani , 1998 ) . < br > < br > To obtain proper probability estimates , use the option that fits logistic regression models to the outputs of the support vector machine . In the multi-class case the predicted probabilities are coupled using Hastie and Tibshirani 's pairwise coupling method. < br > < br > Note : for improved speed normalization should be turned off when operating on SparseInstances. < br > < br > For more information on the SMO algorithm , see < br > < br > J. Platt : Machines using Sequential Minimal Optimization . In B. Schoelkopf and C. Burges and A. Smola , editors , Advances in Kernel Methods - Support Vector Learning , 1998. < br > < br > S.S. Keerthi , S.K . Shevade , C. Bhattacharyya , K.R.K . Murthy ( 2001 ) . Improvements to Platt 's SMO Algorithm for SVM Classifier Design . Neural Computation . 13 ( 3 ) :637-649. < br > < br > Trevor Hastie , Robert Tibshirani : Classification by Pairwise Coupling . In : Advances in Neural Information Processing Systems , 1998 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; incollection { Platt1998 , author = { J. Platt } , booktitle = { Advances in Kernel Methods - Support Vector Learning } , editor = { B. Schoelkopf and C. Burges and A. Smola } , publisher = { MIT Press } , title = { Machines using Sequential Minimal Optimization } , year = { 1998 } , URL = { http : research.microsoft.com ~jplatt smo.html } , PS = { http : research.microsoft.com ~jplatt smo-book.ps.gz } , PDF = { http : research.microsoft.com ~jplatt smo-book.pdf } } & 64 ; article { Keerthi2001 , author = { S.S. Keerthi and S.K . Shevade and C. Bhattacharyya and K.R.K . Murthy } , journal = { Neural Computation } , number = { 3 } , pages = { 637-649 } , title = { Improvements to Platt 's SMO Algorithm for SVM Classifier Design } , volume = { 13 } , year = { 2001 } , PS = { http : guppy.mpe.nus.edu.sg ~mpessk svm smo_mod_nc.ps.gz } } & 64 ; inproceedings { Hastie1998 , author = { Trevor Hastie and Robert Tibshirani } , booktitle = { Advances in Neural Information Processing Systems } , editor = { Michael I. Jordan and Michael J. Kearns and Sara A. Solla } , publisher = { MIT Press } , title = { Classification by Pairwise Coupling } , volume = { 10 } , year = { 1998 } , PS = { http : www-stat.stanford.edu ~hastie Papers 2class.ps } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D If set , classifier is run in debug mode and may output additional info to the console < pre > < pre > -no-checks Turns off all checks - use with caution Turning them off assumes that data is purely numeric , does n't contain any missing values , and has a nominal class . Turning them off also means that no header information will be stored if the machine is linear . Finally , it also assumes that no instance has a weight equal to 0 . ( default : checks on ) < pre > < pre > -C & lt ; double & gt ; The complexity constant C. ( default 1 ) < pre > < pre > -N Whether to 0=normalize 1=standardize 2=neither . ( default 0=normalize ) < pre > < pre > -L & lt ; double & gt ; The tolerance parameter . ( default 1.0e-3 ) < pre > < pre > -P & lt ; double & gt ; The epsilon for round-off error . ( default 1.0e-12 ) < pre > < pre > -M Fit logistic models to SVM outputs . < pre > < pre > -V & lt ; double & gt ; The number of folds for the internal cross-validation . ( default -1 , use training data ) < pre > < pre > -W & lt ; double & gt ; The random number seed . ( default 1 ) < pre > < pre > -K & lt ; classname and parameters & gt ; The Kernel to use . ( default : weka.classifiers.functions.supportVector.PolyKernel ) < pre > < pre > Options specific to kernel weka.classifiers.functions.supportVector.PolyKernel : < pre > < pre > -D Enables debugging output ( if available ) to be printed . ( default : off ) < pre > < pre > -no-checks Turns off all checks - use with caution ( default : checks on ) < pre > < pre > -C & lt ; num & gt ; The size of the cache ( a prime number ) . ( default : 250007 ) < pre > < pre > -E & lt ; num & gt ; The Exponent to use . ( default : 1.0 ) < pre > < pre > -L Use lower-order terms . ( default : no ) < pre > < -- options-end -- > @ author Eibe Frank ( eibe @ cs.waikato.ac.nz ) @ author Shane Legg ( shane @ intelligenesis.net ) ( sparse vector code ) @ author Stuart Inglis ( stuart @ reeltwo.com ) ( sparse vector code ) @ version $ Revision : 1.63 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-5-4/weka/classifiers/functions/SMO.java#L57,Java,yes,book,algorithm,class,reference,machine learning,formulas to source code,no
"`` American Put Options Pricing using the Black-Scholes Equation Background ( European Options ) : The standard European option is a contract where the holder has the right to either buy ( call option ) or sell ( put option ) an underlying asset at a designated future time and price . The classic Black-Scholes model begins with an assumption that the price of the underlying asset behaves as a lognormal random walk . Using this assumption and a no-arbitrage argument , the following linear parabolic partial differential equation for the value of the option results : dV dt + 0.5 ( sigma 2 ) ( S alpha ) ( d2V dS2 ) + ( r - D ) S ( dV dS ) - rV = 0 . Here , sigma is the volatility of the underling asset , alpha is a measure of elasticity ( typically two ) , D measures the dividend payments on the underling asset , and r is the interest rate . To completely specify the problem , we need to impose some boundary conditions . These are as follows : V ( S , T ) = max ( E - S , 0 ) V ( 0 , t ) = E for all 0 < = t < = T V ( s , t ) = 0 for all 0 < = t < = T and s- > infinity where T is the exercise time time and E the strike price ( price paid for the contract ) . An explicit formula for the value of an European option can be found . See the references for examples . Background ( American Options ) : The American option is similar to its European counterpart . The difference is that the holder of the American option can excercise their right to buy or sell the asset at any time prior to the expiration . This additional ability introduce a free boundary into the Black-Scholes equation which can be modeled as a linear complementarity problem . 0 < = - ( dV dt + 0.5 ( sigma 2 ) ( S alpha ) ( d2V dS2 ) + ( r - D ) S ( dV dS ) - rV ) complements V ( S , T ) > = max ( E-S,0 ) where the variables are the same as before and we have the same boundary conditions . There is not explicit formula for calculating the value of an American option . Therefore , we discretize the above problem and solve the resulting linear complementarity problem . We will use backward differences for the time variables and central differences for the space variables . Crank-Nicholson averaging will also be used in the discretization . The algorithm used by the code solves for V ( S , t ) for a fixed t and then uses this value in the calculation of V ( S , t - dt ) . The method stops when V ( S,0 ) has been found . References : Huang and Pang , `` Options Pricing and Linear Complementarity , '' Journal of Computational Finance , volume 2 , number 3 , 1998 . Wilmott , `` Derivatives : The Theory and Practice of Financial Engineering , '' John Wiley and Sons , New York , 1998. ``",https://github.com/petsc/petsc/blob/98228bd44f1ac401f769922ccdd902ec91091718/src/tao/complementarity/examples/tutorials/blackscholes.c#L1,C,404,,,,,,,
"`` `` '' '' Return the Hanning window . The Hanning window is a taper formed by using a weighted cosine . Parameters -- -- -- -- -- M : int Number of points in the output window . If zero or less , an empty array is returned . Returns -- -- -- - out : ndarray , shape ( M , ) The window , with the maximum value normalized to one ( the value one appears only if ` M ` is odd ) . See Also -- -- -- -- bartlett , blackman , hamming , kaiser Notes -- -- - The Hanning window is defined as .. math : : w ( n ) = 0.5 - 0.5cos left ( frac { 2 pi { n } } { M-1 } right ) qquad 0 leq n leq M-1 The Hanning was named for Julius von Hann , an Austrian meteorologist . It is also known as the Cosine Bell . Some authors prefer that it be called a Hann window , to help avoid confusion with the very similar Hamming window . Most references to the Hanning window come from the signal processing literature , where it is used as one of many windowing functions for smoothing values . It is also known as an apodization ( which means `` removing the foot '' , i.e . smoothing discontinuities at the beginning and end of the sampled signal ) or tapering function . References -- -- -- -- -- .. [ 1 ] Blackman , R.B . and Tukey , J.W. , ( 1958 ) The measurement of power spectra , Dover Publications , New York . .. [ 2 ] E.R . Kanasewich , `` Time Sequence Analysis in Geophysics '' , The University of Alberta Press , 1975 , pp . 106-108 . .. [ 3 ] Wikipedia , `` Window function '' , http : en.wikipedia.org wiki Window_function .. [ 4 ] W.H . Press , B.P . Flannery , S.A. Teukolsky , and W.T . Vetterling , `` Numerical Recipes '' , Cambridge University Press , 1986 , page 425 . Examples -- -- -- -- > > > np.hanning ( 12 ) array ( [ 0. , 0.07937323 , 0.29229249 , 0.57115742 , 0.82743037 , 0.97974649 , 0.97974649 , 0.82743037 , 0.57115742 , 0.29229249 , 0.07937323 , 0 . ] ) Plot the window and its frequency response : > > > from numpy.fft import fft , fftshift > > > window = np.hanning ( 51 ) > > > plt.plot ( window ) [ < matplotlib.lines.Line2D object at 0x ... > ] > > > plt.title ( `` Hann window '' ) < matplotlib.text.Text object at 0x ... > > > > plt.ylabel ( `` Amplitude '' ) < matplotlib.text.Text object at 0x ... > > > > plt.xlabel ( `` Sample '' ) < matplotlib.text.Text object at 0x ... > > > > plt.show ( ) > > > plt.figure ( ) < matplotlib.figure.Figure object at 0x ... > > > > A = fft ( window , 2048 ) 25.5 > > > mag = np.abs ( fftshift ( A ) ) > > > freq = np.linspace ( -0.5 , 0.5 , len ( A ) ) > > > response = 20 np.log10 ( mag ) > > > response = np.clip ( response , -100 , 100 ) > > > plt.plot ( freq , response ) [ < matplotlib.lines.Line2D object at 0x ... > ] > > > plt.title ( `` Frequency response of the Hann window '' ) < matplotlib.text.Text object at 0x ... > > > > plt.ylabel ( `` Magnitude [ dB ] '' ) < matplotlib.text.Text object at 0x ... > > > > plt.xlabel ( `` Normalized frequency [ cycles per sample ] '' ) < matplotlib.text.Text object at 0x ... > > > > plt.axis ( 'tight ' ) ( -0.5 , 0.5 , -100.0 , ... ) > > > plt.show ( ) `` '' '' ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/numpy/lib/function_base.py#L2789,Python,yes,book,algorithm,method,reference,other,formulas to source code,no
"`` `` '' '' Mini-Batch K-Medoids clustering . This method finds a set of cluster centers that are themselves data points , attempting to minimize the mean-squared distance from the datapoints to their assigned cluster centers using only mini-batches of the dataset . Mini batches of the dataset are selected , and augmented to include each of the cluster centers . Then , standard KMedoids clustering is performed on the batch , using code based on the C clustering library [ 1 ] . The memory requirement scales as the square `` batch_size `` instead of the square of the size of the dataset . Parameters -- -- -- -- -- n_clusters : int , optional , default : 8 The number of clusters to form as well as the number of centroids to generate . max_iter : int , optional , default=5 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics . batch_size : int , optional , default : 100 Size of the mini batches . metric : { `` euclidean '' , `` sqeuclidean '' , `` cityblock '' , `` chebyshev '' , `` canberra '' , `` braycurtis '' , `` hamming '' , `` jaccard '' , `` cityblock '' , `` rmsd '' } The distance metric to use . metric = `` rmsd '' requires that sequences passed to `` fit ( ) `` be `` ` md.Trajectory `` ` ; other distance metrics require `` np.ndarray `` s . max_no_improvement : int , default : 10 Control early stopping based on the consecutive number of mini batches that do not lead to any modified assignments . random_state : integer or numpy.RandomState , optional The generator used to initialize the centers . If an integer is given , it fixes the seed . Defaults to the global numpy random number generator . References -- -- -- -- -- .. [ 1 ] de Hoon , Michiel JL , et al . `` Open source clustering software . '' Bioinformatics 20.9 ( 2004 ) : 1453-1454 . See Also -- -- -- -- KMedoids : Batch version , requring O ( N^2 ) memory . Attributes -- -- -- -- -- cluster_ids_ : array , [ n_clusters ] Index of the data point that each cluster label corresponds to . cluster_centers_ : array , [ n_clusters , n_features ] or md.Trajectory Coordinates of cluster centers . labels_ : array , [ n_samples , ] The label of each point is an integer in [ 0 , n_clusters ) . inertia_ : float Sum of distances of samples to their closest cluster center. `` '' '' ''",https://github.com/msmbuilder/msmbuilder/blob/dea86dca81b1ca9c666b410eef2ee72303b5960e/msmbuilder/cluster/minibatchkmedoids.py#L24,Python,yes,journal,background,n/a,related,simulation,no transfer,no
"`` Problem C1-DTLZ1 , defined in : Jain , H. and K. Deb . `` An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach , Part II : Handling Constraints and Extending to an Adaptive Approach . '' EEE Transactions on Evolutionary Computation , 18 ( 4 ) :602-622 , 2014 . @ author Antonio J. Nebro < antonio @ lcc.uma.es > ``",https://github.com/jMetal/jMetal/blob/e6baf75aa9bf475359ca249096d0231eb61bd0c2/jmetal-problem/src/main/java/org/uma/jmetal/problem/multiobjective/cdtlz/C1_DTLZ1.java#L9,Java,yes,journal,background,n/a,related,science,no transfer,no
"`` Revised wakeup rule [ 1 ] : For self-suspending tasks , rather then re-initializing task 's runtime and deadline , the revised wakeup rule adjusts the task 's runtime to avoid the task to overrun its density . Reasoning : a task may overrun the density if : runtime ( deadline - t ) > dl_runtime dl_deadline Therefore , runtime can be adjusted to : runtime = ( dl_runtime dl_deadline ) ( deadline - t ) In such way that runtime will be equal to the maximum density the task can use without breaking any rule . [ 1 ] Luca Abeni , Giuseppe Lipari , and Juri Lelli . 2015 . Constant bandwidth server revisited . SIGBED Rev . 11 , 4 ( January 2015 ) , 19-24. ``",https://github.com/torvalds/linux/blob/63f04777162181798399a2c4e5436d0d0c16291b/kernel/sched/deadline.c#L771,C,yes,journal,background,n/a,related,networks and os,no transfer,no
"`` `` '' '' Compute the fractional power of a matrix , for fractions -1 < t < 1 . This uses algorithm ( 3.1 ) of [ 1 ] _ . The Pade approximation itself uses algorithm ( 4.1 ) of [ 2 ] _. Parameters -- -- -- -- -- A : ( N , N ) array_like Matrix whose fractional power to evaluate . t : float Fractional power between -1 and 1 exclusive . Returns -- -- -- - X : ( N , N ) array_like The fractional power of the matrix . References -- -- -- -- -- .. [ 1 ] Nicholas J. Higham and Lijing Lin ( 2013 ) `` An Improved Schur-Pade Algorithm for Fractional Powers of a Matrix and their Frechet Derivatives . '' .. [ 2 ] Nicholas J. Higham and Lijing lin ( 2011 ) `` A Schur-Pade Algorithm for Fractional Powers of a Matrix . '' SIAM Journal on Matrix Analysis and Applications , 32 ( 3 ) . pp . 1056-1078 . ISSN 0895-4798 `` '' '' ''",https://github.com/scipy/scipy/blob/dfb0cc2e5946e81a7ccefbe101a72b0f96317a6e/scipy/linalg/_matfuncs_inv_ssq.py#L599,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` q_high = mpn_bdivmod ( qp , up , usize , vp , vsize , d ) . Puts the low d BITS_PER_MP_LIMB limbs of Q = U V mod 2^d at qp , and returns the high d % BITS_PER_MP_LIMB bits of Q as the result . Also , U - Q V mod 2^ ( usize BITS_PER_MP_LIMB ) is placed at up . Since the low d BITS_PER_MP_LIMB limbs of this difference are zero , the code allows the limb vectors at qp to overwrite the low limbs at up , provided qp < = up . Preconditions : 1 . V is odd . 2. usize BITS_PER_MP_LIMB > = d. 3 . If Q and U overlap , qp < = up . Ken Weber ( kweber @ mat.ufrgs.br , kweber @ mcs.kent.edu ) Funding for this work has been partially provided by Conselho Nacional de Desenvolvimento Cienti'fico e Tecnolo'gico ( CNPq ) do Brazil , Grant 301314194-2 , and was done while I was a visiting reseacher in the Instituto de Matema'tica at Universidade Federal do Rio Grande do Sul ( UFRGS ) . References : T. Jebelean , An algorithm for exact division , Journal of Symbolic Computation , v. 15 , 1993 , pp . 169-180 . K. Weber , The accelerated integer GCD algorithm , ACM Transactions on Mathematical Software , v. 21 ( March ) , 1995 , pp . 111-122. ``",https://github.com/modula3/cm3/blob/d2bd6c488e087a60224cc02481e5889067a09219/m3-sys/m3cc/gmp-4.3.2/mpn/generic/bdivmod.c#L21,C,yes,journal,background,n/a,related,science,no transfer,no
"`` class LBFGSOptimizerv4 brief Wrap of the vnl_lbfgs algorithm for use in ITKv4 registration framework . The vnl_lbfgs is a wrapper for the NETLIB fortran code by Nocedal [ 1 ] . LBFGS is a quasi-Newton method . Quasi-Newton methods use an approximate estimate of the inverse Hessian f $ ( abla^2 f ( x ) ) ^ { -1 } f $ to scale the gradient step : f [ x_ { n+1 } = x_n - s ( abla^2 f ( x_n ) ) ^ { -1 } abla f ( x ) f ] with f $ s f $ the step size . The inverse Hessian is approximated from the gradients of previous iteration and thus only the gradient of the objective function is required . The step size f $ s f $ is determined through line search with the approach by More and Thuente [ 4 ] . This line search approach finds a step size such that f [ lVert abla f ( x + s ( abla^2 f ( x_n ) ) ^ { -1 } abla f ( x ) ) rVert le u lVert abla f ( x ) rVert f ] The parameter f $ u f $ is set through SetLineSearchAccuracy ( ) ( default 0.9 ) The default step length , i.e . starting step length for the line search , is set through SetDefaultStepLength ( ) ( default 1.0 ) . The optimization stops when either the gradient satisfies the condition f [ lVert abla f ( x ) rVert le epsilon max ( 1 , lVert X rVert ) f ] or a maximum number of function evaluations has been reached . The tolerance f $ epsilon f $ is set through SetGradientConvergenceTolerance ( ) ( default 1e-5 ) and the maximum number of function evaluations is set through SetMaximumNumberOfFunctionEvaluations ( ) ( default 2000 ) . Note : The scaling of the optimization paramaters , set through SetScales ( ) , should be set or left at one . Otherwise the Hessian approximation as well as the line search will be disturbed and the optimizer is unlikely to find a minima . References : [ 1 ] [ NETLIB lbfgs ] ( http : users.iems.northwestern.edu ~nocedal lbfgs.html ) [ 2 ] Jorge Nocedal . Updating Quasi-Newton Matrices with Limited Storage . Mathematics of Computation , Vol . 35 , No . 151 , pp . 773‚Äì782 , 1980 . [ 3 ] Dong C. Liu and Jorge Nocedal . On the limited memory BFGS method for large scale optimization . Mathematical Programming B , Vol . 45 , No . 3 , pp . 503-528 , 1989 . [ 4 ] More , J. J. and D. J. Thuente . Line Search Algorithms with Guaranteed Sufficient Decrease . ACM Transactions on Mathematical Software 20 , no . 3 ( 1994 ) : 286‚Äì307 . ingroup ITKOptimizersv4 ``",https://github.com/InsightSoftwareConsortium/ITK/blob/a077e98358a87c2b507e5ee9c27939918a9120ea/Modules/Numerics/Optimizersv4/include/itkLBFGSOptimizerv4.h#L27,C++,yes,other(documentation),background,n/a,related,computer vision,no transfer,no
"`` Generate the random number using the linear congruential generator defined by the following recurrence : seed = ( IA seed ) mod IM where IA is 16807 and IM is ( 2^31 ) - 1 . The recurrence maps a seed in the range [ 1 , IM - 1 ] to a new seed in that same range . The recurrence maps IM to 0 , and maps 0 back to 0 , so those two values must not be allowed as initial values of seed . In order to avoid potential problems with integer overflow , the recurrence is implemented in terms of additional constants IQ and IR such that IM = IA IQ + IR None of the operations in the implementation overflows a 32-bit signed integer , and the C type long is guaranteed to be at least 32 bits wide . For more details on how this algorithm works , refer to the following papers : S.K . Park & K.W . Miller , `` Random number generators : good ones are hard to find , '' Comm ACM 31 ( 10 ) :1192-1201 , Oct 1988 W.H . Press & S.A. Teukolsky , `` Portable random number generators , '' Computers in Physics 6 ( 5 ) :522-524 , Sep Oct 1992. ``",https://github.com/jdkoftinoff/mb-linux-msli/blob/8adb103e34af9ff1d8d122e9ee2a1808836f0a14/uClinux-dist/user/tcl/generic/tclExecute.c#L3926,C,yes,journal,algorithm,method,reference,networks and os,formulas to source code,no
"`` r '' '' '' Represents unevaluated summation . `` Sum `` represents a finite or infinite series , with the first argument being the general form of terms in the series , and the second argument being `` ( dummy_variable , start , end ) `` , with `` dummy_variable `` taking all integer values from `` start `` through `` end `` . In accordance with long-standing mathematical convention , the end term is included in the summation . For finite sums ( and sums with symbolic limits assumed to be finite ) we follow the summation convention described by Karr [ 1 ] , especially definition 3 of section 1.4 . The sum : .. math : : sum_ { m leq i < n } f ( i ) has the obvious meaning for ` m < n ` , namely : .. math : : sum_ { m leq i < n } f ( i ) = f ( m ) + f ( m+1 ) + ldots + f ( n-2 ) + f ( n-1 ) with the upper limit value ` f ( n ) ` excluded . The sum over an empty set is zero if and only if ` m = n ` : .. math : : sum_ { m leq i < n } f ( i ) = 0 quad mathrm { for } quad m = n Finally , for all other sums over empty sets we assume the following definition : .. math : : sum_ { m leq i < n } f ( i ) = - sum_ { n leq i < m } f ( i ) quad mathrm { for } quad m > n It is important to note that Karr defines all sums with the upper limit being exclusive . This is in contrast to the usual mathematical notation , but does not affect the summation convention . Indeed we have : .. math : : sum_ { m leq i < n } f ( i ) = sum_ { i = m } ^ { n - 1 } f ( i ) where the difference in notation is intentional to emphasize the meaning , with limits typeset on the top being inclusive . Examples ======== > > > from diofant.abc import i > > > Sum ( k , ( k , 1 , m ) ) Sum ( k , ( k , 1 , m ) ) > > > Sum ( k , ( k , 1 , m ) ) .doit ( ) m 2 2 + m 2 > > > Sum ( k 2 , ( k , 1 , m ) ) Sum ( k 2 , ( k , 1 , m ) ) > > > Sum ( k 2 , ( k , 1 , m ) ) .doit ( ) m 3 3 + m 2 2 + m 6 > > > Sum ( x k , ( k , 0 , oo ) ) Sum ( x k , ( k , 0 , oo ) ) > > > Sum ( x k , ( k , 0 , oo ) ) .doit ( ) Piecewise ( ( 1 ( -x + 1 ) , Abs ( x ) < 1 ) , ( Sum ( x k , ( k , 0 , oo ) ) , true ) ) > > > Sum ( x k factorial ( k ) , ( k , 0 , oo ) ) .doit ( ) E x Here are examples to do summation with symbolic indices . You can use either Function of IndexedBase classes : > > > f = Function ( ' f ' ) > > > Sum ( f ( n ) , ( n , 0 , 3 ) ) .doit ( ) f ( 0 ) + f ( 1 ) + f ( 2 ) + f ( 3 ) > > > Sum ( f ( n ) , ( n , 0 , oo ) ) .doit ( ) Sum ( f ( n ) , ( n , 0 , oo ) ) > > > f = IndexedBase ( ' f ' ) > > > Sum ( f [ n ] 2 , ( n , 0 , 3 ) ) .doit ( ) f [ 0 ] 2 + f [ 1 ] 2 + f [ 2 ] 2 + f [ 3 ] 2 An example showing that the symbolic result of a summation is still valid for seemingly nonsensical values of the limits . Then the Karr convention allows us to give a perfectly valid interpretation to those sums by interchanging the limits according to the above rules : > > > S = Sum ( i , ( i , 1 , n ) ) .doit ( ) > > > S n 2 2 + n 2 > > > S.subs ( n , -4 ) 6 > > > Sum ( i , ( i , 1 , -4 ) ) .doit ( ) 6 > > > Sum ( -i , ( i , -3 , 0 ) ) .doit ( ) 6 An explicit example of the Karr summation convention : > > > S1 = Sum ( i 2 , ( i , m , m+n-1 ) ) .doit ( ) > > > S1 m 2 n + m n 2 - m n + n 3 3 - n 2 2 + n 6 > > > S2 = Sum ( i 2 , ( i , m+n , m-1 ) ) .doit ( ) > > > S2 -m 2 n - m n 2 + m n - n 3 3 + n 2 2 - n 6 > > > S1 + S2 0 > > > S3 = Sum ( i , ( i , m , m-1 ) ) .doit ( ) > > > S3 0 See Also ======== diofant.concrete.summations.summation diofant.concrete.products.Product diofant.concrete.products.product References ========== .. [ 1 ] Michael Karr , `` Summation in Finite Terms '' , Journal of the ACM , Volume 28 Issue 2 , April 1981 , Pages 305-350 https : dl.acm.org citation.cfm ? doid=322248.322255 .. [ 2 ] https : en.wikipedia.org wiki Summation Capital-sigma_notation .. [ 3 ] https : en.wikipedia.org wiki Empty_sum `` '' '' ''",https://github.com/diofant/diofant/blob/4d9f43afde5cae9c6c20890e4ee7352f3f296166/diofant/concrete/summations.py#L11,Python,yes,journal,algorithm,class,reference,science,formulas to source code,no
"`` r '' '' '' btdtria ( p , b , x ) Inverse of ` btdtr ` with respect to ` a ` . This is the inverse of the beta cumulative distribution function , ` btdtr ` , considered as a function of ` a ` , returning the value of ` a ` for which ` btdtr ( a , b , x ) = p ` , or .. math : : p = int_0^x frac { Gamma ( a + b ) } { Gamma ( a ) Gamma ( b ) } t^ { a-1 } ( 1-t ) ^ { b-1 } , dt Parameters -- -- -- -- -- p : array_like Cumulative probability , in [ 0 , 1 ] . b : array_like Shape parameter ( ` b ` > 0 ) . x : array_like The quantile , in [ 0 , 1 ] . Returns -- -- -- - a : ndarray The value of the shape parameter ` a ` such that ` btdtr ( a , b , x ) = p ` . See Also -- -- -- -- btdtr : Cumulative density function of the beta distribution . btdtri : Inverse with respect to ` x ` . btdtrib : Inverse with respect to ` b ` . Notes -- -- - Wrapper for the CDFLIB [ 1 ] _ Fortran routine ` cdfbet ` . The cumulative distribution function ` p ` is computed using a routine by DiDinato and Morris [ 2 ] _. Computation of ` a ` involves a seach for a value that produces the desired value of ` p ` . The search relies on the monotinicity of ` p ` with ` a ` . References -- -- -- -- -- .. [ 1 ] Barry Brown , James Lovato , and Kathy Russell , CDFLIB : Library of Fortran Routines for Cumulative Distribution Functions , Inverses , and Other Parameters . .. [ 2 ] DiDinato , A. R. and Morris , A. H. , Algorithm 708 : Significant Digit Computation of the Incomplete Beta Function Ratios . ACM Trans . Math . Softw . 18 ( 1993 ) , 360-373. `` '' '' ''",https://github.com/metamorph-inc/meta-core/blob/771af3b12e17e01c16b32c61649cb16d826fd5d9/bin/Python27/Lib/site-packages/scipy/special/add_newdocs.py#L410,Python,yes,other(website),background,n/a,related,other,no transfer,no
"`` `` '' '' Evaluate all derivatives of a B-spline . Given the knots and coefficients of a cubic B-spline compute all derivatives up to order k at a point ( or set of points ) . Parameters -- -- -- -- -- x : array_like A point or a set of points at which to evaluate the derivatives . Note that `` t ( k ) < = x < = t ( n-k+1 ) `` must hold for each ` x ` . tck : tuple A tuple ( t , c , k ) containing the vector of knots , the B-spline coefficients , and the degree of the spline . Returns -- -- -- - results : { ndarray , list of ndarrays } An array ( or a list of arrays ) containing all derivatives up to order k inclusive for each point ` x ` . See Also -- -- -- -- splprep , splrep , splint , sproot , splev , bisplrep , bisplev , UnivariateSpline , BivariateSpline References -- -- -- -- -- .. [ 1 ] de Boor C : On calculating with b-splines , J. Approximation Theory 6 ( 1972 ) 50-62 . .. [ 2 ] Cox M.G . : The numerical evaluation of b-splines , J. Inst . Maths applics 10 ( 1972 ) 134-149 . .. [ 3 ] Dierckx P. : Curve and surface fitting with splines , Monographs on Numerical Analysis , Oxford University Press , 1993. `` '' '' ''",https://github.com/scipy/scipy/blob/dfb0cc2e5946e81a7ccefbe101a72b0f96317a6e/scipy/interpolate/_fitpack_impl.py#L737,Python,yes,journal,background,n/a,related,science,no transfer,no
"`` This is a lagged fibonacci generator with skipping developed by Luescher . The sequence is a series of 24-bit integers , x_n , x_n = d_n + b_n where d_n = x_ { n-10 } - x_ { n-24 } - c_ { n-1 } , b_n = 0 if d_n > = 0 and b_n = 2^24 if d_n < 0 , c_n = 0 if d_n > = 0 and c_n = 1 if d_n < 0 , where after 24 samples a group of p integers are `` skipped '' , to reduce correlations . By default p = 199 , but can be increased to 365 . The period of the generator is around 10^171 . From : M. Luescher , `` A portable high-quality random number generator for lattice field theory calculations '' , Computer Physics Communications , 79 ( 1994 ) 100-110 . Available on the net as hep-lat 9309020 at http : xxx.lanl.gov See also , F. James , `` RANLUX : A Fortran implementation of the high-quality pseudo-random number generator of Luscher '' , Computer Physics Communications , 79 ( 1994 ) 111-114 Kenneth G. Hamilton , F. James , `` Acceleration of RANLUX '' , Computer Physics Communications , 101 ( 1997 ) 241-248 Kenneth G. Hamilton , `` Assembler RANLUX for PCs '' , Computer Physics Communications , 101 ( 1997 ) 249-253 ``",https://github.com/praat/praat/blob/a01b498b4c521358b9571794e82fd79c1f31d452/external/gsl/gsl_rng__ranlux.c#L24,C++,yes,journal,number,file,reference,other,numbers to hard coded values,no
"`` `` '' '' Return HITS hubs and authorities values for nodes . The HITS algorithm computes two numbers for a node . Authorities estimates the node value based on the incoming links . Hubs estimates the node value based on outgoing links . Parameters -- -- -- -- -- G : graph A NetworkX graph normalized : bool ( default=True ) Normalize results by the sum of all of the values . Returns -- -- -- - ( hubs , authorities ) : two-tuple of dictionaries Two dictionaries keyed by node containing the hub and authority values . Examples -- -- -- -- > > > G=nx.path_graph ( 4 ) > > > h , a=nx.hits ( G ) Notes -- -- - The eigenvector calculation uses NumPy 's interface to LAPACK . The HITS algorithm was designed for directed graphs but this algorithm does not check if the input graph is directed and will execute on undirected graphs . References -- -- -- -- -- .. [ 1 ] A. Langville and C. Meyer , `` A survey of eigenvector methods of web information retrieval . '' http : citeseer.ist.psu.edu 713792.html .. [ 2 ] Jon Kleinberg , Authoritative sources in a hyperlinked environment Journal of the ACM 46 ( 5 ) : 604-32 , 1999. doi:10.1145 324133.324140. http : www.cs.cornell.edu home kleinber auth.pdf. `` '' '' ''",https://github.com/networkx/networkx/blob/404679f7a8a9f015a693eaa01a5b319091123b31/networkx/algorithms/link_analysis/hits_alg.py#L142,Python,yes,journal,algorithm,method,reference,networks and os,formulas to source code,no
"`` function igraph_authority_score Kleinerg 's authority scores The authority scores of the vertices are defined as the principal eigenvector of < code > A^T A < code > , where < code > A < code > is the adjacency matrix of the graph , < code > A^T < code > is its transposed . < para > < para > See the following reference on the meaning of this score : J. Kleinberg . Authoritative sources in a hyperlinked environment . emb Proc . 9th ACM-SIAM Symposium on Discrete Algorithms , eme 1998 . Extended version in emb Journal of the ACM eme 46 ( 1999 ) . Also appears as IBM Research Report RJ 10076 , May 1997. param graph The input graph . Can be directed and undirected . param vector Pointer to an initialized vector , the result is stored here . If a null pointer then it is ignored . param value If not a null pointer then the eigenvalue corresponding to the calculated eigenvector is stored here . param scale If not zero then the result will be scaled such that the absolute value of the maximum centrality is one . param weights A null pointer ( =no edge weights ) , or a vector giving the weights of the edges . param options Options to ARPACK . See ref igraph_arpack_options_t for details . Note that the function overwrites the < code > n < code > ( number of vertices ) parameter and it always starts the calculation from a non-random vector calculated based on the degree of the vertices . return Error code . Time complexity : depends on the input graph , usually it is O ( |V| ) , the number of vertices . sa ref igraph_hub_score ( ) for the companion measure , ref igraph_pagerank ( ) , ref igraph_personalized_pagerank ( ) , ref igraph_eigenvector_centrality ( ) for similar measures. ``",https://github.com/igraph/igraph/blob/ef2b26e2cbe8eaa182810a0b71e8bba2ad070c1b/src/centrality.c#L848,C,yes,conference,background,n/a,related,networks and os,no transfer,no
"`` `` '' '' Calculates temperature and emission measure from GOES XRS data . This function calculates the isothermal temperature and volume emission measure of the solar soft X-ray emitting plasma observed by the GOES XRS . This is done using the observed flux ratio of the short ( 0.5-4 angstrom ) to long ( 1-8 angstrom ) channels . Parameters -- -- -- -- -- longflux , shortflux : ` ~astropy.units.Quantity ` Arrays containing the long and short GOES XRS flux measurements respectively as a function of time . Must be of same length . [ W m 2 ] . satellite : int ( optional ) Number of GOES satellite used to make observations , important for correct calibration of data . Default=8 date : ` datetime.datetime ` or ` str ` Date when observations made . Important for correctcalibration . Default=today abundances : ( optional ) string equalling 'coronal ' or 'photospheric ' States whether photospheric or coronal abundances should be assumed . Default='coronal ' download : ( optional ) bool If True , the GOES temperature and emission measure data files are downloaded . It is important to do this if a new version of the files has been generated due to a new CHIANTI version being released or the launch of new GOES satellites since these files were last downloaded . Default=False download_dir : ( optional ) string The directory to download the GOES temperature and emission measure data files to . Default=SunPy default download directory Returns -- -- -- - temp : ` ~astropy.units.Quantity ` Array of temperature values of same length as longflux and shortflux . Units= [ MK ] em : ` ~astropy.units.Quantity ` Array of volume emission measure values of same length as longflux and shortflux . Units= [ 10 49 cm -3 ] Notes -- -- - The temperature and volume emission measure are calculated here using the methods of [ 1 ] _ who used the CHIANTI atomic physics database to model the response of the ratio of the short ( 0.5-4 angstrom ) to long ( 1-8 angstrom ) channels of the XRSs onboard various GOES satellites . This method assumes an isothermal plasma , the ionisation equilibria of [ 2 ] _ , and a constant density of 10 10 cm -3 . ( See White et al . 2005 for justification of this last assumption . ) This function is based on goes_chianti_tem.pro in SolarSoftWare written in IDL by Stephen White . Recent fluxes released to the public are scaled to be consistent with GOES-7 . In fact these recent fluxes are correct and so this correction must be removed before proceeding to use transfer functions . Email Rodney Viereck ( NOAA ) for more information . Measurements of short channel flux of less than 1e-10 W m 2 or long channel flux less than 3e-8 W m 2 are not considered good . Ratio values corresponding to such fluxes are set to 0.003 . References -- -- -- -- -- .. [ 1 ] White , S. M. , Thomas , R. J. , & Schwartz , R. A . 2005 , Sol . Phys. , 227 , 231 , DOI : 10.1007 s11207-005-2445-z .. [ 2 ] Mazzotta , P. , Mazzitelli , G. , Colafrancesco , S. , & Vittorio , N. 1998 , A & AS , 133 , 339 , DOI : 10.1051 aas:1998330 Examples -- -- -- -- > > > from sunpy.instr.goes import _goes_chianti_tem > > > from astropy.units import Quantity > > > longflux = Quantity ( [ 7e-6 , 7e-6 ] , unit= '' W m m '' ) > > > shortflux = Quantity ( [ 7e-7 , 7e-7 ] , unit= '' W m m '' ) > > > temp , em = _goes_chianti_tem ( longflux , shortflux , satellite=15 , ... date='2014-04-16 ' , ... abundances= '' coronal '' ) doctest : +REMOTE_DATA > > > temp doctest : +REMOTE_DATA < Quantity [ 11.28295376 , 11.28295376 ] MK > > > > em doctest : +REMOTE_DATA < Quantity [ 4.78577516e+48 , 4.78577516e+48 ] 1 cm3 > `` '' '' ''",https://github.com/sunpy/sunpy/blob/cf6df141e0c9a66b94818318eebfa6f906ec9b2f/sunpy/instr/goes.py#L281,Python,yes,journal,algorithm,method,reference,science,paper not available,no
"`` see http : rsb.info.nih.gov ij developer source ij process AutoThresholder.java.html . Huang : Implements Huang 's fuzzy thresholding method : Huang , L-K & Wang , M-J J ( 1995 ) , `` Image thresholding by minimizing the measure of fuzziness '' , Pattern Recognition 28 ( 1 ) : 41-51 ``",https://github.com/image-js/core/blob/4b7a84ca0670b25fb7ee3441578fd933964fcfd0/src/image/transform/mask/huang.js#L1,JavaScript,yes,journal,algorithm,method,reference,computer vision,formulas to source code,no
"`` Cubic Filters using B , C determined values : Mitchell-Netravali B = 1 3 C = 1 3 `` Balanced '' cubic spline filter Catmull-Rom B = 0 C = 1 2 Interpolatory and exact on linears Spline B = 1 C = 0 B-Spline Gaussian approximation Hermite B = 0 C = 0 B-Spline interpolator See paper by Mitchell and Netravali , Reconstruction Filters in Computer Graphics Computer Graphics , Volume 22 , Number 4 , August 1988 http : www.cs.utexas.edu users fussell courses cs384g lectures mitchell Mitchell.pdf . Coefficents are determined from B , C values : P0 = ( 6 - 2 B ) 6 = coeff [ 0 ] P1 = 0 P2 = ( -18 +12 B + 6 C ) 6 = coeff [ 1 ] P3 = ( 12 - 9 B - 6 C ) 6 = coeff [ 2 ] Q0 = ( 8 B +24 C ) 6 = coeff [ 3 ] Q1 = ( -12 B -48 C ) 6 = coeff [ 4 ] Q2 = ( 6 B +30 C ) 6 = coeff [ 5 ] Q3 = ( - 1 B - 6 C ) 6 = coeff [ 6 ] which are used to define the filter : P0 + P1 x + P2 x^2 + P3 x^3 0 < = x < 1 Q0 + Q1 x + Q2 x^2 + Q3 x^3 1 < = x < 2 which ensures function is continuous in value and derivative ( slope ) . ``",https://github.com/svn2github/ImageMagick/blob/5d50b26bdb9dd008db4cc8f1b128e8bed7906843/MagickCore/accelerate-private.h#L2067,C,yes,journal,algorithm,method,reference,machine learning,formulas to source code,no
"`` colorquant2.c Modified median cut color quantization High level PIX pixMedianCutQuant ( ) PIX pixMedianCutQuantGeneral ( ) PIX pixMedianCutQuantMixed ( ) PIX pixFewColorsMedianCutQuantMixed ( ) Median cut indexed histogram l_int32 pixMedianCutHisto ( ) Static helpers static PIXCMAP pixcmapGenerateFromHisto ( ) static PIX pixQuantizeWithColormap ( ) static void getColorIndexMedianCut ( ) static L_BOX3D pixGetColorRegion ( ) static l_int32 medianCutApply ( ) static PIXCMAP pixcmapGenerateFromMedianCuts ( ) static l_int32 vboxGetAverageColor ( ) static l_int32 vboxGetCount ( ) static l_int32 vboxGetVolume ( ) static L_BOX3D box3dCreate ( ) ; static L_BOX3D box3dCopy ( ) ; Paul Heckbert published the median cut algorithm , `` Color Image Quantization for Frame Buffer Display , '' in Proc . SIGGRAPH '82 , Boston , July 1982 , pp . 297-307 . A copy of the paper without figures can be found on the web . Median cut starts with either the full color space or the occupied region of color space . If you 're not dithering , the occupied region can be used , but with dithering , pixels can end up in any place in the color space , so you must represent the entire color space in the final colormap . Color components are quantized to typically 5 or 6 significant bits ( for each of r , g and b ) . Call a 3D region of color space a 'vbox ' . Any color in this quantized space is represented by an element of a linear histogram array , indexed by rgb value . The initial region is then divided into two regions that have roughly equal pixel occupancy ( hence the name `` median cut '' ) . Subdivision continues until the requisite number of vboxes has been generated . But the devil is in the details of the subdivision process . Here are some choices that you must make : ( 1 ) Along which axis to subdivide ? ( 2 ) Which box to put the bin with the median pixel ? ( 3 ) How to order the boxes for subdivision ? ( 4 ) How to adequately handle boxes with very small numbers of pixels ? ( 5 ) How to prevent a little-represented but highly visible color from being masked out by other colors in its vbox . Taking these in order : ( 1 ) Heckbert suggests using either the largest vbox side , or the vbox side with the largest variance in pixel occupancy . We choose to divide based on the largest vbox side . ( 2 ) Suppose you 've chosen a side . Then you have a histogram of pixel occupancy in 2D slices of the vbox . One of those slices includes the median pixel . Suppose there are L bins to the left ( smaller index ) and R bins to the right . Then this slice ( or bin ) should be assigned to the box containing the smaller of L and R. This both shortens the larger of the subdivided dimensions and helps a low-count color far from the subdivision boundary to better express itself . ( 2a ) One can also ask if the boundary should be moved even farther into the longer side . This is feasable if we have a method for doing extra subdivisions on the high count vboxes . And we do ( see ( 3 ) ) . ( 3 ) To make sure that the boxes are subdivided toward equal occupancy , use an occupancy-sorted priority queue , rather than a simple queue . ( 4 ) With a priority queue , boxes with small number of pixels wo n't be repeatedly subdivided . This is good . ( 5 ) Use of a priority queue allows tricks such as in ( 2a ) to let small occupancy clusters be better expressed . In addition , rather than splitting near the median , small occupancy colors are best reproduced by cutting half-way into the longer side . However , serious problems can arise with dithering if a priority queue is used based on population alone . If the picture has large regions of nearly constant color , some vboxes can be very large and have a sizeable population ( but not big enough to get to the head of the queue ) . If one of these large , occupied vboxes is near in color to a nearly constant color region of the image , dithering can inject pixels from the large vbox into the nearly uniform region . These pixels can be very far away in color , and the oscillations are highly visible . To prevent this , we can take either or both of these actions : ( 1 ) Subdivide a fraction ( < 1.0 ) based on population , and do the rest of the subdivision based on the product of the vbox volume and its population . By using the product , we avoid further subdivision of nearly empty vboxes , and directly target large vboxes with significant population . ( 2 ) Threshold the excess color transferred in dithering to neighboring pixels . Doing either of these will stop the most annoying oscillations in dithering . Furthermore , by doing ( 1 ) , we also improve the rendering of regions of nearly constant color , both with and without dithering . It turns out that the image quality is not sensitive to the value of the parameter in ( 1 ) ; values between 0.3 and 0.9 give very good results . Here 's the lesson : subdivide the color space into vboxes such that ( 1 ) the most populated vboxes that can be further subdivided ( i.e. , that occupy more than one quantum volume in color space ) all have approximately the same population , and ( 2 ) all large vboxes have no significant population . If these conditions are met , the quantization will be excellent . Once the subdivision has been made , the colormap is generated , with one color for each vbox and using the average color in the vbox . At the same time , the histogram array is converted to an inverse colormap table , storing the colormap index in every cell in the vbox . Finally , using both the colormap and the inverse colormap , a colormapped pix is quickly generated from the original rgb pix . In the present implementation , subdivided regions of colorspace that are not occupied are retained , but not further subdivided . This is required for our inverse colormap lookup table for dithering , because dithered pixels may fall into these unoccupied regions . For such empty regions , we use the center as the rgb colormap value . This variation on median cut can be referred to as `` Modified Median Cut '' quantization , or MMCQ . Overall , the undithered MMCQ gives comparable results to the two-pass Octcube Quantizer ( OQ ) . Comparing the two methods on the test24.jpg painting , we see : ( 1 ) For rendering spot color ( the various reds and pinks in the image ) , MMCQ is not as good as OQ . ( 2 ) For rendering majority color regions , MMCQ does a better job of avoiding posterization . That is , it does better dividing the color space up in the most heavily populated regions. ``",https://github.com/ONLYOFFICE/core/blob/50ff5fc0b2eb016209d3ae3b06c090b7f4f791d0/DesktopEditor/raster/JBig2/source/LeptonLib/colorquant2.cpp#L16,C++,yes,conference,algorithm,file,reference,other,description to source code,no
"`` SECTION : element-retinex Basic and multiscale retinex for colour image enhancement , see article : Rahman , Zia-ur , Daniel J. Jobson , and Glenn A. Woodell . `` Multi-scale retinex for color image enhancement . '' Image Processing , 1996 . Proceedings. , International Conference on . Vol . 3 . IEEE , 1996 . < refsect2 > < title > Example launch line < title > | [ gst-launch-1.0 videotestsrc decodebin videoconvert retinex videoconvert xvimagesink ] | < refsect2 > ``",https://github.com/GrokImageCompression/gst-plugins-bad/blob/4e239a6632ca648e3bf39b120e2d15822e5540e4/ext/opencv/gstretinex.cpp#L44,C,404,,,,,,,
"`` Find zero of an univariate function f. @ param { function } f Function , whose root is to be found @ param { Array , Number } x0 Start value or start interval enclosing the root @ param { Object } object Parent object in case f is method of it @ returns { Number } the approximation of the root Algorithm : G.Forsythe , M.Malcolm , C.Moler , Computer methods for mathematical computations . M. , Mir , 1980 , p.180 of the Russian edition If x0 is an array containing lower and upper bound for the zero algorithm 748 is applied . Otherwise , if x0 is a number , the algorithm tries to bracket a zero of f starting from x0 . If this fails , we fall back to Newton 's method . @ memberof JXG.Math.Numerics ``",https://github.com/jsxgraph/jsxgraph/blob/6c153a5686a93f98e3e59e50851c08e7ee63e13a/JSXCompressor/jsxgraphcore.js#L7229,JavaScript,yes,book,algorithm,method,reference,other,paper not available,no
"`` Functor to threshold on Harris corner measure proposed in : I. Laptev . On space-time interest points . Int . J . Computer Vision , 64 ( 2 ) :107-123 , 2005 ''",https://github.com/vxl/vxl/blob/d036598b083ad5d7ab7d62bc1b610121ae81445d/contrib/brl/bseg/bvpl/bvpl_octree/bvpl_corner_functors.h#L18,C++,yes,journal,algorithm,class,reference,computer vision,formulas to source code,no
"`` file Broyden.C This is an example of using NOX with the NOX : :Solver : :TensorBased tensor-Krylov method . This test problem is a modified extension of the `` Broyden Tridiagonal Problem '' from Jorge J . More ' , Burton S. Garbow , and Kenneth E. Hillstrom , Testing Unconstrained Optimization Software , ACM TOMS , Vol . 7 , No . 1 , March 1981 , pp . 14-41 . The modification involves squaring the last equation fn ( x ) and using it in a homotopy-type equation . The parameter `` lambda '' is a homotopy-type parameter that may be varied from 0 to 1 to adjust the ill-conditioning of the problem . A value of 0 is the original , unmodified problem , while a value of 1 is that problem with the last equation squared . Typical values for increasingly ill-conditioned problems might be 0.9 , 0.99 , 0.999 , etc . The standard starting point is x ( i ) = -1 , but setting x ( i ) = 0 tests the selected global strategy . author Brett Bader , CU Boulder , 2002 ``",https://github.com/trilinos/Trilinos/blob/d1a98e6faf63065bee9155d63ad18248d3d32ad8/packages/nox/examples/lapack/NOX_SimpleExamples/Broyden.C#L51,C++,yes,book,algorithm,file,reference,science,paper not available,no
"`` function igraph_cliques brief Find all or some cliques in a graph < para > < para > Cliques are fully connected subgraphs of a graph . < para > < para > If you are only interested in the size of the largest clique in the graph , use ref igraph_clique_number ( ) instead . < para > < para > The current implementation of this function searches for maximal independent vertex sets ( see ref igraph_maximal_independent_vertex_sets ( ) ) in the complementer graph using the algorithm published in : S. Tsukiyama , M. Ide , H. Ariyoshi and I. Shirawaka . A new algorithm for generating all the maximal independent sets . SIAM J Computing , 6:505 -- 517 , 1977. param graph The input graph . param res Pointer to a pointer vector , the result will be stored here , ie . c res will contain pointers to c igraph_vector_t objects which contain the indices of vertices involved in a clique . The pointer vector will be resized if needed but note that the objects in the pointer vector will not be freed . param min_size Integer giving the minimum size of the cliques to be returned . If negative or zero , no lower bound will be used . param max_size Integer giving the maximum size of the cliques to be returned . If negative or zero , no upper bound will be used . return Error code . sa ref igraph_largest_cliques ( ) and ref igraph_clique_number ( ) . Time complexity : TODO example examples simple igraph_cliques.c ``",https://github.com/igraph/igraph/blob/ef2b26e2cbe8eaa182810a0b71e8bba2ad070c1b/src/cliques.c#L291,C,yes,journal,background,n/a,related,networks and os,no transfer,no
"`` Copyright ( c ) 2014 : Steinbeis Forschungszentrum ( STZ √ñlbronn ) Author : Michael Hoffer This file is part of UG4 . UG4 is free software : you can redistribute it and or modify it under the terms of the GNU Lesser General Public License version 3 ( as published by the Free Software Foundation ) with the following additional attribution requirements ( according to LGPL GPL v3 ¬ß7 ) : ( 1 ) The following notice must be displayed in the Appropriate Legal Notices of covered and combined works : `` Based on UG4 ( www.ug4.org license ) '' . ( 2 ) The following notice must be displayed at a prominent place in the terminal output of covered works : `` Based on UG4 ( www.ug4.org license ) '' . ( 3 ) The following bibliography is recommended for citation and must be preserved in all covered files : `` Reiter , S. , Vogel , A. , Heppner , I. , Rupp , M. , and Wittum , G. A massively parallel geometric multigrid solver on hierarchically distributed grids . Computing and visualization in science 16 , 4 ( 2013 ) , 151-164 '' `` Vogel , A. , Reiter , S. , Rupp , M. , N√§gel , A. , and Wittum , G. UG4 -- a novel flexible software system for simulating pde based models on high performance computers . Computing and visualization in science 16 , 4 ( 2013 ) , 165-179 '' This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU Lesser General Public License for more details. ``",https://github.com/UG4/ugcore/blob/fd783d5abfa67f0bbaae380cd52cbb2beff8e0d4/ugbase/bindings/vrl/vrl_bridge.cpp#L1,C++,yes,journal,background,n/a,related,simulation,no transfer,yes
"`` Author : Dave Coleman Desc : Implementation of the Lightning Framework for experienced-based planning Paper : Berenson , Dmitry , Pieter Abbeel , and Ken Goldberg . `` A robot path planning framework that learns from experience . '' Robotics and Automation ( ICRA ) , 2012 IEEE International Conference on . IEEE , 2012 . Notes : The user of this class should invoke the loading and saving from file , otherwise experiences will be lost. ``",https://github.com/ompl/ompl/blob/7f57fc5e5ed6f839108eb7b4e270c81f38d27d90/src/ompl/tools/lightning/Lightning.h#L35,C++,yes,conference,background,n/a,related,other,no transfer,no
"`` @ file sf-refs References and Further Reading -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- The library follows the conventions of Abramowitz & Stegun where possible , Abramowitz & Stegun ( eds . ) , Handbook of Mathematical Functions The following papers contain information on the algorithms used to compute the special functions , Allan J. MacLeod , MISCFUN : A software package to compute uncommon special functions . ACM Trans . Math . Soft . , vol . 22 , 1996 , 288-301 G.N . Watson , A Treatise on the Theory of Bessel Functions , 2nd Edition ( Cambridge University Press , 1944 ) . G. Nemeth , Mathematical Approximations of Special Functions , Nova Science Publishers , ISBN 1-56072-052-2 B.C . Carlson , Special Functions of Applied Mathematics ( 1977 ) N. M. Temme , Special Functions : An Introduction to the Classical Functions of Mathematical Physics ( 1996 ) , ISBN 978-0471113133 . W.J . Thompson , Atlas for Computing Mathematical Functions , John Wiley & Sons , New York ( 1997 ) . Y.Y . Luke , Algorithms for the Computation of Mathematical Functions , Academic Press , New York ( 1977 ) . ``",https://github.com/ampl/mp/blob/a8825f720395586e48701ddf6833a488ece0a310/src/gsl/amplgsl.cc#L5387,C++,yes,book,background,n/a,related,science,no transfer,no
"`` AODE achieves highly accurate classification by averaging over all of a small space of alternative naive-Bayes-like models that have weaker ( and hence less detrimental ) independence assumptions than naive Bayes . The resulting algorithm is computationally efficient while delivering highly accurate classification on many learning tasks. < br > For more information , see < p > G. Webb , J. Boughton & Z. Wang ( 2004 ) . < i > Not So Naive Bayes. < i > To be published in Machine Learning. < br > G. Webb , J. Boughton & Z. Wang ( 2002 ) . < i > Averaged One-Dependence Estimators : Preliminary Results. < i > AI2002 Data Mining Workshop , Canberra . Valid options are : < p > -D < br > Debugging information is printed if this flag is specified. < p > -F < br > Specify the frequency limit for parent attributes. < p > @ author Janice Boughton ( jrbought @ csse.monash.edu.au ) & Zhihai Wang ( zhw @ csse.monash.edu.au ) @ version $ Revision : 1.8.2.3 $ this version resolves errors in the handling of missing attribute values. ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/stable-3-4-10/weka/classifiers/bayes/AODE.java#L33,Java,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` `` '' '' Non-parametric cluster-level test for spatio-temporal data . This function provides a convenient wrapper for data organized in the form ( observations x time x space ) to use : func : ` mne.stats.permutation_cluster_test ` . See [ 1 ] _ for more information . Parameters -- -- -- -- -- X : list of arrays List of data arrays , shape `` ( n_observations , n_times , n_vertices ) `` in each group . threshold : float | dict | None If threshold is None , it will choose an F-threshold equivalent to p < 0.05 for the given number of observations ( only valid when using an F-statistic ) . If a dict is used , then threshold-free cluster enhancement ( TFCE ) will be used , and it must have keys `` 'start ' `` and `` 'step ' `` to specify the integration parameters , see the : ref : ` TFCE example < tfce_example > ` . n_permutations : int See permutation_cluster_test . tail : -1 or 0 or 1 ( default = 0 ) See permutation_cluster_test . stat_fun : callable | None Function called to calculate statistics , must accept 1d-arrays as arguments ( default None uses : func : ` mne.stats.f_oneway ` ) . connectivity : sparse matrix or None Defines connectivity between features . The matrix is assumed to be symmetric and only the upper triangular half is used . Default is None , i.e , a regular lattice connectivity . verbose : bool , str , int , or None If not None , override default verbose level ( see : func : ` mne.verbose ` and : ref : ` Logging documentation < tut_logging > ` for more ) . n_jobs : int Number of permutations to run in parallel ( requires joblib package ) . seed : int | instance of RandomState | None Seed the random number generator for results reproducibility . max_step : int When connectivity is a n_vertices x n_vertices matrix , specify the maximum number of steps between vertices along the second dimension ( typically time ) to be considered connected . This is not used for full or None connectivity matrices . spatial_exclude : list of int or None List of spatial indices to exclude from clustering . step_down_p : float To perform a step-down-in-jumps test , pass a p-value for clusters to exclude from each successive iteration . Default is zero , perform no step-down test ( since no clusters will be smaller than this value ) . Setting this to a reasonable value , e.g . 0.05 , can increase sensitivity but costs computation time . t_power : float Power to raise the statistical values ( usually F-values ) by before summing ( sign will be retained ) . Note that t_power == 0 will give a count of nodes in each cluster , t_power == 1 will weight each node by its statistical score . out_type : str For arrays with connectivity , this sets the output format for clusters . If 'mask ' , it will pass back a list of boolean mask arrays . If 'indices ' , it will pass back a list of lists , where each list is the set of vertices in a given cluster . Note that the latter may use far less memory for large datasets . check_disjoint : bool If True , the connectivity matrix ( or list ) will be examined to determine of it can be separated into disjoint sets . In some cases ( usually with connectivity as a list and many `` time '' points ) , this can lead to faster clustering , but results should be identical . buffer_size : int or None The statistics will be computed for blocks of variables of size `` buffer_size '' at a time . This is option significantly reduces the memory requirements when n_jobs > 1 and memory sharing between processes is enabled ( see set_cache_dir ( ) ) , as X will be shared between processes and each process only needs to allocate space for a small block of variables . Returns -- -- -- - t_obs : array , shape ( n_times n_vertices , ) Statistic ( t by default ) observed for all variables clusters : list List type defined by out_type above . cluster_pv : array P-value for each cluster H0 : array , shape ( n_permutations , ) Max cluster level stats observed under permutation . References -- -- -- -- -- .. [ 1 ] Maris Oostenveld ( 2007 ) , `` Nonparametric statistical testing of EEG- and MEG-data '' , Journal of Neuroscience Methods , Vol . 164 , No . 1. , pp . 177-190. doi:10.1016 j.jneumeth.2007.03.024. `` '' '' ''",https://github.com/mne-tools/mne-python/blob/424e768ea43fac9e4f9adf920ee30d0d5adaf949/mne/stats/cluster_level.py#L1348,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` Function below reused from A.V . Aho , R. Sethi , J.D . Ullman , `` Compilers - Principle , Techniques , and Tools '' , Addison-Wesley , 1986 , pp 435-437 , and in turn from P.J . Weineberger 's C compiler. ``",https://github.com/HomerReid/scuff-em/blob/964af2454703ef5229785657b91fe1581cf7bbdb/src/libs/libMatProp/cmatheval/symbol_table.c#L275,C,yes,book,algorithm,method,reference,science,formulas to source code,no
"`` sqrt { x y } equation ( 14 ) . see W. Gautschi , J. Slavik , On the Computation of Modified Bessel Function Ratios , Mathematics of Computation , Vol . 32 , No . 143 ( Jul. , 1978 ) , pp . 865-875. for information on using continued fractions to compute the ratio directly. ``",https://github.com/skymoo/lalsuite/blob/8cbd1b7187ce3ed9a825d6ed11cc432f3cfde9a5/lal/src/stats/XLALMarcumQ.c#L105,C,404,,,,,,,
"`` < -- globalinfo-start -- > Converts the given set of predictor variables into a kernel matrix . The class value remains unchangedm , as long as the preprocessing filter does n't change it. < br > By default , the data is preprocessed with the Center filter , but the user can choose any filter ( NB : one must be careful that the filter does not alter the class attribute unintentionally ) . With weka.filters.AllFilter the preprocessing gets disabled. < br > < br > For more information regarding preprocessing the data , see : < br > < br > K.P . Bennett , M.J. Embrechts : An Optimization Perspective on Kernel Partial Least Squares Regression . In : Advances in Learning Theory : Methods , Models and Applications , 227-249 , 2003 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; inproceedings { Bennett2003 , author = { K.P . Bennett and M.J. Embrechts } , booktitle = { Advances in Learning Theory : Methods , Models and Applications } , editor = { J. Suykens et al . } , pages = { 227-249 } , publisher = { IOS Press , Amsterdam , The Netherlands } , series = { NATO Science Series , Series III : Computer and System Sciences } , title = { An Optimization Perspective on Kernel Partial Least Squares Regression } , volume = { 190 } , year = { 2003 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D Turns on output of debugging information. < pre > < pre > -no-checks Turns off all checks - use with caution Turning them off assumes that data is purely numeric , does n't contain any missing values , and has a nominal class . Turning them off also means that no header information will be stored if the machine is linear . Finally , it also assumes that no instance has a weight equal to 0 . ( default : checks on ) < pre > < pre > -F & lt ; filename & gt ; The file to initialize the filter with ( optional ) . < pre > < pre > -C & lt ; num & gt ; The class index for the file to initialize with , First and last are valid ( optional , default : last ) . < pre > < pre > -K & lt ; classname and parameters & gt ; The Kernel to use . ( default : weka.classifiers.functions.supportVector.PolyKernel ) < pre > < pre > -kernel-factor Defines a factor for the kernel . - RBFKernel : a factor for gamma Standardize : 1 ( 2 N ) Normalize.. : 6 N Available parameters are : N for of instances , A for of attributes ( default : 1 ) < pre > < pre > -P & lt ; classname and parameters & gt ; The Filter used for preprocessing ( use weka.filters.AllFilter to disable preprocessing ) . ( default : weka.filters.unsupervised.attribute.Center ) < pre > < pre > Options specific to kernel weka.classifiers.functions.supportVector.PolyKernel : < pre > < pre > -D Enables debugging output ( if available ) to be printed . ( default : off ) < pre > < pre > -no-checks Turns off all checks - use with caution ( default : checks on ) < pre > < pre > -C & lt ; num & gt ; The size of the cache ( a prime number ) , 0 for full cache and -1 to turn it off . ( default : 250007 ) < pre > < pre > -E & lt ; num & gt ; The Exponent to use . ( default : 1.0 ) < pre > < pre > -L Use lower-order terms . ( default : no ) < pre > < pre > Options specific to preprocessing filter weka.filters.unsupervised.attribute.Center : < pre > < pre > -unset-class-temporarily Unsets the class index temporarily before the filter is applied to the data . ( default : no ) < pre > < -- options-end -- > @ author Jonathan Miles ( jdm18 @ cs.waikato.ac.nz ) @ author FracPete ( fracpete at waikato dot ac dot nz ) @ version $ Revision : 1.2 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/beforeCupJflexExtraction_trunk/weka/src/main/java/weka/filters/unsupervised/attribute/KernelFilter.java#L55,Java,yes,book,algorithm,class,reference,machine learning,formulas to source code,no
"`` Computes ( approximate ) Beetweenness Centrality of all nodes and all edges of the network . To obtain exact betweenness values one needs to solve single-source shortest-path problem for every node . To speed up the algorithm we solve the shortest-path problem for the BtwNIdV subset of nodes . This gives centrality values that are about Graph- > GetNodes ( ) BtwNIdV.Len ( ) times lower than the exact betweenness centrality valus . See `` A Faster Algorithm for Beetweenness Centrality '' , Ulrik Brandes , Journal of Mathematical Sociology , 2001 , and `` Centrality Estimation in Large Networks '' , Urlik Brandes and Christian Pich , 2006 for more details . ''",https://github.com/qminer/qminer/blob/c62b31c502994450636788c30dc952c18cb08340/src/third_party/Snap/snap-core/centr.h#L46,C++,yes,journal,background,n/a,related,data science,no transfer,no
"`` Mathlib : A C Library of Special Functions Copyright ( C ) 1998 Ross Ihaka Copyright ( C ) 2000-2011 The R Core Team This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation ; either version 2 of the License , or ( at your option ) any later version . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . You should have received a copy of the GNU General Public License along with this program ; if not , a copy is available at http : www.r-project.org Licenses SYNOPSIS include < Rmath.h > double rpois ( double lambda ) DESCRIPTION Random variates from the Poisson distribution . REFERENCE Ahrens , J.H . and Dieter , U . ( 1982 ) . Computer generation of Poisson deviates from modified normal distributions . ACM Trans . Math . Software 8 , 163-179. ``",https://github.com/statgen/gotcloud/blob/a3b344e81e4475ade7dce593ecaa68fd2c15e2f2/src/vt/lib/include/Rmath/rpois.c#L1,C++,yes,journal,algorithm,file,reference,science,pseudocode to source code,no
"`` PURPOSE Identify new text as repetitions of old text within a fixed- length sliding window trailing behind the new text . DISCUSSION The `` deflation '' process depends on being able to identify portions of the input text which are identical to earlier input ( within a sliding window trailing behind the input currently being processed ) . The most straightforward technique turns out to be the fastest for most input files : try all possible matches and select the longest . The key feature of this algorithm is that insertions into the string dictionary are very simple and thus fast , and deletions are avoided completely . Insertions are performed at each input character , whereas string matches are performed only when the previous match ends . So it is preferable to spend more time in matches to allow very fast string insertions and avoid deletions . The matching algorithm for small strings is inspired from that of Rabin & Karp . A brute force approach is used to find longer strings when a small match has been found . A similar algorithm is used in comic ( by Jan-Mark Wams ) and freeze ( by Leonid Broukhis ) . A previous version of this file used a more sophisticated algorithm ( by Fiala and Greene ) which is guaranteed to run in linear amortized time , but has a larger average cost , uses more memory and is patented . However the F & G algorithm may be faster for some highly redundant files if the parameter max_chain_length ( described below ) is too large . ACKNOWLEDGEMENTS The idea of lazy evaluation of matches is due to Jan-Mark Wams , and I found it in 'freeze ' written by Leonid Broukhis . Thanks to many info-zippers for bug reports and testing . REFERENCES APPNOTE.TXT documentation file in PKZIP 1.93a distribution . A description of the Rabin and Karp algorithm is given in the book `` Algorithms '' by R. Sedgewick , Addison-Wesley , p252 . Fiala , E.R. , and Greene , D.H . Data Compression with Finite Windows , Comm.ACM , 32,4 ( 1989 ) 490-595 INTERFACE void lm_init ( int pack_level , ush flags ) Initialize the `` longest match '' routines for a new file off_t deflate ( void ) Processes a new input file and return its compressed length . Sets the compressed length , crc , deflate flags and internal file attributes. ``",https://github.com/codespecs/daikon/blob/ec4f7b13a54d55d6ed7259820f7f149b5d9633c5/tests/kvasir-tests/gzip/deflate.c#L7,C,yes,other(documentation),algorithm,file,reference,other,description to source code,no
"`` References : Geometric Tools for Computer Graphics , Philip Schneider , David Eberly . 2003 . Ear clipping for triangulation is described in Chapter 13 on Polygon Partitioning . Also described in a small article `` Triangulation by Ear Clipping '' , David Eberly , http : www.geometrictools.com ''",https://github.com/NREL/EnergyPlus/blob/b89c060c96ebac032d8d93647e94cf73e8cac8bc/src/EnergyPlus/DXFEarClipping.cc#L561,C++,yes,book,algorithm,method,reference,simulation,formulas to source code,no
"`` Class to apply spectral or spatial FFT . If both spectral and spatial transforms are required , then should be called multiple timw with SetFFTDomain set appropriately . Based on reconstruction methods developed by Sarah J. Nelson , Ph.D ( UCSF ) . 1 . Nelson S.J , `` Analysis of volume MRI and MR spectroscopic imaging data for the evaluation of patients with brain tumors '' , Magnetic Resonance in Medicine , 46 ( 2 ) , p228-239 ( 2001 ) . In development ``",https://github.com/SIVICLab/sivic/blob/55b5cf3097d7166fdaed320464cc9192dc759521/libs/src/svkMrsImageFFT.h#L67,C++,yes,journal,background,n/a,related,other,no transfer,no
"`` CHOKe stateless AQM for fair bandwidth allocation ================================================= CHOKe ( CHOose and Keep for responsive flows , CHOose and Kill for unresponsive flows ) is a variant of RED that penalizes misbehaving flows but maintains no flow state . The difference from RED is an additional step during the enqueuing process . If average queue size is over the low threshold ( qmin ) , a packet is chosen at random from the queue . If both the new and chosen packet are from the same flow , both are dropped . Unlike RED , CHOKe is not really a `` classful '' qdisc because it needs to access packets in queue randomly . It has a minimal class interface to allow overriding the builtin flow classifier with filters . Source : R. Pan , B. Prabhakar , and K. Psounis , `` CHOKe , A Stateless Active Queue Management Scheme for Approximating Fair Bandwidth Allocation '' , IEEE INFOCOM , 2000 . A. Tang , J. Wang , S. Low , `` Understanding CHOKe : Throughput and Spatial Characteristics '' , IEEE ACM Transactions on Networking , 2004 ``",https://github.com/Tomoms/helium_kernel/blob/a20b5d0758d5308803394385a8eb753b6b24d345/net/sched/sch_choke.c#L23,C,yes,conference,algorithm,file,reference,networks and os,formulas to source code,no
"`` - - Mode : python ; tab-width : 4 ; indent-tabs-mode : nil ; coding : utf-8 - - vim : tabstop=4 expandtab shiftwidth=4 softtabstop=4 fileencoding=utf-8 MDAnalysis -- - http : www.mdanalysis.org Copyright ( c ) 2006-2017 The MDAnalysis Development Team and contributors ( see the file AUTHORS for the full list of names ) Released under the GNU Public Licence , v2 or any higher version Please cite your use of MDAnalysis in published work : R. J. Gowers , M. Linke , J. Barnoud , T. J. E. Reddy , M. N. Melo , S. L. Seyler , D. L. Dotson , J. Domanski , S. Buchoux , I. M. Kenney , and O. Beckstein . MDAnalysis : A Python package for the rapid analysis of molecular dynamics simulations . In S. Benthall and S. Rostrup editors , Proceedings of the 15th Python in Science Conference , pages 102-109 , Austin , TX , 2016 . SciPy . N. Michaud-Agrawal , E. J. Denning , T. B. Woolf , and O. Beckstein . MDAnalysis : A Toolkit for the Analysis of Molecular Dynamics Simulations . J. Comput . Chem . 32 ( 2011 ) , 2319 -- 2327 , doi:10.1002 jcc.21787 ``",https://github.com/MDAnalysis/mdanalysis/blob/bdb1844d17d1be6340452a25e95ed39d614d9edf/testsuite/MDAnalysisTests/coordinates/test_copying.py#L1,Python,yes,conference,background,n/a,related,simulation,no transfer,yes
"`` Following routines are variants of the algorithm described in `` Numerical recipes in C '' , Second Edition , Cambridge University Press , 1992 , Section 8.5 , ISBN 0-521-43108-5 Original code by Nicolas Devillard - 1998 . Public domain . Modified by G. Duvert , 2017 , for NaN INF handling and correction of Nicolas 's code which gave erroneous results when two or more elements were identical. ``",https://github.com/olebole/gnudatalanguage/blob/01f4aaebcb31bd5a2454ac23836fd54a21bdf7a5/src/basic_fun.cpp#L3892,C++,404,,,,,,,
"`` data information about this continuum : Lafferty , W.J. , A.M. Solodov , A . Weber , W.B . Olson and J._M . Hartmann , Infrared collision-induced absorption by N2 near 4.3 microns for atmospheric applications : Measurements and emprirical modeling , Appl . Optics , 35 , 5911-5917 , ( 1996 ) ''",https://github.com/olemke/arts/blob/a421111a8e5a220a9d422339511ed8855b770d1e/src/continua.cc#L19553,C++,yes,journal,algorithm,method,reference,simulation,paper not available,no
"`` brief Multi-objective optimization benchmark function LZ7 . The function is described in H. Li and Q. Zhang . Multiobjective Optimization Problems with Complicated Pareto Sets , MOEA D and NSGA-II , IEEE Trans on Evolutionary Computation , 2 ( 12 ) :284-302 , April 2009. ``",https://github.com/Shark-ML/Shark/blob/67990bcd2c4a90a27be97d933b3740931e9da141/include/shark/ObjectiveFunctions/Benchmarks/LZ7.h#L47,C++,yes,journal,background,n/a,related,machine learning,no transfer,no
"`` Low level class for the real description of water . The reference is W. Wagner , A. Pruss , `` The IAPWS Formulation 1995 for the Thermodynamic Properties of Ordinary Water Substance for General and Scientific Use , '' J. Phys . Chem . Ref . Dat , 31 , 387 , 2002 . Units Note : This class works with reduced units exclusively. ``",https://github.com/Cantera/cantera/blob/a9ad75e974b7d1aed642a23f5b58ff7bd27a2d8d/include/cantera/thermo/WaterPropsIAPWSphi.h#L21,C++,yes,journal,background,n/a,related,science,no transfer,no
"`` < -- globalinfo-start -- > The implementation of a RIpple-DOwn Rule learner. < br > < br > It generates a default rule first and then the exceptions for the default rule with the least ( weighted ) error rate . Then it generates the `` best '' exceptions for each exception and iterates until pure . Thus it performs a tree-like expansion of exceptions.The exceptions are a set of rules that predict classes other than the default . IREP is used to generate the exceptions. < br > < br > For more information about Ripple-Down Rules , see : < br > < br > Brian R. Gaines , Paul Compton ( 1995 ) . Induction of Ripple-Down Rules Applied to Modeling Large Databases . J. Intell . Inf . Syst.. 5 ( 3 ) :211-228 . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Gaines1995 , author = { Brian R. Gaines and Paul Compton } , journal = { J. Intell . Inf . Syst . } , number = { 3 } , pages = { 211-228 } , title = { Induction of Ripple-Down Rules Applied to Modeling Large Databases } , volume = { 5 } , year = { 1995 } , PDF = { http : pages.cpsc.ucalgary.ca ~gaines reports ML JIIS95 JIIS95.pdf } } < pre > < p > < -- technical-bibtex-end -- > There are five inner classes defined in this class . < br > The first is Ridor_node , which implements one node in the Ridor tree . It 's basically composed of a default class and a set of exception rules to the default class. < br > The second inner class is RidorRule , which implements a single exception rule using REP. < br > The last three inner classes are only used in RidorRule . They are Antd , NumericAntd and NominalAntd , which all implement a single antecedent in the RidorRule . < br > The Antd class is an abstract class , which has two subclasses , NumericAntd and NominalAntd , to implement the corresponding abstract functions . These two subclasses implement the functions related to a antecedent with a nominal attribute and a numeric attribute respectively. < p > < -- options-start -- > Valid options are : < p > < pre > -F & lt ; number of folds & gt ; Set number of folds for IREP One fold is used as pruning set . ( default 3 ) < pre > < pre > -S & lt ; number of shuffles & gt ; Set number of shuffles to randomize the data in order to get better rule . ( default 10 ) < pre > < pre > -A Set flag of whether use the error rate of all the data to select the default class in each step . If not set , the learner will only use the error rate in the pruning data < pre > < pre > -M Set flag of whether use the majority class as the default class in each step instead of choosing default class based on the error rate ( if the flag is not set ) < pre > < pre > -N & lt ; min . weights & gt ; Set the minimal weights of instances within a split . ( default 2.0 ) < pre > < -- options-end -- > @ author Xin XU ( xx5 @ cs.waikato.ac.nz ) @ version $ Revision : 1.20 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/before_revisionhandler/weka/classifiers/rules/Ridor.java#L48,Java,yes,journal,algorithm,class,reference,machine learning,formulas to source code,no
"`` page pg_md_basics MD Basics The molecular dynamics ( MD ) computer simulation method is a well established and important tool for the study of the dynamical properties of liquids , solids , and other systems of interest in Materials Science and Engineering , Chemistry and Biology . A material is represented in terms of atoms and molecules . The method of MD simulation involves the evaluation of the force acting on each atom due to all other atoms in the system and the numerical integration of the Newtonian equations of motion . Though MD was initially developed to compute the equilibrium thermodynamic behavior of materials ( equation of state ) , most recent applications have used MD to study non-equilibrium processes . Wikipeda offers a basic introduction to molecular dynamics with many references : http : en.wikipedia.org wiki Molecular_dynamics For a thorough treatment of MD methods , see : - `` Computer simulation of liquids '' by M.P . Allen and D.J . Tildesley ( Oxford , 1989 ) ISBN-10 : 0198556454 | ISBN-13 : 978-0198556459 . For an understanding of MD simulations and application to statistical mechanics : - `` Understanding Molecular Simulation , Second Edition : From Algorithms to Applications , '' by D. Frenkel and B. Smit ( Academic Press , 2001 ) ISBN-10 : 0122673514 | ISBN-13 : 978-0122673511 - `` Statistical and Thermal Physics : With Computer Applications , '' by H. Gould and J. Tobochnik ( Princeton , 2010 ) ISBN-10 : 0691137447 | ISBN-13 : 978-0691137445 CoMD implements both the Lennard-Jones Potential ( ljForce.c ) and the Embedded Atom Method Potential ( eam.c ) . ``",https://github.com/sstsimulator/sst-macro/blob/63833b12797bf3c4d4e203c96bd8fbc1cb042a7b/skeletons/CoMD/src/CoMD.c#L1073,C++,yes,book,background,n/a,related,simulation,no transfer,no
"`` The algorithm is very close to that in `` Implementing the complex arcsine and arccosine functions using exception handling '' by T. E. Hull , Thomas F. Fairgrieve , and Ping Tak Peter Tang , published in ACM Transactions on Mathematical Software , Volume 23 Issue 3 , 1997 , Pages 299-335 , http : dl.acm.org citation.cfm ? id=275324 . Throughout we use the convention z = x + I y. casinh ( z ) = sign ( x ) log ( A+sqrt ( A A-1 ) ) + I asin ( B ) where A = ( |z+I| + |z-I| ) 2 B = ( |z+I| - |z-I| ) 2 = y A These formulas become numerically unstable : ( a ) for Re ( casinh ( z ) ) when z is close to the line segment [ -I , I ] ( that is , Re ( casinh ( z ) ) is close to 0 ) ; ( b ) for Im ( casinh ( z ) ) when z is close to either of the intervals [ I , I infinity ) or ( -I infinity , -I ] ( that is , |Im ( casinh ( z ) ) | is close to PI 2 ) . These numerical problems are overcome by defining f ( a , b ) = ( hypot ( a , b ) - b ) 2 = a a ( hypot ( a , b ) + b ) 2 Then if A < A_crossover , we use log ( A + sqrt ( A A-1 ) ) = log1p ( ( A-1 ) + sqrt ( ( A-1 ) ( A+1 ) ) ) A-1 = f ( x , 1+y ) + f ( x , 1-y ) and if B > B_crossover , we use asin ( B ) = atan2 ( y , sqrt ( A A - y y ) ) = atan2 ( y , sqrt ( ( A+y ) ( A-y ) ) ) A-y = f ( x , y+1 ) + f ( x , y-1 ) where without loss of generality we have assumed that x and y are non-negative . Much of the difficulty comes because the intermediate computations may produce overflows or underflows . This is dealt with in the paper by Hull et al by using exception handling . We do this by detecting when computations risk underflow or overflow . The hardest part is handling the underflows when computing f ( a , b ) . Note that the function f ( a , b ) does not appear explicitly in the paper by Hull et al , but the idea may be found on pages 308 and 309 . Introducing the function f ( a , b ) allows us to concentrate many of the clever tricks in this paper into one function. ``",https://github.com/BarrelfishOS/barrelfish/blob/5b5e93a13a7ed4ec53bd8613b15674ed948b2efe/lib/msun/src/catrig.c#L84,C,yes,journal,background,n/a,related,networks and os,no transfer,no
"`` `` '' '' Tie correction factor for ties in the Mann-Whitney U and Kruskal-Wallis H tests . Parameters -- -- -- -- -- rankvals : array_like A 1-D sequence of ranks . Typically this will be the array returned by ` stats.rankdata ` . Returns -- -- -- - factor : float Correction factor for U or H. See Also -- -- -- -- rankdata : Assign ranks to the data mannwhitneyu : Mann-Whitney rank test kruskal : Kruskal-Wallis H test References -- -- -- -- -- .. [ 1 ] Siegel , S. ( 1956 ) Nonparametric Statistics for the Behavioral Sciences . New York : McGraw-Hill . Examples -- -- -- -- > > > from scipy.stats import tiecorrect , rankdata > > > tiecorrect ( [ 1 , 2.5 , 2.5 , 4 ] ) 0.9 > > > ranks = rankdata ( [ 1 , 3 , 2 , 4 , 5 , 7 , 2 , 8 , 4 ] ) > > > ranks array ( [ 1. , 4. , 2.5 , 5.5 , 7. , 8. , 2.5 , 9. , 5.5 ] ) > > > tiecorrect ( ranks ) 0.9833333333333333 `` '' '' ''",https://github.com/metamorph-inc/meta-core/blob/771af3b12e17e01c16b32c61649cb16d826fd5d9/bin/Python27/Lib/site-packages/scipy/stats/stats.py#L4578,Python,yes,book,algorithm,method,reference,other,paper not available,no
"`` Returns the viscosity of water at the current conditions ( kg m s ) This function calculates the value of the viscosity of pure water at the current T and P. The formulas used are from the paper : J. V. Sengers , J. T. R. Watson , `` Improved International Formulations for the Viscosity and Thermal Conductivity of Water Substance '' , J. Phys . Chem . Ref . Data , 15 , 1291 ( 1986 ) . The formulation is accurate for all temperatures and pressures , for steam and for water , even near the critical point . Pressures above 500 MPa and temperature above 900 C are suspect. ``",https://github.com/Cantera/cantera/blob/a9ad75e974b7d1aed642a23f5b58ff7bd27a2d8d/include/cantera/transport/WaterTransport.h#L47,C++,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` PTRHASH -- hash a pointer value Parameters : p -- pointer . Returns : hash value . ptrhash hashes a pointer value to a uniformly distributed random number between 0 and 255 . This hash algorithm is based on Peter K. Pearson , `` Fast Hashing of Variable-Length Text Strings '' , in Communications of the ACM , June 1990 , vol 33 no 6. ``",https://github.com/opnsense/src/blob/bdf43282b19766c6cea95108ec77e2e654f7e701/contrib/sendmail/libsm/heap.c#L286,C,yes,journal,algorithm,method,reference,networks and os,pseudocode to source code,no
"`` Class for manipulating chi-square mixture distributions . < p > REFERENCES < p > Wang , Y . ( 2000 ) . `` A new approach to fitting linear models in high dimensional spaces . '' PhD Thesis . Department of Computer Science , University of Waikato , New Zealand . < p > Wang , Y. and Witten , I. H. ( 2002 ) . `` Modeling for optimal probability prediction . '' Proceedings of ICML'2002 . Sydney . < p > @ author Yong Wang ( yongwang @ cs.waikato.ac.nz ) @ version $ Revision : 1.2 $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-5-2/weka/classifiers/functions/pace/ChisqMixture.java#L29,Java,yes,other (thesis),algorithm,class,reference,machine learning,formulas to source code,no
"`` calculation based on : Z. Wang , A. C. Bovik , H. R. Sheikh and E. P. Simoncelli , `` Image quality assessment : From error visibility to structural similarity , '' IEEE Transactions on Image Processing , vol . 13 , no . 4 , pp . 600-612 , Apr . 2004 . ''",https://github.com/OGRECave/ogre/blob/1a49dfd48a18dfce36db67f3b3a1fc2db748aca8/Tests/VisualTests/Common/include/ImageValidator.h#L179,C++,yes,journal,algorithm,method,reference,other,formulas to source code,no
"`` file Rosenbrock.C brief A simple 2D example using % Rosenbrock 's function based on NOX : :Example This is an example of using % NOX with the NOX : :LAPACK : :Group and NOX : :LAPACK : :Vector classes . These are very basic classes intended only to illustrate and test NOX . They are based on a combination of C++ STL and LAPACK . This example is the `` % Rosenbrock function '' from Jorge J . More ' , Burton S. Garbow , and Kenneth E. Hillstrom , Testing Unconstrained Optimization Software , ACM TOMS , Vol . 7 , No . 1 , March 1981 , pp . 14-41 . It comes originally from H. H. % Rosenbrock , An Automatic Method for Finding the Greatest or Least Value of a Function , J. Comput . 3 ( 1960 ) :175-184 . The function is defined as f [ F ( x ) = left [ begin { array } { c } 10 ( x [ 2 ] - x [ 1 ] ^2 ) 1 - x [ 1 ] end { array } right ] f ] The initial guess is given by f [ x_0 = left [ begin { array } { c } -1.2 1 end { array } right ] f ] The solution is f [ x_ = left [ begin { array } { c } 1 1 end { array } right ] f ] ``",https://github.com/trilinos/Trilinos/blob/d1a98e6faf63065bee9155d63ad18248d3d32ad8/packages/nox/examples/lapack/NOX_ExportMakefile/Rosenbrock.C#L51,C++,yes,journal,algorithm,file,reference,science,pseudocode to source code,no
"`` TODO : Implement method from Gabow , Galil , Spence and Tarjan : @ article { year= { 1986 } , issn= { 0209-9683 } , journal= { Combinatorica } , volume= { 6 } , number= { 2 } , doi= { 10.1007 BF02579168 } , title= { Efficient algorithms for finding minimum spanning trees in undirected and directed graphs } , url= { http : dx.doi.org 10.1007 BF02579168 } , publisher= { Springer-Verlag } , keywords= { 68 B 15 ; 68 C 05 } , author= { Gabow , Harold N. and Galil , Zvi and Spencer , Thomas and Tarjan , Robert E. } , pages= { 109-122 } , language= { English } } ''",https://github.com/metamorph-inc/openmeta-mms/blob/373073ec39ab240b77ef49aefb8f8197917a077f/bin/Python27/Lib/site-packages/networkx/algorithms/tree/branchings.py#L11,Python,yes,journal,background,n/a,related,other,no transfer,no
"`` `` '' '' Function of unitary fourier transform and utilitiesThis module implements the unitary fourier transform , also known asthe ortho-normal transform . It is especially useful for convolution [ 1 ] , as it respects the Parseval equality . The value of the nullfrequency is equal to.. math : : frac { 1 } { sqrt { n } } sum_i x_iso the Fourier transform has the same energy as the original image ( see `` image_quad_norm `` function ) . The transform is applied from thelast axis for performance ( assuming a C-order array input ) .References -- -- -- -- -- .. [ 1 ] B. R. Hunt `` A matrix theory proof of the discrete convolution theorem '' , IEEE Trans . on Audio and Electroacoustics , vol . au-19 , no . 4 , pp . 285-288 , dec. 1971 '' '' '' ''",https://github.com/scikit-image/scikit-image/blob/51f598aaedc73ef180913c670d2c20a8032aaf1e/skimage/restoration/uft.py#L4,Python,yes,journal,background,n/a,related,computer vision,no transfer,no
"`` Conditional constant propagation ( CCP ) is based on the SSA propagation engine ( tree-ssa-propagate.c ) . Constant assignments of the form VAR = CST are propagated from the assignments into uses of VAR , which in turn may generate new constants . The simulation uses a four level lattice to keep track of constant values associated with SSA names . Given an SSA name V_i , it may take one of the following values : UNINITIALIZED - > the initial state of the value . This value is replaced with a correct initial value the first time the value is used , so the rest of the pass does not need to care about it . Using this value simplifies initialization of the pass , and prevents us from needlessly scanning statements that are never reached . UNDEFINED - > V_i is a local variable whose definition has not been processed yet . Therefore we do n't yet know if its value is a constant or not . CONSTANT - > V_i has been found to hold a constant value C. VARYING - > V_i can not take a constant value , or if it does , it is not possible to determine it at compile time . The core of SSA-CCP is in ccp_visit_stmt and ccp_visit_phi_node : 1- In ccp_visit_stmt , we are interested in assignments whose RHS evaluates into a constant and conditional jumps whose predicate evaluates into a boolean true or false . When an assignment of the form V_i = CONST is found , V_i 's lattice value is set to CONSTANT and CONST is associated with it . This causes the propagation engine to add all the SSA edges coming out the assignment into the worklists , so that statements that use V_i can be visited . If the statement is a conditional with a constant predicate , we mark the outgoing edges as executable or not executable depending on the predicate 's value . This is then used when visiting PHI nodes to know when a PHI argument can be ignored . 2- In ccp_visit_phi_node , if all the PHI arguments evaluate to the same constant C , then the LHS of the PHI is set to C. This evaluation is known as the `` meet operation '' . Since one of the goals of this evaluation is to optimistically return constant values as often as possible , it uses two main short cuts : - If an argument is flowing in through a non-executable edge , it is ignored . This is useful in cases like this : if ( PRED ) a_9 = 3 ; else a_10 = 100 ; a_11 = PHI ( a_9 , a_10 ) If PRED is known to always evaluate to false , then we can assume that a_11 will always take its value from a_10 , meaning that instead of consider it VARYING ( a_9 and a_10 have different values ) , we can consider it CONSTANT 100 . - If an argument has an UNDEFINED value , then it does not affect the outcome of the meet operation . If a variable V_i has an UNDEFINED value , it means that either its defining statement has n't been visited yet or V_i has no defining statement , in which case the original symbol ' V ' is being used uninitialized . Since ' V ' is a local variable , the compiler may assume any initial value for it . After propagation , every variable V_i that ends up with a lattice value of CONSTANT will have the associated constant value in the array CONST_VAL [ i ] .VALUE . That is fed into substitute_and_fold for final substitution and folding . References : Constant propagation with conditional branches , Wegman and Zadeck , ACM TOPLAS 13 ( 2 ) :181-210 . Building an Optimizing Compiler , Robert Morgan , Butterworth-Heinemann , 1998 , Section 8.9 . Advanced Compiler Design and Implementation , Steven Muchnick , Morgan Kaufmann , 1997 , Section 12.6 ``",https://github.com/DragonFlyBSD/DragonFlyBSD/blob/ce2989fe1212f664d615268edc64a57801fc7404/contrib/gcc-4.7/gcc/tree-ssa-ccp.c#L23,C,yes,conference,algorithm,file,reference,networks and os,description to source code,no
"`` < -- globalinfo-start -- > Class that splits a BallNode of a ball tree using Uhlmann 's described method. < br > < br > For information see : < br > < br > Jeffrey K. Uhlmann ( 1991 ) . Satisfying general proximity similarity queries with metric trees . Information Processing Letters . 40 ( 4 ) :175-179. < br > < br > Ashraf Masood Kibriya ( 2007 ) . Fast Algorithms for Nearest Neighbour Search . Hamilton , New Zealand . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; article { Uhlmann1991 , author = { Jeffrey K. Uhlmann } , journal = { Information Processing Letters } , month = { November } , number = { 4 } , pages = { 175-179 } , title = { Satisfying general proximity similarity queries with metric trees } , volume = { 40 } , year = { 1991 } } & 64 ; mastersthesis { Kibriya2007 , address = { Hamilton , New Zealand } , author = { Ashraf Masood Kibriya } , school = { Department of Computer Science , School of Computing and Mathematical Sciences , University of Waikato } , title = { Fast Algorithms for Nearest Neighbour Search } , year = { 2007 } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -S & lt ; num & gt ; The seed value for the random number generator . ( default : 17 ) < pre > < -- options-end -- > @ author Ashraf M. Kibriya ( amk14 [ at-the-rate ] cs [ dot ] waikato [ dot ] ac [ dot ] nz ) @ version $ Revision : 10203 $ ``",https://github.com/umple/umple/blob/67002649b1a9db2c15b6e9bb5de7b803204e1b14/Umplificator/UmplifiedProjects/weka-umplified-0/src/main/java/weka/core/neighboursearch/balltrees/MedianDistanceFromArbitraryPoint.java#L39,Java,yes,journal,algorithm,class,reference,other,pseudocode to source code,no
"`` < -- globalinfo-start -- > A filter for wavelet transformation. < br > < br > For more information see : < br > < br > Wikipedia ( 2004 ) . Discrete wavelet transform. < br > < br > Kristian Sandberg ( 2000 ) . The Haar wavelet transform . University of Colorado at Boulder , USA . < p > < -- globalinfo-end -- > < -- technical-bibtex-start -- > BibTeX : < pre > & 64 ; misc { Wikipedia2004 , author = { Wikipedia } , title = { Discrete wavelet transform } , year = { 2004 } , HTTP = { http : en.wikipedia.org wiki Discrete_wavelet_transform } } & 64 ; misc { Sandberg2000 , address = { University of Colorado at Boulder , USA } , author = { Kristian Sandberg } , institution = { Dept . of Applied Mathematics } , title = { The Haar wavelet transform } , year = { 2000 } , HTTP = { http : amath.colorado.edu courses 5720 2000Spr Labs Haar haar.html } } < pre > < p > < -- technical-bibtex-end -- > < -- options-start -- > Valid options are : < p > < pre > -D Turns on output of debugging information . < pre > < pre > -A & lt ; Haar & gt ; The algorithm to use . ( default : HAAR ) < pre > < pre > -P & lt ; Zero & gt ; The padding to use . ( default : ZERO ) < pre > < pre > -F & lt ; filter specification & gt ; The filter to use as preprocessing step ( classname and options ) . ( default : MultiFilter with ReplaceMissingValues and Normalize ) < pre > < pre > Options specific to filter weka.filters.MultiFilter ( '-F ' ) : < pre > < pre > -D Turns on output of debugging information . < pre > < pre > -F & lt ; classname [ options ] & gt ; A filter to apply ( can be specified multiple times ) . < pre > < -- options-end -- > @ author FracPete ( fracpete at waikato dot ac dot nz ) @ version $ Revision $ ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-7-12/packages/internal/wavelet/src/main/java/weka/filters/unsupervised/attribute/Wavelet.java#L48,Java,yes,other (web),algorithm,class,reference,machine learning,pseudocode to source code,no
"`` brief Multi-objective optimization benchmark function LZ6 . The function is described in H. Li and Q. Zhang . Multiobjective Optimization Problems with Complicated Pareto Sets , MOEA D and NSGA-II , IEEE Trans on Evolutionary Computation , 2 ( 12 ) :284-302 , April 2009. ``",https://github.com/Shark-ML/Shark/blob/67990bcd2c4a90a27be97d933b3740931e9da141/include/shark/ObjectiveFunctions/Benchmarks/LZ6.h#L47,C++,yes,journal,background,n/a,related,machine learning,no transfer,no
"`` < p > Perform a topological ( R -Tree ) split of a list of node entries . < p > < p > Only distributions that have between < code > m < code > and < code > M-m+1 < code > entries in the first group will be tested . < p > @ see `` Beckmann , Kriegel , Schneider , Seeger : The R -tree : An Efficient and Robust Access Method for Points and Rectangles , ACM SIGMOD Int . Conf . on Management of Data ( SIGMOD'90 ) , Atlantic City , NJ , 1990 , pp . 322-331 '' @ return chosen split distribution ; note that this method returns null , if the minimum overlap split has a volume which is larger than the allowed < code > maxOverlap < code > ratio of tree ``",https://github.com/elki-project/elki/blob/3e820f9f6382cb82e40955692337737c24f94b54/addons/xtree/src/main/java/de/lmu/ifi/dbs/elki/index/tree/spatial/rstarvariants/xtree/util/XSplitter.java#L700,Java,yes,conference,algorithm,method,reference,data science,pseudocode to source code,no
"`` `` '' '' ==============Normalized Cut==============This example constructs a Region Adjacency Graph ( RAG ) and recursively performsa Normalized Cut on it [ 1 ] _.References -- -- -- -- -- .. [ 1 ] Shi , J. ; Malik , J. , `` Normalized cuts and image segmentation '' , Pattern Analysis and Machine Intelligence , IEEE Transactions on , vol . 22 , no . 8 , pp . 888-905 , August 2000 . '' '' '' ''",https://github.com/scikit-image/scikit-image/blob/51f598aaedc73ef180913c670d2c20a8032aaf1e/doc/examples/segmentation/plot_ncut.py#L1,Python,yes,journal,background,n/a,related,computer vision,no transfer,no
"`` Cumulative bivariate normal distribution function Drezner ( 1978 ) algorithm , six decimal places accuracy . For this implementation see `` Option pricing formulas '' , E.G . Haug , McGraw-Hill 1998 todo check accuracy of this algorithm and compare with : 1 ) Drezner , Z , ( 1978 ) , Computation of the bivariate normal integral , Mathematics of Computation 32 , pp . 277-279 . 2 ) Drezner , Z. and Wesolowsky , G. O . ( 1990 ) ` On the Computation of the Bivariate Normal Integral ' , Journal of Statistical Computation and Simulation 35 , pp . 101-107 . 3 ) Drezner , Z ( 1992 ) Computation of the Multivariate Normal Integral , ACM Transactions on Mathematics Software 18 , pp . 450-460 . 4 ) Drezner , Z ( 1994 ) Computation of the Trivariate Normal Integral , Mathematics of Computation 62 , pp . 289-294 . 5 ) Genz , A . ( 1992 ) ` Numerical Computation of the Multivariate Normal Probabilities ' , J. Comput . Graph . Stat . 1 , pp . 141-150. test the correctness of the returned value is tested by checking it against known good results. ``",https://github.com/lballabio/QuantLib/blob/2c2dd00a3ecd71ae7c7607e025bba159d8687637/ql/math/distributions/bivariatenormaldistribution.hpp#L32,C++,yes,journal,background,n/a,related,other,no transfer,no
"`` `` '' '' p , q are polynomials in Z [ x ] or Q [ x ] . It is assumed that degree ( p , x ) > = degree ( q , x ) . Computes the `` modified '' subresultant prs of p and q in Z [ x ] or Q [ x ] ; the coefficients of the polynomials in the sequence are `` modified '' subresultants . That is , they are determinants of appropriately selected submatrices of sylvester2 , Sylvester 's matrix of 1853 . To compute the coefficients , no determinant evaluation takes place . Instead , polynomial divisions in Q [ x ] are performed , using the function rem ( p , q , x ) ; the coefficients of the remainders computed this way become `` modified '' subresultants with the help of the Pell-Gordon Theorem of 1917 . If the `` modified '' subresultant prs is complete , and LC ( p ) > 0 , it coincides with the ( generalized ) Sturm sequence of the polynomials p , q. References : =========== 1 . Pell A. J. , R. L. Gordon . The Modified Remainders Obtained in Finding the Highest Common Factor of Two Polynomials . Annals of MatheMatics , Second Series , 18 ( 1917 ) , No . 4 , 188‚Äì193 . 2 . Akritas , A. G. , G.I . Malaschonok and P.S . Vigklas : `` Sturm Sequences and Modified Subresultant Polynomial Remainder Sequences . '' Serdica Journal of Computing , Vol . 8 , No 1 , 29‚Äì46 , 2014. `` '' '' ''",https://github.com/sympy/sympy/blob/c05c2755fec78ff19af89a4599f76edb2ee104af/sympy/polys/subresultants_qq_zz.py#L1151,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` Computes the convex hull of a set of three dimensional points . < p > The algorithm is a three dimensional implementation of Quickhull , as described in Barber , Dobkin , and Huhdanpaa , < a href=http : citeseer.ist.psu.edu barber96quickhull.html > `` The Quickhull Algorithm for Convex Hulls '' < a > ( ACM Transactions on Mathematical Software , Vol . 22 , No . 4 , December 1996 ) , and has a complexity of O ( n log ( n ) ) with respect to the number of points . A well-known C implementation of Quickhull that works for arbitrary dimensions is provided by < a href=http : www.qhull.org > qhull < a > . < p > A hull is constructed by providing a set of points to either a constructor or a { @ link build ( Point3d [ ] ) build } method . After the hull is built , its vertices and faces can be retrieved using { @ link getVertices ( ) getVertices } and { @ link getFaces ( ) getFaces } . A typical usage might look like this : < pre > x y z coordinates of 6 points Point3d [ ] points = new Point3d [ ] { new Point3d ( 0.0 , 0.0 , 0.0 ) , new Point3d ( 1.0 , 0.5 , 0.0 ) , new Point3d ( 2.0 , 0.0 , 0.0 ) , new Point3d ( 0.5 , 0.5 , 0.5 ) , new Point3d ( 0.0 , 0.0 , 2.0 ) , new Point3d ( 0.1 , 0.2 , 0.3 ) , new Point3d ( 0.0 , 2.0 , 0.0 ) , } ; QuickHull3D hull = new QuickHull3D ( ) ; hull.build ( points ) ; System.out.println ( `` Vertices : '' ) ; Point3d [ ] vertices = hull.getVertices ( ) ; for ( int i = 0 ; i < vertices.length ; i++ ) { Point3d pnt = vertices [ i ] ; System.out.println ( pnt.x + `` `` + pnt.y + `` `` + pnt.z ) ; } System.out.println ( `` Faces : '' ) ; int [ ] [ ] faceIndices = hull.getFaces ( ) ; for ( int i = 0 ; i < vertices.length ; i++ ) { for ( int k = 0 ; k < faceIndices [ i ] .length ; k++ ) { System.out.print ( faceIndices [ i ] [ k ] + `` `` ) ; } System.out.println ( `` '' ) ; } < pre > As a convenience , there are also { @ link build ( double [ ] ) build } and { @ link getVertices ( double [ ] ) getVertex } methods which pass point information using an array of doubles . < h3 > < a name=distTol > Robustness < h3 > Because this algorithm uses floating point arithmetic , it is potentially vulnerable to errors arising from numerical imprecision . We address this problem in the same way as < a href=http : www.qhull.org > qhull < a > , by merging faces whose edges are not clearly convex . A face is convex if its edges are convex , and an edge is convex if the centroid of each adjacent plane is clearly < i > below < i > the plane of the other face . The centroid is considered below a plane if its distance to the plane is less than the negative of a { @ link getDistanceTolerance ( ) distance tolerance } . This tolerance represents the smallest distance that can be reliably computed within the available numeric precision . It is normally computed automatically from the point data , although an application may { @ link setExplicitDistanceTolerance set this tolerance explicitly } . < p > Numerical problems are more likely to arise in situations where data points lie on or within the faces or edges of the convex hull . We have tested QuickHull3D for such situations by computing the convex hull of a random point set , then adding additional randomly chosen points which lie very close to the hull vertices and edges , and computing the convex hull again . The hull is deemed correct if { @ link check check } returns < code > true < code > . These tests have been successful for a large number of trials and so we are confident that QuickHull3D is reasonably robust . < h3 > Merged Faces < h3 > The merging of faces means that the faces returned by QuickHull3D may be convex polygons instead of triangles . If triangles are desired , the application may { @ link triangulate triangulate } the faces , but it should be noted that this may result in triangles which are very small or thin and hence difficult to perform reliable convexity tests on . In other words , triangulating a merged face is likely to restore the numerical problems which the merging process removed . Hence is it possible that , after triangulation , { @ link check check } will fail ( the same behavior is observed with triangulated output from < a href=http : www.qhull.org > qhull < a > ) . < h3 > Degenerate Input < h3 > It is assumed that the input points are non-degenerate in that they are not coincident , colinear , or colplanar , and thus the convex hull has a non-zero volume . If the input points are detected to be degenerate within the { @ link getDistanceTolerance ( ) distance tolerance } , an IllegalArgumentException will be thrown . @ author John E. Lloyd , Fall 2004 ``",https://github.com/geogebra/geogebra/blob/3d238082343530b2f80144825c5f3f46cc0f8739/common/src/main/java/com/github/quickhull3d/QuickHull3D.java#L38,Java,yes,journal,algorithm,class,reference,science,pseudocode to source code,no
"`` RELEASE INFORMATION ( December 27 , 2004 ) FCBF algorithm : Template obtained from Weka Developed for Weka by Zheng Alan Zhao December 27 , 2004 FCBF algorithm is a feature selection method based on Symmetrical Uncertainty Measurement for relevance redundancy analysis . The details of FCBF algorithm are in L. Yu and H. Liu . Feature selection for high-dimensional data : a fast correlation-based filter solution . In Proceedings of the twentieth International Conference on Machine Learning , pages 856 -- 863 , 2003 . CONTACT INFORMATION For algorithm implementation : Zheng Zhao : zhaozheng at asu.edu For the algorithm : Lei Yu : leiyu at asu.edu Huan Liu : hliu at asu.edu Data Mining and Machine Learning Lab Computer Science and Engineering Department Fulton School of Engineering Arizona State University Tempe , AZ 85287 SymmetricalUncertAttributeSetEval.java Copyright ( C ) 2004 Data Mining and Machine Learning Lab , Computer Science and Engineering Department , Fulton School of Engineering , Arizona State University ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/dev-3-5-1/weka/attributeSelection/SymmetricalUncertAttributeSetEval.java#L17,Java,yes,conference,algorithm,file,reference,machine learning,pseudocode to source code,no
"`` r '' '' '' Return communicability between all pairs of nodes in G. Communicability between pair of node ( u , v ) of node in G is the sum of closed walks of different lengths starting at node u and ending at node v. Parameters -- -- -- -- -- G : graph Returns -- -- -- - comm : dictionary of dictionaries Dictionary of dictionaries keyed by nodes with communicability as the value . Raises -- -- -- NetworkXError If the graph is not undirected and simple . See Also -- -- -- -- communicability : Communicability between pairs of nodes in G. communicability_betweenness_centrality : Communicability betweeness centrality for each node in G. Notes -- -- - This algorithm uses matrix exponentiation of the adjacency matrix . Let G= ( V , E ) be a simple undirected graph . Using the connection between the powers of the adjacency matrix and the number of walks in the graph , the communicability between nodes u and v is [ 1 ] _ , .. math : : C ( u , v ) = ( e^A ) _ { uv } , where ` A ` is the adjacency matrix of G. References -- -- -- -- -- .. [ 1 ] Ernesto Estrada , Naomichi Hatano , `` Communicability in complex networks '' , Phys . Rev . E 77 , 036111 ( 2008 ) . https : arxiv.org abs 0707.0756 Examples -- -- -- -- > > > G = nx.Graph ( [ ( 0,1 ) , ( 1,2 ) , ( 1,5 ) , ( 5,4 ) , ( 2,4 ) , ( 2,3 ) , ( 4,3 ) , ( 3,6 ) ] ) > > > c = nx.communicability_exp ( G ) `` '' '' ''",https://github.com/networkx/networkx/blob/404679f7a8a9f015a693eaa01a5b319091123b31/networkx/algorithms/communicability_alg.py#L105,Python,yes,journal,algorithm,method,reference,networks and os,formulas to source code,no
"`` Purpose -- -- -- - SGEHRD reduces a REAL general matrix A to upper Hessenberg form H by an orthogonal similarity transformation : Q ' A Q = H . This version stores the triangular matrices used in the factorization so that they can be applied directly ( i.e. , without being recomputed ) later . As a result , the application of Q is much faster . Arguments -- -- -- -- - @ param [ in ] n INTEGER The order of the matrix A. N > = 0 . @ param [ in ] ilo INTEGER @ param [ in ] ihi INTEGER It is assumed that A is already upper triangular in rows and columns 1 : ILO-1 and IHI+1 : N. ILO and IHI are normally set by a previous call to SGEBAL ; otherwise they should be set to 1 and N respectively . See Further Details . 1 < = ILO < = IHI < = N , if N > 0 ; ILO=1 and IHI=0 , if N=0 . @ param [ in , out ] A REAL array , dimension ( LDA , N ) On entry , the N-by-N general matrix to be reduced . On exit , the upper triangle and the first subdiagonal of A are overwritten with the upper Hessenberg matrix H , and the elements below the first subdiagonal , with the array TAU , represent the orthogonal matrix Q as a product of elementary reflectors . See Further Details . @ param [ in ] lda INTEGER The leading dimension of the array A. LDA > = max ( 1 , N ) . @ param [ out ] tau REAL array , dimension ( N-1 ) The scalar factors of the elementary reflectors ( see Further Details ) . Elements 1 : ILO-1 and IHI : N-1 of TAU are set to zero . @ param [ out ] work ( workspace ) REAL array , dimension ( LWORK ) On exit , if INFO = 0 , WORK [ 0 ] returns the optimal LWORK . @ param [ in ] lwork INTEGER The length of the array WORK . LWORK > = max ( 1 , N ) . For optimum performance LWORK > = N NB , where NB is the optimal blocksize . If LWORK = -1 , then a workspace query is assumed ; the routine only calculates the optimal size of the WORK array , returns this value as the first entry of the WORK array , and no error message related to LWORK is issued by XERBLA . @ param [ out ] T REAL array , dimension NB N , where NB is the optimal blocksize . It stores the NB NB blocks of the triangular T matrices used in the reduction . @ param [ out ] info INTEGER - = 0 : successful exit - < 0 : if INFO = -i , the i-th argument had an illegal value . Further Details -- -- -- -- -- -- -- - The matrix Q is represented as a product of ( ihi-ilo ) elementary reflectors Q = H ( ilo ) H ( ilo+1 ) . . . H ( ihi-1 ) . Each H ( i ) has the form H ( i ) = I - tau v v ' where tau is a real scalar , and v is a real vector with v ( 1 : i ) = 0 , v ( i+1 ) = 1 and v ( ihi+1 : n ) = 0 ; v ( i+2 : ihi ) is stored on exit in A ( i+2 : ihi , i ) , and tau in TAU ( i ) . The contents of A are illustrated by the following example , with n = 7 , ilo = 2 and ihi = 6 : @ verbatim on entry , on exit , ( a a a a a a a ) ( a a h h h h a ) ( a a a a a a ) ( a h h h h a ) ( a a a a a a ) ( h h h h h h ) ( a a a a a a ) ( v2 h h h h h ) ( a a a a a a ) ( v2 v3 h h h h ) ( a a a a a a ) ( v2 v3 v4 h h h ) ( a ) ( a ) @ endverbatim where a denotes an element of the original matrix A , h denotes a modified element of the upper Hessenberg matrix H , and vi denotes an element of the vector defining H ( i ) . This implementation follows the hybrid algorithm and notations described in S. Tomov and J. Dongarra , `` Accelerating the reduction to upper Hessenberg form through hybrid GPU-based computing , '' University of Tennessee Computer Science Technical Report , UT-CS-09-642 ( also LAPACK Working Note 219 ) , May 24 , 2009 . This version stores the T matrices , for later use in magma_sorghr . @ ingroup magma_sgeev_comp ``",https://github.com/cjy7117/FT-MAGMA/blob/f084356fd94ecdce75926a4651f8fd95390280ec/old_source_code/magma-1.6.2-v7/src/sgehrd_m.cpp#L15,C++,yes,other (report),algorithm,file,reference,other,pseudocode to source code,no
"`` Purpose -- -- -- - ZLAHR2 reduces the first NB columns of a complex general n-BY- ( n-k+1 ) matrix A so that elements below the k-th subdiagonal are zero . The reduction is performed by an orthogonal similarity transformation Q ' A Q . The routine returns the matrices V and T which determine Q as a block reflector I - V T V ' , and also the matrix Y = A V. ( Note this is different than LAPACK , which computes Y = A V T. ) This is an auxiliary routine called by ZGEHRD . Arguments -- -- -- -- - @ param [ in ] n INTEGER The order of the matrix A . @ param [ in ] k INTEGER The offset for the reduction . Elements below the k-th subdiagonal in the first NB columns are reduced to zero . K < N. @ param [ in ] nb INTEGER The number of columns to be reduced . @ param [ in , out ] A COMPLEX_16 array , dimension ( LDA , N-K+1 ) On entry , the n-by- ( n-k+1 ) general matrix A . On exit , the elements on and above the k-th subdiagonal in the first NB columns are overwritten with the corresponding elements of the reduced matrix ; the elements below the k-th subdiagonal , with the array TAU , represent the matrix Q as a product of elementary reflectors . The other columns of A are unchanged . See Further Details . @ param [ in ] lda INTEGER The leading dimension of the array A. LDA > = max ( 1 , N ) . @ param [ out ] tau COMPLEX_16 array , dimension ( NB ) The scalar factors of the elementary reflectors . See Further Details . @ param [ out ] T COMPLEX_16 array , dimension ( LDT , NB ) The upper triangular matrix T. @ param [ in ] ldt INTEGER The leading dimension of the array T. LDT > = NB . @ param [ out ] Y COMPLEX_16 array , dimension ( LDY , NB ) The n-by-nb matrix Y . @ param [ in ] ldy INTEGER The leading dimension of the array Y. LDY > = N. @ param [ in , out ] data Structure with pointers to dA , dT , dV , dW , dY which are distributed across multiple GPUs . Further Details -- -- -- -- -- -- -- - The matrix Q is represented as a product of nb elementary reflectors Q = H ( 1 ) H ( 2 ) . . . H ( nb ) . Each H ( i ) has the form H ( i ) = I - tau v v ' where tau is a complex scalar , and v is a complex vector with v ( 1 : i+k-1 ) = 0 , v ( i+k ) = 1 ; v ( i+k+1 : n ) is stored on exit in A ( i+k+1 : n , i ) , and tau in TAU ( i ) . The elements of the vectors v together form the ( n-k+1 ) -by-nb matrix V which is needed , with T and Y , to apply the transformation to the unreduced part of the matrix , using an update of the form : A : = ( I - V T V ' ) ( A - Y T V ' ) . The contents of A on exit are illustrated by the following example with n = 7 , k = 3 and nb = 2 : @ verbatim ( a a a a a ) ( a a a a a ) ( a a a a a ) ( h h a a a ) ( v1 h a a a ) ( v1 v2 a a a ) ( v1 v2 a a a ) @ endverbatim where `` a '' denotes an element of the original matrix A , h denotes a modified element of the upper Hessenberg matrix H , and vi denotes an element of the vector defining H ( i ) . This implementation follows the hybrid algorithm and notations described in S. Tomov and J. Dongarra , `` Accelerating the reduction to upper Hessenberg form through hybrid GPU-based computing , '' University of Tennessee Computer Science Technical Report , UT-CS-09-642 ( also LAPACK Working Note 219 ) , May 24 , 2009 . @ ingroup magma_lahr2 ``",https://github.com/cjy7117/FT-MAGMA/blob/f084356fd94ecdce75926a4651f8fd95390280ec/magma-2.3.0/src/zlahr2_m.cpp#L15,C++,yes,other (report),algorithm,file,reference,other,pseudocode to source code,no
"`` Basic line search class . This method minimizes the energy of a system along a given direction using a two stage algorithm . It is based on cubic and quadratic interpolation methods of Jorge J . More and David J. Thuente . See [ 1 ] J . More and D. Thuente , `` Line search algorithms with guaranteed sufficient decrease , '' ACM Transactions on Mathematical Software 20 ( 1994 ) , no . 3 , pp . 286-307. ingroup MolmecEnergyMinimizer ``",https://github.com/BALL-Project/ball/blob/b9ac048ae7d9307d3bfc9d09c8853f5a2d797580/include/BALL/MOLMEC/MINIMIZATION/lineSearch.h#L20,C++,yes,journal,background,n/a,related,science,no transfer,no
"`` `` '' '' Models , modeling and model-fitting of birth-death processes.- Nee , S. 2001 . Inferring speciation rates from phylogenies . Evolution 55:661-668.- Yule , G. U . 1924 . A mathematical theory of evolution based on the conclusions of Dr. J. C. Willis . Phil . Trans . R. Soc . Lond . B 213:21-87.- Hoehna , S. ( 2015 ) . The time-dependent reconstructed evolutionary process with a key-role for mass-extinction events . Journal of theoretical biology , 380 , 321-331 . '' '' '' ''",https://github.com/jeetsukumaran/DendroPy/blob/78135cb50d5417ad06c35750ae2aafe4fe54aa35/src/dendropy/model/birthdeath.py#L40,Python,yes,journal,background,n/a,related,science,no transfer,no
"`` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- This routine calculates the GAMMA function for a float argument X. Computation is based on an algorithm outlined in reference [ 1 ] . The program uses rational functions that approximate the GAMMA function to at least 20 significant decimal digits . Coefficients for the approximation over the interval ( 1,2 ) are unpublished . Those for the approximation for X > = 12 are from reference [ 2 ] . The accuracy achieved depends on the arithmetic system , the compiler , the intrinsic functions , and proper selection of the machine-dependent constants . Error returns The program returns the value XINF for singularities or when overflow would occur . The computation is believed to be free of underflow and overflow . Intrinsic functions required are : INT , DBLE , EXP , LOG , REAL , SIN References : [ 1 ] `` An Overview of Software Development for Special Functions '' , W. J. Cody , Lecture Notes in Mathematics , 506 , Numerical Analysis Dundee , 1975 , G. A. Watson ( ed . ) , Springer Verlag , Berlin , 1976 . [ 2 ] Computer Approximations , Hart , Et . Al. , Wiley and sons , New York , 1968 . Latest modification : October 12 , 1989 Authors : W. J. Cody and L. Stoltz Applied Mathematics Division Argonne National Laboratory Argonne , IL 60439 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ``",https://github.com/RevolutionAnalytics/RRO/blob/eb18c92e462c0620bceee4c8af6da8044af9aa4d/R-src/src/nmath/gamma_cody.c#L14,C,yes,book,algorithm,method,reference,other,paper not available,no
"`` Generic binary BCH encoding decoding library This program is free software ; you can redistribute it and or modify it under the terms of the GNU General Public License version 2 as published by the Free Software Foundation . This program is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . See the GNU General Public License for more details . Copyright ¬© 2011 Parrot S.A . Author : Ivan Djelic < ivan.djelic @ parrot.com > Description : This library provides runtime configurable encoding decoding of binary Bose-Chaudhuri-Hocquenghem ( BCH ) codes . Call init_bch to get a pointer to a newly allocated bch_control structure for the given m ( Galois field order ) , t ( error correction capability ) and ( optional ) primitive polynomial parameters . Call encode_bch to compute and store ecc parity bytes to a given buffer . Call decode_bch to detect and locate errors in received data . On systems supporting hw BCH features , intermediate results may be provided to decode_bch in order to skip certain steps . See decode_bch ( ) documentation for details . Option CONFIG_BCH_CONST_PARAMS can be used to force fixed values of parameters m and t ; thus allowing extra compiler optimizations and providing better ( up to 2x ) encoding performance . Using this option makes sense when ( m , t ) are fixed and known in advance , e.g . when using BCH error correction on a particular NAND flash device . Algorithmic details : Encoding is performed by processing 32 input bits in parallel , using 4 remainder lookup tables . The final stage of decoding involves the following internal steps : a . Syndrome computation b . Error locator polynomial computation using Berlekamp-Massey algorithm c. Error locator root finding ( by far the most expensive step ) In this implementation , step c is not performed using the usual Chien search . Instead , an alternative approach described in [ 1 ] is used . It consists in factoring the error locator polynomial using the Berlekamp Trace algorithm ( BTA ) down to a certain degree ( 4 ) , after which ad hoc low-degree polynomial solving techniques [ 2 ] are used . The resulting algorithm , called BTZ , yields much better performance than Chien search for usual ( m , t ) values ( typically m > = 13 , t < 32 , see [ 1 ] ) . [ 1 ] B. Biswas , V. Herbert . Efficient root finding of polynomials over fields of characteristic 2 , in : Western European Workshop on Research in Cryptology - WEWoRC 2009 , Graz , Austria , LNCS , Springer , July 2009 , to appear . [ 2 ] [ Zin96 ] V.A . Zinoviev . On the solution of equations of degree 10 over finite fields GF ( 2^q ) . In Rapport de recherche INRIA no 2829 , 1996 . Taken from the linux kernel. ``",https://github.com/woodsts/barebox/blob/7257e80f5c052467333d800f2951c9338400111a/lib/bch.c#L1,C,yes,conference,algorithm,file,reference,networks and os,pseudocode to source code,no
"`` weighted_p_square_quantile_impl single quantile estimation with weighted samples @ brief Single quantile estimation with the f $ P^2 f $ algorithm for weighted samples This version of the f $ P^2 f $ algorithm extends the f $ P^2 f $ algorithm to support weighted samples . The f $ P^2 f $ algorithm estimates a quantile dynamically without storing samples . Instead of storing the whole sample cumulative distribution , only five points ( markers ) are stored . The heights of these markers are the minimum and the maximum of the samples and the current estimates of the f $ ( p 2 ) f $ - , f $ p f $ - and f $ ( 1+p ) 2 f $ -quantiles . Their positions are equal to the number of samples that are smaller or equal to the markers . Each time a new sample is added , the positions of the markers are updated and if necessary their heights are adjusted using a piecewise- parabolic formula . For further details , see R. Jain and I. Chlamtac , The P^2 algorithm for dynamic calculation of quantiles and histograms without storing observations , Communications of the ACM , Volume 28 ( October ) , Number 10 , 1985 , p. 1076-1085 . @ param quantile_probability ``",https://github.com/mapsme/omim/blob/a8e44c7f0e0e3622088b36e21d3ea459ca6da1ad/3party/boost/boost/accumulators/statistics/weighted_p_square_quantile.hpp#L30,C++,yes,journal,algorithm,method,reference,other,paper not available,no
"`` For normalized density , we use the same version as the one used in Newmans ' Modularity Newman , M. E. J . ( 2006 ) . Modularity and community structure in networks . Proceedings of the National Academy of ‚Ä¶ , 103 ( 23 ) , 8577‚Äì8582 . http : doi.org 10.1073 pnas.0601602103 Here , for a directed network ''",https://github.com/medialab/hyphe/blob/388ad5aa549cc918c95d8627c9eb97bfe96d9a98/hyphe_frontend/app/views/tool_networkTagStats.js#L111,JavaScript,yes,journal,background,n/a,related,other,no transfer,no
"`` Sobol low-discrepancy sequence generator A Gray code counter and bitwise operations are used for very fast sequence generation . The implementation relies on primitive polynomials modulo two from the book `` Monte Carlo Methods in Finance '' by Peter J√§ckel . 21 200 primitive polynomials modulo two are provided in QuantLib . J√§ckel has calculated 8 129 334 polynomials : if you need that many dimensions you can replace the primitivepolynomials.cpp file included in QuantLib with the one provided in the CD of the `` Monte Carlo Methods in Finance '' book . The choice of initialization numbers ( also know as free direction integers ) is crucial for the homogeneity properties of the sequence . Sobol defines two homogeneity properties : Property A and Property A ' . The unit initialization numbers suggested in `` Numerical Recipes in C '' , 2nd edition , by Press , Teukolsky , Vetterling , and Flannery ( section 7.7 ) fail the test for Property A even for low dimensions . Bratley and Fox published coefficients of the free direction integers up to dimension 40 , crediting unpublished work of Sobol ' and Levitan . See Bratley , P. , Fox , B.L . ( 1988 ) `` Algorithm 659 : Implementing Sobol 's quasirandom sequence generator , '' ACM Transactions on Mathematical Software 14:88-100 . These values satisfy Property A for d < =20 and d = 23 , 31 , 33 , 34 , 37 ; Property A ' holds for d < =6 . J√§ckel provides in his book ( section 8.3 ) initialization numbers up to dimension 32 . Coefficients for d < =8 are the same as in Bradley-Fox , so Property A ' holds for d < =6 but Property A holds for d < =32 . The implementation of Lemieux , Cieslak , and Luttmer includes coefficients of the free direction integers up to dimension 360 . Coefficients for d < =40 are the same as in Bradley-Fox . For dimension 40 < d < =360 the coefficients have been calculated as optimal values based on the `` resolution '' criterion . See `` RandQMC user 's guide - A package for randomized quasi-Monte Carlo methods in C , '' by C. Lemieux , M. Cieslak , and K. Luttmer , version January 13 2004 , and references cited there ( http : www.math.ucalgary.ca ~lemieux randqmc.html ) . The values up to d < =360 has been provided to the QuantLib team by Christiane Lemieux , private communication , September 2004 . For more info on Sobol ' sequences see also `` Monte Carlo Methods in Financial Engineering , '' by P. Glasserman , 2004 , Springer , section 5.2.3 The Joe -- Kuo numbers and the Kuo numbers are due to Stephen Joe and Frances Kuo . S. Joe and F. Y. Kuo , Constructing Sobol sequences with better two-dimensional projections , preprint Nov 22 2007 See http : web.maths.unsw.edu.au ~fkuo sobol for more information . The Joe-Kuo numbers are available under a BSD-style license available at the above link . Note that the Kuo numbers were generated to work with a different ordering of primitive polynomials for the first 40 or so dimensions which is why we have the Alternative Primitive Polynomials . test - the correctness of the returned values is tested by reproducing known good values . - the correctness of the returned values is tested by checking their discrepancy against known good values. ``",https://github.com/lballabio/QuantLib/blob/2c2dd00a3ecd71ae7c7607e025bba159d8687637/ql/math/randomnumbers/sobolrsg.hpp#L35,C++,yes,book,background,n/a,related,other,no transfer,no
"`` < p > OPTICS.java < br > Authors : Rainer Holzmann , Zhanna Melnikova-Albrecht , Matthias Schubert < br > Date : Sep 5 , 2004 < br > Time : 9:18:51 PM < br > $ Revision 1.4 $ < br > < br > < br > Reference : Ankerst M. , Breunig M. M. , Kriegel H.-P. , Sander J. : < br > OPTICS : Ordering Points To Identify the Clustering Structure < br > Proc . ACM SIGMOD Int . Conf . on Management of Data ( SIGMOD'99 ) , Philadelphia , PA , 1999 , pp . 49-60 . < br > < p > ``",https://github.com/svn2github/weka/blob/c8366c454e9718d0e1634ddf4a72319dac3ce559/tags/stable-3-4-4/weka/clusterers/OPTICS.java#L44,Java,yes,conference,algorithm,class,reference,machine learning,pseudocode to source code,no
"`` Implementation Notes David Rowe April 2007 This code started life as Steve 's NLMS algorithm with a tap rotation algorithm to handle divergence during double talk . I added a Geigel Double Talk Detector ( DTD ) [ 2 ] and performed some G168 tests . However I had trouble meeting the G168 requirements , especially for double talk - there were always cases where my DTD failed , for example where near end speech was under the 6dB threshold required for declaring double talk . So I tried a two path algorithm [ 1 ] , which has so far given better results . The original tap rotation Geigel algorithm is available in SVN http : svn.rowetel.com software oslec tags before_16bit . It 's probably possible to make it work if some one wants to put some serious work into it . At present no special treatment is provided for tones , which generally cause NLMS algorithms to diverge . Initial runs of a subset of the G168 tests for tones ( e.g . echo_test 6 ) show the current algorithm is passing OK , which is kind of surprising . The full set of tests needs to be performed to confirm this result . One other interesting change is that I have managed to get the NLMS code to work with 16 bit coefficients , rather than the original 32 bit coefficents . This reduces the MIPs and storage required . I evaulated the 16 bit port using g168_tests.sh and listening tests on 4 real-world samples . I also attempted the implementation of a block based NLMS update [ 2 ] but although this passes g168_tests.sh it did n't converge well on the real-world samples . I have no idea why , perhaps a scaling problem . The block based code is also available in SVN http : svn.rowetel.com software oslec tags before_16bit . If this code can be debugged , it will lead to further reduction in MIPS , as the block update code maps nicely onto DSP instruction sets ( it 's a dot product ) compared to the current sample-by-sample update . Steve also has some nice notes on echo cancellers in echo.h References : [ 1 ] Ochiai , Areseki , and Ogihara , `` Echo Canceller with Two Echo Path Models '' , IEEE Transactions on communications , COM-25 , No . 6 , June 1977. http : www.rowetel.com images echo dual_path_paper.pdf [ 2 ] The classic , very useful paper that tells you how to actually build a real world echo canceller : Messerschmitt , Hedberg , Cole , Haoui , Winship , `` Digital Voice Echo Canceller with a TMS320020 , http : www.rowetel.com images echo spra129.pdf [ 3 ] I have written a series of blog posts on this work , here is Part 1 : http : www.rowetel.com blog ? p=18 [ 4 ] The source code http : svn.rowetel.com software oslec [ 5 ] A nice reference on LMS filters : http : en.wikipedia.org wiki Least_mean_squares_filter Credits : Thanks to Steve Underwood , Jean-Marc Valin , and Ramakrishnan Muthukrishnan for their suggestions and email discussions . Thanks also to those people who collected echo samples for me such as Mark , Pawel , and Pavel. ``",https://github.com/RadeonOpenCompute/ROCK-Kernel-Driver/blob/66b2faa04fb369d833930ebff03b8a79d9d41773/drivers/misc/echo/echo.c#L34,C,yes,journal,background,n/a,related,other,no transfer,no
"`` Recommended papers : Y. Ephraim and D. Malah , `` Speech enhancement using minimum mean-square error short-time spectral amplitude estimator '' . IEEE Transactions on Acoustics , Speech and Signal Processing , vol . ASSP-32 , no . 6 , pp . 1109-1121 , 1984 . Y. Ephraim and D. Malah , `` Speech enhancement using minimum mean-square error log-spectral amplitude estimator '' . IEEE Transactions on Acoustics , Speech and Signal Processing , vol . ASSP-33 , no . 2 , pp . 443-445 , 1985 . I. Cohen and B. Berdugo , `` Speech enhancement for non-stationary noise environments '' . Signal Processing , vol . 81 , no . 2 , pp . 2403-2418 , 2001 . Stefan Gustafsson , Rainer Martin , Peter Jax , and Peter Vary . `` A psychoacoustic approach to combined acoustic echo cancellation and noise reduction '' . IEEE Transactions on Speech and Audio Processing , 2002 . J.-M. Valin , J. Rouat , and F. Michaud , `` Microphone array post-filter for separation of simultaneous non-stationary sources '' . In Proceedings IEEE International Conference on Acoustics , Speech , and Signal Processing , 2004. ``",https://github.com/Rockbox/rockbox/blob/992a12670e65eab504eec2f8c4d7a120a4a6dd50/lib/rbcodec/codecs/libspeex/preprocess.c#L35,C,yes,journal,background,n/a,related,other,no transfer,no
"`` 80 Purpose : LD0434 computes the 434 point Lebedev angular grid . Modified : 12 September 2010 Author : Dmitri Laikov Reference : Vyacheslav Lebedev , Dmitri Laikov , A quadrature formula for the sphere of the 131st algebraic order of accuracy , Russian Academy of Sciences Doklady Mathematics , Volume 59 , Number 3 , 1999 , pages 477-481 . Parameters : Output , double X [ N ] , Y [ N ] , Z [ N ] , W [ N ] , the coordinates and weights of the points. ``",https://github.com/nubakery/bagel/blob/7c55fcbb32a9c53a4785074ed1aa4820ecc5b378/src/scf/ks/lebedev.cc#L1136,C++,yes,journal,algorithm,method,reference,science,paper not available,no
"`` `` '' '' Perfoms Cholesky decomposition of the diffusion tensor Parameters -- -- -- -- -- tensor_elements : array ( 6 , ) Array containing the six elements of diffusion tensor 's lower triangular . Returns -- -- -- - cholesky_elements : array ( 6 , ) Array containing the six Cholesky 's decomposition elements ( R0 , R1 , R2 , R3 , R4 , R5 ) [ 1 ] _. References -- -- -- -- -- .. [ 1 ] Koay , C.G. , Carew , J.D. , Alexander , A.L. , Basser , P.J. , Meyerand , M.E. , 2006 . Investigation of anomalous estimates of tensor-derived quantities in diffusion tensor imaging . Magnetic Resonance in Medicine , 55 ( 4 ) , 930-936. doi:10.1002 mrm.20832 `` '' '' ''",https://github.com/nipy/dipy/blob/6bee25c1d25acc1437752be0e166acce42bfed84/dipy/reconst/fwdti.py#L736,Python,yes,journal,algorithm,method,reference,science,formulas to source code,no
"`` tiny_ssim.c Computes the Structural Similarity Metric between two rawYV12 video files . original algorithm : Z. Wang , A. C. Bovik , H. R. Sheikh and E. P. Simoncelli , `` Image quality assessment : From error visibility to structural similarity , '' IEEE Transactions on Image Processing , vol . 13 , no . 4 , pp . 600-612 , Apr . 2004 . To improve speed , this implementation uses the standard approximation of overlapped 8x8 block sums , rather than the original gaussian weights. ``",https://github.com/FFmpeg/FFmpeg/blob/ea0010bf9cd2be63ab0c174abd3cac754af9ee40/tests/tiny_ssim.c#L18,C,yes,journal,algorithm,file,reference,other,formulas to source code,no
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,
,,,,,,,,,,